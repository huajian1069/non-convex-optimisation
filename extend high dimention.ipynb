{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from library.optimiser import *\n",
    "from library.post_analysis import *\n",
    "from library.experiments import *\n",
    "%matplotlib inline\n",
    "SEED = 23191"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check func value: expected: 0 , actual: 4.440892098500626e-16\n",
      "check gradient: expected:[0,0], actual: [0 0]\n"
     ]
    }
   ],
   "source": [
    "class objective_func(ABC):\n",
    "    @abstractmethod\n",
    "    def func(self, x):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def dfunc(self, x):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def get_optimal(self):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def get_optimum(self):\n",
    "        pass\n",
    "    def visualise2d(self, lim, n):\n",
    "        x, y = np.linspace(-lim, lim, n), np.linspace(-lim, lim, n)\n",
    "        xx, yy = np.meshgrid(x, y)\n",
    "        zz = np.zeros(xx.shape)\n",
    "        for j in range(n):\n",
    "            for i in range(n):\n",
    "                zz[j, i] = self.func((x[i], y[j]))\n",
    "        fig = plt.figure(figsize=(4,4))\n",
    "        ax = fig.add_subplot(111)\n",
    "        sc = ax.scatter(x=xx.ravel(), y=yy.ravel(), c=zz.ravel())\n",
    "        ax.scatter(x=[self.optimal[0]], y=[self.optimal[1]], c='red', marker='x')\n",
    "        plt.colorbar(sc)\n",
    "        fig.show()\n",
    "        return ax\n",
    "    def visualise3d(self, lim, n):\n",
    "        x, y = np.linspace(-lim, lim, n), np.linspace(-lim, lim, n)\n",
    "        z = []\n",
    "        for i in y:\n",
    "            z_line = []\n",
    "            for j in x:\n",
    "                z_line.append(self.func([j,i]))\n",
    "            z.append(z_line)\n",
    "        fig = go.Figure(data=[go.Surface(z=z, x=x, y=y),  \\\n",
    "                              go.Scatter3d(x=[self.optimal[0]], y=[self.optimal[1]], z=[self.optimum])])\n",
    "        fig.update_layout(autosize=False,\n",
    "                          scene_camera_eye=dict(x=1.87, y=0.88, z=-0.64),\n",
    "                          width=500, height=500,\n",
    "                          margin=dict(l=65, r=50, b=65, t=90))\n",
    "        fig.show()\n",
    "    def visualise_gradient(self, lim, n):\n",
    "        x, y = np.linspace(-lim, lim, n), np.linspace(-lim, lim, n)\n",
    "        xx, yy = np.meshgrid(x, y)\n",
    "        zz = np.zeros((n, n, 2))\n",
    "        for j in range(len(y)):\n",
    "            for i in range(len(x)):\n",
    "                zz[j, i, :] = self.dfunc([x[i], y[j]])\n",
    "        fig = plt.figure(figsize=(8,8))\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.quiver(xx,yy,zz[:,:,0],zz[:,:,1])\n",
    "        ax.scatter(x=[self.optimal[0]], y=[self.optimal[1]], c='red', marker='x')\n",
    "        fig.show()\n",
    "        return ax\n",
    "    def visualise2d_section(self, pos, dire):\n",
    "        fig = plt.figure(figsize=(4,4))\n",
    "        xs = np.linspace(-self.lim, self.lim, 301)\n",
    "        fs = []\n",
    "        if dire == 'x':\n",
    "            for x in xs:\n",
    "                fs.append(self.func([x, pos]))\n",
    "        else:\n",
    "            for x in xs:\n",
    "                fs.append(self.func([pos, x]))\n",
    "        plt.plot(xs, fs)\n",
    "        fig.show()\n",
    "    def visualise2d_section_gradient(self, pos, dire):\n",
    "        fig = plt.figure(figsize=(4,4))\n",
    "        xs = np.linspace(-self.lim, self.lim, 300)\n",
    "        dfs = []\n",
    "        if dire == 'x':\n",
    "            for x in xs:\n",
    "                dfs.append(self.dfunc([x, pos]))\n",
    "        else:\n",
    "            for x in xs:\n",
    "                dfs.append(self.dfunc([pos, x]))\n",
    "        dfs = np.array(dfs)\n",
    "        plt.plot(xs, dfs[:,0])\n",
    "        plt.plot(xs, dfs[:,1])\n",
    "        fig.show()\n",
    "        \n",
    "class ackley(objective_func):\n",
    "    def __init__(self, dim=2):\n",
    "        self.optimal = np.array([0, 0])\n",
    "        self.optimum = 0\n",
    "        self.lim = 25\n",
    "        self.dim = dim\n",
    "    def func(self, x):\n",
    "        '''\n",
    "        the period of local minimum along each axis is 1, integer coordinate (1,1), (2,3)... \n",
    "        x and y is interchangeable\n",
    "        global minimum is 0 with arguments x=y=0\n",
    "        local minimums far away from orgin are 20\n",
    "        supremum is 20 + e - 1/e = 22.35\n",
    "        symmetric along x=0, y=0, y=x lines\n",
    "        disappearing global gradient when far away from optimal\n",
    "        '''\n",
    "        arg1 = -0.2 * np.sqrt(np.power(x, 2).mean())\n",
    "        arg2 = np.cos(2*np.pi*x).mean()\n",
    "        return -20. * np.exp(arg1) - np.exp(arg2) + 20. + np.e\n",
    "    def dfunc(self, x):\n",
    "        if np.linalg.norm(x) == 0:\n",
    "            return x\n",
    "        arg1 = -0.2 * np.sqrt(np.power(x, 2).mean())\n",
    "        arg2 = np.cos(2*np.pi*x).mean()\n",
    "        g = lambda xx: -0.8 * xx / arg1 * np.exp(arg1) / self.dim + 2 * np.pi * np.sin(2 * np.pi * xx) * np.exp(arg2) / self.dim\n",
    "        return g(x)\n",
    "    def get_optimal(self):\n",
    "        return self.optimal\n",
    "    def get_optimum(self):\n",
    "        return self.optimum\n",
    "a = ackley()\n",
    "print(\"check func value: expected:\", a.get_optimum(), \", actual:\", a.func(a.get_optimal()))\n",
    "print(\"check gradient: expected:[0,0], actual:\",a.dfunc(a.get_optimal()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6253849384403627"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.func(np.ones(12,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*******starting optimisation from intitial point:  [12.34232 34.3412  34.3     23.3434 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([11.99985268, 33.99972505, 33.99985151, 22.99979022]),\n",
       " 19.915226349433443,\n",
       " {'status': None, 'evals': 163})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad = adam()\n",
    "optmizerParas = {'x0': np.array([12.34232, 34.3412, 34.3, 23.3434]),\n",
    "         'alpha': 0.01,\n",
    "         'beta_1': 0.9, \n",
    "         'beta_2': 0.999, \n",
    "         'epsilon': 1e-8, \n",
    "         'max_iter': 1000,\n",
    "         'tol': 1e-5,              \n",
    "         'verbose': True,\n",
    "         'record': False}\n",
    "ad.set_parameters(optmizerParas)\n",
    "ad.optimise(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*******starting optimisation from intitial point:  [12.34232 34.3412  34.3     23.3434 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([11.99993067, 33.99980343, 33.99980345, 22.99986705]),\n",
       " 19.91522607560959,\n",
       " {'status': None, 'evals': 27})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls = line_search()\n",
    "optmizerParas = {'x0': np.array([12.34232, 34.3412, 34.3, 23.3434]),\n",
    "         'alpha': 1,\n",
    "         'beta': 0.1, \n",
    "         'max_iter': 1000,\n",
    "         'tol': 1e-5,              \n",
    "         'verbose': True,\n",
    "         'record': False}\n",
    "ls.set_parameters(optmizerParas)\n",
    "ls.optimise(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ABC\n",
    "\n",
    "class optimizer(ABC):\n",
    "    @abstractmethod\n",
    "    def set_parameters(self, para):\n",
    "        '''\n",
    "        input: parameters, in dictionary\n",
    "        '''\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def optimise(self, objective_cls):\n",
    "        '''\n",
    "        input: objective function class\n",
    "        output: empirical found optimal, optimum, and statistics of procedure information\n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "class adjust_optimizer(optimizer):\n",
    "    def adjust(self, x0, obj):\n",
    "        self.x0 = x0\n",
    "        arg, val, stats = self.optimise(obj)\n",
    "        return arg, val, stats['evals']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cma_es(adjust_optimizer):\n",
    "    def __init__(self, dim=2):\n",
    "        self.dim = dim\n",
    "        paras = {'x0': np.zeros((dim,)),\n",
    "                 'std': np.ones((dim,)) * 3, \n",
    "                 'tol': 1e-5, \n",
    "                 'adjust_func': do_nothing(), \n",
    "                 'record': False, \n",
    "                 'verbose': False}\n",
    "        self.set_parameters(paras)\n",
    "    def set_parameters(self, paras):\n",
    "        self.paras = paras\n",
    "        self.x0 = paras['x0'] \n",
    "        self.std = paras['std']\n",
    "        self.tol = paras['tol']\n",
    "        self.adjust_func = paras['adjust_func']\n",
    "        self.max_iter = 400 if 'max_iter' not in paras.keys() else paras['max_iter']\n",
    "        # set none to use default value \n",
    "        self.cluster_size = None if 'cluster_size' not in paras.keys() else paras['cluster_size']\n",
    "        self.survival_size = None if 'survival_size' not in paras.keys() else paras['survival_size']\n",
    "        self.record = True if 'record' not in paras.keys() else paras['record']\n",
    "        self.verbose = True if 'verbose' not in paras.keys() else paras['verbose']\n",
    "    def optimise(self, obj):\n",
    "        '''\n",
    "        @param obj: objective function class instance\n",
    "        return arg: found minimum arguments\n",
    "               val: found minimum value\n",
    "               stats: collection of recorded statistics for post-analysis\n",
    "        '''                  \n",
    "        def update_mean(x):\n",
    "            return (weights @ x).reshape(dim, 1)\n",
    "        def update_ps(ps, sigma, C, mean, mean_old):\n",
    "            return (1 - cs) * ps + np.sqrt(cs * (2 - cs) * mueff) * invsqrtC @ (mean - mean_old) / sigma \n",
    "        def update_pc(pc, sigma, ps, mean, mean_old):\n",
    "            hsig = np.abs(ps) / np.sqrt(1 - (1 - cs)**(2 * iter_/lambda_)) / chiN < 1.4 + 2/(dim + 1)\n",
    "            return (1 - cc) * pc + hsig * np.sqrt(cc * (2 - cc) * mueff) * (mean - mean_old) / sigma\n",
    "        def update_C(C, pc, x, mean_old, sigma):\n",
    "            hsig = np.abs(ps) / np.sqrt(1 - (1 - cs)**(2 * iter_/lambda_)) / chiN < 1.4 + 2/(dim + 1)\n",
    "            artmp = (1 / sigma) * (x - mean_old.reshape(1, dim))\n",
    "            return (1 - c1 - cmu) * C + c1 * (pc * pc.T + (1 - hsig) * cc * (2 - cc) * C) + cmu * artmp.T @ np.diag(weights) @ artmp\n",
    "        def update_sigma(sigma, ps):\n",
    "            return sigma * np.exp((cs / damps) * (np.linalg.norm(ps)/ chiN - 1))\n",
    "        def is_not_moving(arg, val, pre_arg, pre_val, tol):\n",
    "            dis_arg = np.linalg.norm(arg - pre_arg)\n",
    "            dis_val = np.linalg.norm(val - pre_val)\n",
    "            return (dis_arg < tol and dis_val < tol*1e5) or (dis_val < tol and dis_arg < tol*1e5) \n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"\\n\\n*******starting optimisation from intitial mean: \", self.x0.ravel())\n",
    "        # User defined input parameters \n",
    "        dim = 2    \n",
    "        sigma = 0.3\n",
    "        D = self.std / sigma\n",
    "        mean = self.x0\n",
    "\n",
    "        # the size of solutions group\n",
    "        lambda_ = 4 + int(3 * np.log(dim)) if self.cluster_size == None else self.cluster_size  \n",
    "        # only best \"mu\" solutions are used to generate iterations\n",
    "        mu = int(lambda_ / 2) if self.survival_size == None else self.survival_size\n",
    "        # used to combine best \"mu\" solutions                                               \n",
    "        weights = np.log(mu + 1/2) - np.log(np.arange(mu) + 1) \n",
    "        weights = weights / np.sum(weights)     \n",
    "        mueff = np.sum(weights)**2 / np.sum(weights**2) \n",
    "\n",
    "        # Strategy parameter setting: Adaptation\n",
    "        # time constant for cumulation for C\n",
    "        cc = (4 + mueff / dim) / (dim + 4 + 2 * mueff / dim)  \n",
    "        # t-const for cumulation for sigma control\n",
    "        cs = (mueff + 2) / (dim + mueff + 5)  \n",
    "        # learning rate for rank-one update of C\n",
    "        c1 = 2 / ((dim + 1.3)**2 + mueff)    \n",
    "        # and for rank-mu update\n",
    "        cmu = min(1 - c1, 2 * (mueff - 2 + 1 / mueff) / ((dim + 2)**2 + mueff))  \n",
    "        # damping for sigma, usually close to 1  \n",
    "        damps = 1 + 2 * max(0, np.sqrt((mueff - 1)/( dim + 1)) - 1) + cs                                                                 \n",
    "\n",
    "        # Initialize dynamic (internal) strategy parameters and constants\n",
    "        # evolution paths for C and sigma\n",
    "        pc = np.zeros((dim, 1))     \n",
    "        ps = np.zeros((dim, 1)) \n",
    "        # B defines the coordinate system\n",
    "        B = np.eye(dim)       \n",
    "        # covariance matrix C\n",
    "        C = B * np.diag(D**2) * B.T \n",
    "        # C^-1/2 \n",
    "        invsqrtC = B * np.diag(D**-1) * B.T   \n",
    "        # expectation of ||N(0,I)|| == norm(randn(N,1)) \n",
    "        chiN = dim**0.5 * (1 - 1/(4 * dim) + 1 / (21 * dim**2))  \n",
    "\n",
    "        # --------------------  Initialization --------------------------------  \n",
    "        x, x_old, f = np.zeros((lambda_, dim)), np.zeros((lambda_, dim)), np.zeros((lambda_,))\n",
    "        stats = {}\n",
    "        stats['val'], stats['arg'] = [], []\n",
    "        stats['x_adjust'] = []\n",
    "        iter_eval, stats['evals_per_iter'] = np.zeros((lambda_, )), []\n",
    "        stats['mean'], stats['std'] = [], []\n",
    "        stats['status'] = None\n",
    "        iter_, eval_ = 0, 0\n",
    "\n",
    "        # initial data in record\n",
    "        for i in range(lambda_):\n",
    "            x[i] = (mean + np.random.randn(dim, 1)).ravel()\n",
    "            f[i] = obj.func(x[i])\n",
    "        idx = np.argsort(f)\n",
    "        x_ascending = x[idx]\n",
    "        if self.record:\n",
    "            stats['arg'].append(x_ascending)\n",
    "            stats['val'].append(f[idx])\n",
    "            stats['mean'].append(mean)\n",
    "            stats['std'].append(sigma * B @ np.diag(D))\n",
    "            stats['evals_per_iter'].append(np.ones((lambda_,)))\n",
    "            stats['x_adjust'].append(np.vstack((x.T.copy(), x.T.copy())))\n",
    "        arg = x_ascending\n",
    "        val = f[idx]\n",
    "        pre_arg = x_ascending\n",
    "        pre_val = f[idx]\n",
    "        \n",
    "        # optimise by iterations\n",
    "        try:\n",
    "            while iter_ < self.max_iter:\n",
    "                iter_ += 1\n",
    "                \n",
    "                # generate candidate solutions with some stochastic elements\n",
    "                for i in range(lambda_):\n",
    "                    x[i] = (mean + sigma * B @ np.diag(D) @ np.random.randn(dim, 1)).ravel() \n",
    "                    x_old[i] = x[i]\n",
    "                    x[i], f[i], eval_cnt = self.adjust_func.adjust(x[i], obj)\n",
    "                    eval_ += eval_cnt\n",
    "                    iter_eval[i] = eval_cnt\n",
    "                # sort the value and positions of solutions \n",
    "                idx = np.argsort(f)\n",
    "                x_ascending = x[idx]\n",
    "\n",
    "                # update the parameter for next iteration\n",
    "                mean_old = mean\n",
    "                mean = update_mean(x_ascending[:mu])\n",
    "                ps =   update_ps(ps, sigma, C, mean, mean_old)\n",
    "                pc =   update_pc(pc, sigma, ps, mean, mean_old)\n",
    "                sigma = update_sigma(sigma, ps)\n",
    "                C =    update_C(C, pc, x_ascending[:mu], mean_old, sigma)\n",
    "                C = np.triu(C) + np.triu(C, 1).T\n",
    "                D, B = np.linalg.eig(C)\n",
    "                D = np.sqrt(D)\n",
    "                invsqrtC = B @ np.diag(D**-1) @ B\n",
    "\n",
    "                # record data during process for post analysis\n",
    "                if self.record:\n",
    "                    stats['arg'].append(x_ascending)\n",
    "                    stats['val'].append(f[idx])\n",
    "                    stats['mean'].append(mean)\n",
    "                    stats['std'].append(sigma * B @ np.diag(D))\n",
    "                    stats['evals_per_iter'].append(iter_eval.copy())\n",
    "                    stats['x_adjust'].append(np.vstack((x_old.T.copy(), x.T.copy())))\n",
    "                # stopping condition    \n",
    "                arg = x_ascending\n",
    "                val = f[idx]\n",
    "                \n",
    "                # check the stop condition\n",
    "                if np.max(D) > (np.min(D) * 1e6):\n",
    "                    stats['status'] = 'diverge'\n",
    "                    print('diverge, concentrate in low dimension manifold')\n",
    "                    break\n",
    "                if is_not_moving(arg, val, pre_arg, pre_val, self.tol) :\n",
    "                    break\n",
    "                pre_arg = arg\n",
    "                pre_val = val\n",
    "        except np.linalg.LinAlgError as err:\n",
    "            stats['status'] = 'diverge'\n",
    "            print('diverge, raise LinAlgError!')\n",
    "        finally:\n",
    "            if self.verbose:\n",
    "                print('eigenvalue of variance = {}'.format(D))\n",
    "                print('total iterations = {}, total evaluatios = {}'.format(iter_, eval_))\n",
    "                print('found minimum position = {}, found minimum = {}'.format(arg[0], val[0]))\n",
    "\n",
    "        # carry statistics info before quit\n",
    "        if self.record:\n",
    "            stats['arg'] = np.array(stats['arg'])\n",
    "            stats['val'] = np.array(stats['val'])\n",
    "            stats['mean'] = np.array(stats['mean'])\n",
    "            stats['std'] = np.array(stats['std'])\n",
    "            stats['evals_per_iter'] = np.array(stats['evals_per_iter'])\n",
    "            stats['x_adjust'] = np.array(stats['x_adjust'])\n",
    "        stats['evals'] = eval_\n",
    "        return arg[0], val[0], stats\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-ada] *",
   "language": "python",
   "name": "conda-env-miniconda3-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
