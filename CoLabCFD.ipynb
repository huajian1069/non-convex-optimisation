{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CoLabCFD.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-wgCgJ6AieVb",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "import math\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import skimage.measure\n",
        "import plyfile\n",
        "from plyfile import PlyData\n",
        "from sklearn.neighbors import KDTree\n",
        "import trimesh\n",
        "import torch_geometric\n",
        "from torch_geometric.nn import (NNConv, GMMConv, GraphConv, Set2Set)\n",
        "from torch_geometric.nn import (SplineConv, graclus, max_pool, max_pool_x, global_mean_pool)\n",
        "import matplotlib as mpl\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DtBoNNSMieVi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "cae7ae6d-15fb-46b4-ff53-16569f0bc230"
      },
      "source": [
        "print(torch.__version__)\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.5.0+cu101\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnQzjn9XaOAI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install torch==1.5.0+cu101 torchvision==0.6.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_U0sLodQi_fS",
        "colab": {}
      },
      "source": [
        "!pip install plyfile\n",
        "!pip install torch-scatter==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html \n",
        "!pip install trimesh\n",
        "!pip install torch-sparse==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html -D_GLIBCXX_USE_CXX11_ABI=1\n",
        "!pip install torch-cluster==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html \n",
        "!pip install torch-spline-conv==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html \n",
        "!pip install torch-geometric -f https://pytorch-geometric.com/whl/torch-1.5.0.html "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLmgVqj1lULM",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Zuy6igvMnjMZ",
        "colab": {}
      },
      "source": [
        "#! mkdir networks\n",
        "#! mkdir data_for_this_experiments\n",
        "#! mv specs.json data_for_this_experiments\n",
        "#! mv cfdModel.nn data_for_this_experiments\n",
        "#! mv avg_trans_matrix.npy data_for_this_experiments\n",
        "#! mv decoderModel.pth data_for_this_experiments\n",
        "#! mv ../deep_sdf_decoder.py data_for_this_experiments/\n",
        "#! mv ../latentCodes.pth data_for_this_experiments/"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-S7b4I2sVkR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5vlOE0_iieV8",
        "colab": {}
      },
      "source": [
        "def get_pressure_predictor(path):\n",
        "    model = SplineCNN8Residuals(3)\n",
        "    model.load_state_dict(torch.load(experiment_directory + \"/cfdModel.nn\"))\n",
        "    model = model.to(\"cuda:0\").eval()\n",
        "    return model\n",
        "\n",
        "def load_latent_vectors(experiment_directory, checkpoint):\n",
        "\n",
        "    filename = os.path.join(\n",
        "        experiment_directory, checkpoint + \".pth\"\n",
        "    )\n",
        "    if not os.path.isfile(filename):\n",
        "        raise Exception(\n",
        "            \"The experiment directory ({}) does not include a latent code file\"\n",
        "            + \" for checkpoint '{}'\".format(experiment_directory, checkpoint)\n",
        "        )\n",
        "    data = torch.load(filename)\n",
        "    return data[\"latent_codes\"].cuda()\n",
        "\n",
        "def load_model(experiment_directory, checkpoint):\n",
        "    specs_filename = os.path.join(experiment_directory, \"specs.json\")\n",
        "\n",
        "    if not os.path.isfile(specs_filename):\n",
        "        raise Exception(\n",
        "            'The experiment directory does not include specifications file \"specs.json\"'\n",
        "        )\n",
        "\n",
        "    specs = json.load(open(specs_filename))\n",
        "\n",
        "    arch = __import__(experiment_directory + \".\" + specs[\"NetworkArch\"], fromlist=[\"Decoder\"])\n",
        "\n",
        "    latent_size = specs[\"CodeLength\"]\n",
        "\n",
        "    decoder = arch.Decoder(latent_size, **specs[\"NetworkSpecs\"])\n",
        "\n",
        "    decoder = torch.nn.DataParallel(decoder)\n",
        "\n",
        "    saved_model_state = torch.load(\n",
        "        os.path.join(experiment_directory, checkpoint + \".pth\")\n",
        "    )\n",
        "\n",
        "    decoder.load_state_dict(saved_model_state[\"model_state_dict\"])\n",
        "\n",
        "    decoder = decoder.module.cuda()\n",
        "\n",
        "    decoder.eval()\n",
        "    \n",
        "    return decoder"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-JM2oaapieWC",
        "colab": {}
      },
      "source": [
        "class SplineBlock(nn.Module):\n",
        "    def __init__(self, num_in_features, num_outp_features, mid_features, kernel=3, dim=3, batchnorm1=True):\n",
        "        super(SplineBlock, self).__init__()\n",
        "        self.batchnorm1 = batchnorm1\n",
        "        self.conv1 = SplineConv(num_in_features, mid_features, dim, kernel, is_open_spline=False)\n",
        "        if self.batchnorm1:\n",
        "            self.batchnorm1 = torch.nn.BatchNorm1d(mid_features)\n",
        "        self.conv2 = SplineConv(mid_features, 2 * mid_features, dim, kernel, is_open_spline=False)\n",
        "        self.batchnorm2 = torch.nn.BatchNorm1d(2 * mid_features)\n",
        "        self.conv3 = SplineConv(2 * mid_features + 3, num_outp_features, dim, kernel, is_open_spline=False)\n",
        "  \n",
        "    def forward(self, res, data):\n",
        "        if self.batchnorm1:\n",
        "            res = F.elu(self.batchnorm1(self.conv1(res, data['edge_index'], data['edge_attr'])))\n",
        "        else:\n",
        "            res = F.elu(self.conv1(res, data['edge_index'], data['edge_attr']))\n",
        "        res = F.elu(self.batchnorm2(self.conv2(res, data['edge_index'], data['edge_attr'])))\n",
        "#         res = F.elu(self.conv2(res, data.edge_index, data.edge_attr))\n",
        "        res = torch.cat([res, data['x']], dim=1)\n",
        "        res = self.conv3(res, data['edge_index'], data['edge_attr'])\n",
        "        return res\n",
        "\n",
        "class SplineCNN8Residuals(nn.Module):\n",
        "    def __init__(self, num_features, kernel=3, dim=3):\n",
        "        super(SplineCNN8Residuals, self).__init__()\n",
        "        self.block1 = SplineBlock(num_features, 16, 8, kernel, dim)\n",
        "        self.block2 = SplineBlock(16, 64, 32, kernel, dim)\n",
        "        self.block3 = SplineBlock(64, 64, 128, kernel, dim)\n",
        "        self.block4 = SplineBlock(64, 8, 16, kernel, dim)\n",
        "        self.block5 = SplineBlock(11, 32, 16, kernel, dim)\n",
        "        self.block6 = SplineBlock(32, 64, 32, kernel, dim)\n",
        "        self.block7 = SplineBlock(64, 64, 128, kernel, dim)\n",
        "        self.block8 = SplineBlock(75, 4, 16, kernel, dim)\n",
        "\n",
        "    def forward(self, data):\n",
        "        res = data['x']\n",
        "        res = self.block1(res, data)\n",
        "        res = self.block2(res, data)\n",
        "        res = self.block3(res, data)\n",
        "        res4 = self.block4(res, data)\n",
        "        res = torch.cat([res4, data['x']], dim=1)\n",
        "        res = self.block5(res, data)\n",
        "        res = self.block6(res, data)\n",
        "        res = self.block7(res, data)\n",
        "        res = torch.cat([res, res4, data['x']], dim=1)\n",
        "        res = self.block8(res, data)\n",
        "        return res\n",
        "\n",
        "def create_mesh(\n",
        "    decoder, latent_vec, filename='', N=256, max_batch=32 ** 3, offset=None, scale=None\n",
        "):\n",
        "    start = time.time()\n",
        "    ply_filename = filename\n",
        "\n",
        "    decoder.eval()\n",
        "\n",
        "    # NOTE: the voxel_origin is actually the (bottom, left, down) corner, not the middle\n",
        "    voxel_origin = [-1, -1, -1]\n",
        "    voxel_size = 2.0 / (N - 1)\n",
        "\n",
        "    overall_index = torch.arange(0, N ** 3, 1, out=torch.LongTensor())\n",
        "    samples = torch.zeros(N ** 3, 4)\n",
        "\n",
        "    # transform first 3 columns\n",
        "    # to be the x, y, z index\n",
        "    samples[:, 2] = overall_index % N\n",
        "    samples[:, 1] = (overall_index.long() / N) % N\n",
        "    samples[:, 0] = ((overall_index.long() / N) / N) % N\n",
        "\n",
        "    # transform first 3 columns\n",
        "    # to be the x, y, z coordinate\n",
        "    samples[:, 0] = (samples[:, 0] * voxel_size) + voxel_origin[2]\n",
        "    samples[:, 1] = (samples[:, 1] * voxel_size) + voxel_origin[1]\n",
        "    samples[:, 2] = (samples[:, 2] * voxel_size) + voxel_origin[0]\n",
        "\n",
        "    num_samples = N ** 3\n",
        "\n",
        "    samples.requires_grad = False\n",
        "\n",
        "    head = 0\n",
        "\n",
        "    while head < num_samples:\n",
        "        sample_subset = samples[head : min(head + max_batch, num_samples), 0:3].cuda()\n",
        "\n",
        "        samples[head : min(head + max_batch, num_samples), 3] = \\\n",
        "                decode_sdf(decoder, latent_vec, sample_subset).squeeze(1).detach().cpu()\n",
        "        head += max_batch\n",
        "\n",
        "    sdf_values = samples[:, 3]\n",
        "    sdf_values = sdf_values.reshape(N, N, N)\n",
        "\n",
        "    end = time.time()\n",
        "    #print(\"sampling takes: %f\" % (end - start))\n",
        "\n",
        "    return convert_sdf_samples_to_ply(\n",
        "        sdf_values.data.cpu(),\n",
        "        voxel_origin,\n",
        "        voxel_size,\n",
        "        ply_filename + \".ply\",\n",
        "        offset,\n",
        "        scale,\n",
        "    )\n",
        "\n",
        "def convert_sdf_samples_to_ply(\n",
        "    pytorch_3d_sdf_tensor,\n",
        "    voxel_grid_origin,\n",
        "    voxel_size,\n",
        "    ply_filename_out,\n",
        "    offset=None,\n",
        "    scale=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Convert sdf samples to .ply\n",
        "\n",
        "    :param pytorch_3d_sdf_tensor: a torch.FloatTensor of shape (n,n,n)\n",
        "    :voxel_grid_origin: a list of three floats: the bottom, left, down origin of the voxel grid\n",
        "    :voxel_size: float, the size of the voxels\n",
        "    :ply_filename_out: string, path of the filename to save to\n",
        "\n",
        "    This function adapted from: https://github.com/RobotLocomotion/spartan\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    numpy_3d_sdf_tensor = pytorch_3d_sdf_tensor.numpy()\n",
        "\n",
        "    verts, faces, normals, values = skimage.measure.marching_cubes_lewiner(\n",
        "        numpy_3d_sdf_tensor, level=0.0, spacing=[voxel_size] * 3\n",
        "    )\n",
        "\n",
        "    # transform from voxel coordinates to camera coordinates\n",
        "    # note x and y are flipped in the output of marching_cubes\n",
        "    mesh_points = np.zeros_like(verts)\n",
        "    mesh_points[:, 0] = voxel_grid_origin[0] + verts[:, 0]\n",
        "    mesh_points[:, 1] = voxel_grid_origin[1] + verts[:, 1]\n",
        "    mesh_points[:, 2] = voxel_grid_origin[2] + verts[:, 2]\n",
        "\n",
        "    # apply additional offset and scale\n",
        "    if scale is not None:\n",
        "        mesh_points = mesh_points / scale\n",
        "    if offset is not None:\n",
        "        mesh_points = mesh_points - offset\n",
        "\n",
        "    # try writing to the ply file\n",
        "\n",
        "    num_verts = verts.shape[0]\n",
        "    num_faces = faces.shape[0]\n",
        "\n",
        "    verts_tuple = np.zeros((num_verts,), dtype=[(\"x\", \"f4\"), (\"y\", \"f4\"), (\"z\", \"f4\")])\n",
        "    norms_tuple = np.zeros((num_verts,), dtype=[(\"x\", \"f4\"), (\"y\", \"f4\"), (\"z\", \"f4\")])\n",
        "\n",
        "    for i in range(0, num_verts):\n",
        "        verts_tuple[i] = tuple(mesh_points[i, :])\n",
        "        norms_tuple[i] = tuple(normals[i, :])\n",
        "\n",
        "    faces_building = []\n",
        "    for i in range(0, num_faces):\n",
        "        faces_building.append(((faces[i, :].tolist(),)))\n",
        "    faces_tuple = np.array(faces_building, dtype=[(\"vertex_indices\", \"i4\", (3,))])\n",
        "\n",
        "    el_verts = plyfile.PlyElement.describe(verts_tuple, \"vertex\")\n",
        "    el_faces = plyfile.PlyElement.describe(faces_tuple, \"face\")\n",
        "    el_norms = plyfile.PlyElement.describe(norms_tuple, \"normals\")\n",
        "\n",
        "    ply_data = plyfile.PlyData([el_verts, el_faces, el_norms])\n",
        "    return ply_data\n",
        "\n",
        "def compute_lift_faces_diff(data_instance, answers, axis=0):\n",
        "    pressures = torch.mean(answers[data_instance['face'], 0], axis=0)\n",
        "\n",
        "    # TODO: cahnge to x if needed\n",
        "    pos = data_instance['x']\n",
        "    cross_prod = (pos[data_instance['face'][1]] - pos[data_instance['face'][0]]).cross(\n",
        "                  pos[data_instance['face'][2]] - pos[data_instance['face'][0]])\n",
        "    mult = -cross_prod[:, axis] / 2\n",
        "    lift = torch.mul(pressures, mult)\n",
        "    return torch.sum(lift[~torch.isnan(lift)])\n",
        "\n",
        "def boundsLoss(points, box=[(-1, 1, 0)]):\n",
        "    loss = 0\n",
        "    for l, r, i in box:\n",
        "        loss +=  torch.mean(F.relu(-points[:, i] + l))  \\\n",
        "               + torch.mean(F.relu( points[:, i] - r))\n",
        "    return loss\n",
        "\n",
        "def innerBoundsLoss(points, r=1, center=(0, 0, 0)):\n",
        "    radiuses = torch.sum( (points - torch.Tensor(center).to('cuda:0')) ** 2 , dim=1)\n",
        "    return torch.mean(F.relu(r - radiuses))\n",
        "\n",
        "def calculate_loss(mesh, local_preds, axis=0, constraint_rad=0.1):\n",
        "    loss =  (1 - axis) * compute_lift_faces_diff(mesh, local_preds, axis=0) + \\\n",
        "                  axis * compute_lift_faces_diff(mesh, local_preds, axis=1)\n",
        "    print(\"first part of loss: \", loss)\n",
        "    loss += boundsLoss(mesh['x'], box=[(-0.6, 0.6, 0)])\n",
        "    print(\"second part of loss: \", loss)\n",
        "    loss += innerBoundsLoss(mesh['x'], r=constraint_rad**2, center=(-0.05, 0.05, 0))  \\\n",
        "          + innerBoundsLoss(mesh['x'], r=(constraint_rad / 2)**2, center=(0.3, 0, 0))\n",
        "    print(\"third part of loss: \", loss)\n",
        "    return loss\n",
        "\n",
        "def computeAvgTransform():\n",
        "    objects = list()\n",
        "    for (dirpath, dirnames, filenames) in os.walk(\"/cvlabdata2/home/artem/Data/cars_remeshed_dsdf/transforms/\"):\n",
        "        objects += [os.path.join(dirpath, file) for file in filenames if file[-4:] == '.npy']\n",
        "    \n",
        "    matricies = []\n",
        "    for obj in objects:\n",
        "        matricies.append(np.load(obj))\n",
        "    \n",
        "    return np.mean(np.array(matricies), axis=0)\n",
        "\n",
        "def transformPoints(points, AvgTransform):\n",
        "    matrix = torch.cuda.FloatTensor(AvgTransform)\n",
        "    column = torch.zeros((len(points), 1), device=\"cuda:0\") + 1\n",
        "    stacked = torch.cat([points, column], dim=1)\n",
        "    transformed = torch.matmul(matrix, stacked.t()).t()[:, :3]\n",
        "    return transformed\n",
        "\n",
        "def transform_mesh(points, ply_mesh, AvgTransform):\n",
        "    transformed_points = transformPoints(points, AvgTransform)\n",
        "    \n",
        "    edges = trimesh.geometry.faces_to_edges(ply_mesh['face']['vertex_indices'])\n",
        "    np_points = transformed_points.cpu().detach().numpy()\n",
        "    edge_attr = [np_points[a] - np_points[b] for a, b in edges]\n",
        "    mesh = {'x': transformed_points, \n",
        "        'face':torch.tensor(ply_mesh['face']['vertex_indices'], dtype=torch.long).to('cuda:0').t(),\n",
        "        'edge_attr':torch.tensor(edge_attr, dtype=torch.float).to('cuda:0'),\n",
        "        'edge_index':torch.tensor(edges, dtype=torch.long).t().contiguous().to('cuda:0')\n",
        "        }\n",
        "    return mesh\n",
        "\n",
        "def get_trimesh_from_torch_geo_with_colors(mesh, preds, vmin=-8, vmax=8):\n",
        "    norm = mpl.colors.Normalize(vmin= vmin, vmax=vmax)\n",
        "    cmap = cm.hot\n",
        "    m = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
        "    \n",
        "    verticies = mesh['x'].cpu().detach()\n",
        "    faces = mesh['face'].t().cpu().detach()\n",
        "    return trimesh.Trimesh(vertices=verticies, faces=faces, \n",
        "                           vertex_colors=list(map(lambda c: m.to_rgba(c),  preds[:, 0].cpu().detach())))\n",
        "\n",
        "def decode_sdf(decoder, latent_vector, queries):\n",
        "    num_samples = queries.shape[0]\n",
        "\n",
        "    if latent_vector is None:\n",
        "        inputs = queries\n",
        "    else:\n",
        "        latent_repeat = latent_vector.expand(num_samples, -1)\n",
        "        inputs = torch.cat([latent_repeat, queries], 1)\n",
        "\n",
        "    sdf = decoder(inputs)\n",
        "\n",
        "    return sdf"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTdMQeF6Uxx8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-I_hhsIwieWF",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zw1U5NI-ieWJ",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OwDmuXRZieWL",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "j20y4chdieWN",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zovrDIQKieWZ",
        "colab": {}
      },
      "source": [
        "def method4_to_arbitatry_loss(points, ply_mesh, model, constraint_rad=0.1, axis=0):\n",
        "    initial_dir = points.grad.clone()\n",
        "    points.grad.data.zero_()\n",
        "\n",
        "    mesh = transform_mesh(points, ply_mesh, AvgTransform)\n",
        "    #signs = compute_signs_for_loss(mesh, transformPoints(normals, AvgTransform))\n",
        "    local_preds = model(mesh)\n",
        "    loss = calculate_loss(mesh, local_preds, axis=axis, constraint_rad=constraint_rad)\n",
        "    loss.backward()\n",
        "    print(\"normal\", torch.norm(initial_dir))\n",
        "    print(\"dL_dp_i:\", torch.norm(points.grad))\n",
        "    sign = [-p1.dot(p2) for p1, p2 in zip(initial_dir, points.grad)]\n",
        "    \n",
        "    return sign, loss, local_preds, mesh\n",
        "\n",
        "\n",
        "\n",
        "def optimize_shape_deepSDF(decoder, latent, initial_points=None, num_points=None, \n",
        "                           num_iters=100, point_iters=100, num_neignours_constr=10,\n",
        "                           lr=0.2, decreased_by=2, adjust_lr_every=10, alpha_penalty=0.05,\n",
        "                           multiplier_func=method4_to_arbitatry_loss, verbose=None, save_to_dir=None, N=256):\n",
        "\n",
        "    def adjust_learning_rate(\n",
        "        initial_lr, optimizer, num_iterations, decreased_by, adjust_lr_every\n",
        "    ):\n",
        "        lr = initial_lr * ((1 / decreased_by) ** (num_iterations // adjust_lr_every)) \\\n",
        "                        * ((punch_lr_at_reindex_by) ** (num_iterations // reindex_latent_each))\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group[\"lr\"] = lr\n",
        "            \n",
        "        return lr\n",
        "    \n",
        "    if not os.path.exists(os.path.join(save_to_dir, 'meshes')):\n",
        "        os.makedirs(os.path.join(save_to_dir, 'meshes'))\n",
        "    if not os.path.exists(os.path.join(save_to_dir, 'predictions')):\n",
        "        os.makedirs(os.path.join(save_to_dir, 'predictions'))\n",
        "\n",
        "    ref_latent = latent.clone().detach()\n",
        "    decoder.eval()\n",
        "    latent = latent.clone()\n",
        "    latent.requires_grad = True\n",
        "    optimizer = torch.optim.SGD([latent], lr=lr)\n",
        "\n",
        "    loss_plot = []\n",
        "    latent_dist = []\n",
        "    lr_plot = []\n",
        "    latent_plot = []\n",
        "\n",
        "    for i in range(num_iters):\n",
        "\n",
        "        time_start = time.time()\n",
        "\n",
        "        save_path = os.path.join(save_to_dir, 'meshes/' + str(i).zfill(5) + \".ply\")\n",
        "        preds_save_path = os.path.join(save_to_dir, 'predictions/' + str(i).zfill(5) + \".npy\")\n",
        "\n",
        "\n",
        "        cur_rl = adjust_learning_rate(lr, optimizer, i, decreased_by, adjust_lr_every)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            ply_mesh = create_mesh( decoder,\n",
        "                                    latent,\n",
        "                                    N=N,\n",
        "                                    max_batch=int(2 ** 18),\n",
        "                                    offset=None,\n",
        "                                    scale=None)\n",
        "\n",
        "        points = torch.cuda.FloatTensor(np.hstack(( ply_mesh['vertex']['x'][:, None], \n",
        "                                                    ply_mesh['vertex']['y'][:, None], \n",
        "                                                    ply_mesh['vertex']['z'][:, None])))\n",
        "\n",
        "        points.requires_grad = True\n",
        "\n",
        "        sdf_value = decode_sdf(decoder, latent, points)\n",
        "        sdf_value.backward(torch.ones([len(points), 1], dtype=torch.float32).cuda())\n",
        "\n",
        "        mults, loss_value, preds, transformed_mesh = multiplier_func(points, ply_mesh)         \n",
        "        multipliers = torch.cuda.FloatTensor(mults)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        sdf_value = torch.squeeze(decode_sdf(decoder, latent, points))\n",
        "\n",
        "        final_loss = torch.sum(sdf_value * multipliers)\n",
        "        final_loss.backward()\n",
        "\n",
        "        print(\"second sdf: \", torch.sum(sdf_value))\n",
        "        # Soft-constraints\n",
        "        distances, indeces = LATENT_KD_TREE.query(latent.cpu().detach(), k=num_neignours_constr)\n",
        "        penalty = torch.mean(\n",
        "                    torch.stack([torch.sum( \n",
        "                                    (latent - latent_vectors[indeces[0][i]]) ** 2\n",
        "                                 )\n",
        "                                 for i in range(len(indeces[0]))]\n",
        "                               )\n",
        "                    )\n",
        "        apenalty = penalty * alpha_penalty\n",
        "        apenalty.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "       \n",
        "\n",
        "        tri_mesh = get_trimesh_from_torch_geo_with_colors(transformed_mesh, preds)\n",
        "        tri_mesh.export(save_path)\n",
        "        #np.save(preds_save_path, preds.cpu().detach().numpy())\n",
        "\n",
        "        #if save_to_dir is not None:\n",
        "        #    plot_points_from_torch\n",
        "\n",
        "        loss_plot.append(loss_value.cpu().detach().numpy())\n",
        "        latent_dist.append(torch.sum((latent - ref_latent) ** 2 ).cpu().detach().numpy() )\n",
        "        latent_plot.append(latent.detach().cpu().numpy())\n",
        "        lr_plot.append(penalty)\n",
        "\n",
        "        time_end = time.time()\n",
        "\n",
        "        if verbose is not None and i % verbose == 0:\n",
        "            print('Iter ', i, 'Loss: ', loss_value.detach().cpu().numpy(), ' penality: ', apenalty)\n",
        "            print('gradient ', torch.norm(latent.grad))\n",
        "            print(\"multiplier, \", torch.norm(multipliers.detach()), \" backward loss\", final_loss)\n",
        "            print(\"final backward loss\", apenalty + final_loss)\n",
        "    \n",
        "        np.save(os.path.join(save_to_dir, \"latent_plot.npy\"), latent_plot)    \n",
        "        np.save(os.path.join(save_to_dir, \"loss_plot.npy\"), loss_plot)\n",
        "        np.save(os.path.join(save_to_dir, \"latent_dist.npy\"), latent_dist)\n",
        "        np.save(os.path.join(save_to_dir, \"lr_plot.npy\"), lr_plot)\n",
        "\n",
        "\n",
        "\n",
        "def make_full_transformation(initial_latent, experiment_name, \n",
        "                             decoder, model, alpha_penalty=0.05, constraint_rad=0.1, axis=0, **kwargs):\n",
        "    '''\n",
        "    kwargs:\n",
        "        num_iters=1000, \n",
        "        adjust_lr_every=10, \n",
        "        decreased_by=1.2,\n",
        "        lr=0.005\n",
        "        verbose=10,\n",
        "    '''\n",
        "\n",
        "    #ref_points = get_points_from_latent(decoder, ref_latent, N=128)\n",
        "    save_to_dir = experiment_name\n",
        "    if not os.path.exists(save_to_dir):\n",
        "        os.makedirs(save_to_dir)\n",
        "\n",
        "    #np.save(os.path.join(save_to_dir, \"target_verts.npy\"), ref_points)\n",
        "\n",
        "    optimize_shape_deepSDF(decoder, initial_latent, initial_points=None,\n",
        "                                           alpha_penalty=alpha_penalty,\n",
        "                                           num_points=None, point_iters=2,\n",
        "                                           multiplier_func=lambda x, y: \n",
        "                                               method4_to_arbitatry_loss(x, y, model, \n",
        "                                                                         constraint_rad=constraint_rad, \n",
        "                                                                         axis=axis),\n",
        "                                           save_to_dir=save_to_dir, **kwargs)\n",
        "   "
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgCGglB5-4cV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_loss(mesh, local_preds, axis=0, constraint_rad=0.1):\n",
        "    loss =  (1 - axis) * compute_lift_faces_diff(mesh, local_preds, axis=0) + \\\n",
        "                  axis * compute_lift_faces_diff(mesh, local_preds, axis=1)\n",
        "    print(\"first part of loss: \", loss)\n",
        "    loss += boundsLoss(mesh['x'], box=[(-0.6, 0.6, 0)])\n",
        "    print(\"second part of loss: \", loss)\n",
        "    loss += innerBoundsLoss(mesh['x'], r=constraint_rad**2, center=(-0.05, 0.05, 0))  \\\n",
        "          + innerBoundsLoss(mesh['x'], r=(constraint_rad / 2)**2, center=(0.3, 0, 0))\n",
        "    print(\"third part of loss: \", loss)\n",
        "    return loss"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPCUB5-3ACaU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lj_zVLWx44p1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "4d5b10cf-6747-491f-e4c1-9c93288d70b8"
      },
      "source": [
        "\n",
        "LATENT_TO_OPTIMIZE = latent_vectors[32]\n",
        "DIR_for_dump_data = './data_exp'\n",
        "punch_lr_at_reindex_by=1\n",
        "reindex_latent_each = 10000\n",
        "\n",
        "np.random.seed(101)\n",
        "make_full_transformation(LATENT_TO_OPTIMIZE.detach(),\n",
        "                         experiment_name=DIR_for_dump_data, decoder=decoder, model=predictor,\n",
        "                         alpha_penalty=0.2, axis=0,\n",
        "                         constraint_rad=0.05,\n",
        "                         num_iters=1,\n",
        "                         adjust_lr_every=20,\n",
        "                         decreased_by=1.1, \n",
        "                         lr=0.2,\n",
        "                         verbose=1,\n",
        "                         N=128,\n",
        "                         num_neignours_constr=10)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "first part of loss:  tensor(0.0442, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "second part of loss:  tensor(0.0442, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "third part of loss:  tensor(0.0442, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "normal tensor(145.0312, device='cuda:0')\n",
            "dL_dp_i: tensor(0.1497, device='cuda:0')\n",
            "second sdf:  tensor(-4.4190, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Iter  0 Loss:  0.044188626  penality:  tensor(0.0797, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "gradient  tensor(0.3828, device='cuda:0')\n",
            "multiplier,  tensor(0.0952, device='cuda:0')  backward loss tensor(-0.0005, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "final backward loss tensor(0.0793, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
            "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s-LdeJ1zieWi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "9a43c5ea-cd89-4cdc-d3ab-e7c58734399f"
      },
      "source": [
        "np.random.seed(101)\n",
        "\n",
        "LATENT_TO_OPTIMIZE = latent_vectors[32]\n",
        "LATENT_TO_OPTIMIZE.requires_grad = True\n",
        "cfd = cfd_obj(decoder, predictor, latent_vectors, AvgTransform, LATENT_KD_TREE)\n",
        "print(cfd.func(LATENT_TO_OPTIMIZE))\n",
        "%time torch.norm(cfd.dfunc(LATENT_TO_OPTIMIZE))"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "first part of loss:  tensor(0.0442, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "second part of loss:  tensor(0.0442, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "third part of loss:  tensor(0.0442, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.0442, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "if xyz original grad zero,  True\n",
            "if latent grad zero, 1st backward True\n",
            "if latent grad zero, 2nd backward True\n",
            "pred_sdf sum:  tensor(-4.4190, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "normal:  tensor(145.0312, device='cuda:0')\n",
            "dl_dx_i:  tensor(0.2815, device='cuda:0')\n",
            "multipliers:  tensor(0.1658, device='cuda:0') backward loss:  tensor(-0.0004, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "penalty:  tensor(0.0797, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "backward: final:  tensor(0.0793, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "CPU times: user 996 ms, sys: 739 ms, total: 1.74 s\n",
            "Wall time: 1.74 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.5642, device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f_H2o92JieWf",
        "colab": {}
      },
      "source": [
        "DIR_for_dump_data = 'data_for_this_experiments'\n",
        "experiment_directory = DIR_for_dump_data\n",
        "\n",
        "predictor = get_pressure_predictor(experiment_directory)\n",
        "\n",
        "decoder = load_model(experiment_directory, \"decoderModel\")\n",
        "\n",
        "latent_vectors = load_latent_vectors(experiment_directory, \"latentCodes\").detach()\n",
        "\n",
        "AvgTransform = np.load(DIR_for_dump_data + \"/avg_trans_matrix.npy\") #computeAvgTransform()\n",
        "\n",
        "LATENT_TO_OPTIMIZE = latent_vectors[32]\n",
        "LATENT_KD_TREE = KDTree(np.array([lv.cpu().detach().numpy()[0] for lv in latent_vectors]))\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iJJyuM71ieWb",
        "colab": {}
      },
      "source": [
        "class cfd_obj:\n",
        "    def __init__(self, decoder, p_predictor, latent_vectors, AvgTransform, LATENT_KD_TREE):\n",
        "        self.N_MARCHING_CUBE = 128\n",
        "        self.regl2 = 1e-3\n",
        "        self.iter = 0\n",
        "        self.quick = True\n",
        "        self.AvgTransform = AvgTransform\n",
        "        self.decoder = decoder\n",
        "        self.pressure_pred = p_predictor\n",
        "        self.constraint_rad = 0.05\n",
        "        self.num_neignours_constr = 10\n",
        "        self.latent_vectors = latent_vectors\n",
        "        self.LATENT_KD_TREE = LATENT_KD_TREE\n",
        "        self.alpha_penalty = 0.2\n",
        "    \n",
        "    def func(self, latent):\n",
        "        # from latent to xyz\n",
        "        ply_mesh = create_mesh(self.decoder, latent, N=self.N_MARCHING_CUBE, max_batch=int(2 ** 18))\n",
        "        points = torch.Tensor(np.hstack(( ply_mesh['vertex']['x'][:, None], \n",
        "                                            ply_mesh['vertex']['y'][:, None], \n",
        "                                            ply_mesh['vertex']['z'][:, None]))).cuda()\n",
        "        # from mesh to pressure field\n",
        "        #points.requires_grad = True\n",
        "        self.xyz_original = points\n",
        "        scaled_mesh = transform_mesh(points, ply_mesh, self.AvgTransform)\n",
        "        scaled_mesh['x'] = scaled_mesh['x'].detach().requires_grad_(True)\n",
        "        self.xyz_upstream = scaled_mesh['x'] \n",
        "        pressure_field = self.pressure_pred(scaled_mesh)\n",
        "        loss = calculate_loss(scaled_mesh, pressure_field, axis=0, constraint_rad=self.constraint_rad)\n",
        "        self.last_loss = loss\n",
        "        self.last_latent = latent\n",
        "        return loss\n",
        "    \n",
        "    def dfunc(self, latent):\n",
        "        if latent.grad is not None:\n",
        "            latent.grad.detach_()\n",
        "            latent.grad.zero_()\n",
        "        # step 1\n",
        "        if self.quick and self.last_latent is not None and torch.all(latent == self.last_latent):\n",
        "            loss = self.last_loss\n",
        "        else:\n",
        "            loss = self.func(latent)\n",
        "        loss.backward()\n",
        "        # TODO:   dL_dx_i = self.xyz_upstream.grad\n",
        "        dL_dx_i = self.xyz_upstream.grad\n",
        "        print(\"if xyz original grad zero, \", self.xyz_original.grad == None)\n",
        "        print(\"if latent grad zero, 1st backward\", latent.grad == None)\n",
        "        \n",
        "        # step 2\n",
        "        # calculate mesh normal\n",
        "        xyz = self.xyz_original\n",
        "        xyz.requires_grad = True\n",
        "        \n",
        "        #latent_inputs = latent.expand(xyz.shape[0], -1)\n",
        "        latent_inputs = latent.detach().expand(xyz.shape[0], -1)\n",
        "        inputs = torch.cat([latent_inputs, xyz], 1).cuda()      #Add .cuda() if you want to run on GPU\n",
        "        #first compute normals\n",
        "        pred_sdf = self.decoder(inputs)\n",
        "        \n",
        "        loss_normals = torch.sum(pred_sdf)\n",
        "        loss_normals.backward(retain_graph = True)\n",
        "        normals = xyz.grad#/torch.norm(xyz.grad, 2, 1).unsqueeze(-1)\n",
        "        print(\"if latent grad zero, 2nd backward\", latent.grad == None)\n",
        "\n",
        "        print(\"pred_sdf sum: \", loss_normals)\n",
        "        print(\"normal: \", torch.norm(normals))\n",
        "        print(\"dl_dx_i: \", torch.norm(dL_dx_i))\n",
        "        \n",
        "        # step 3\n",
        "        # now assemble inflow derivative\n",
        "        #latent.grad.zero_()\n",
        "        latent_inputs = latent.expand(xyz.shape[0], -1)\n",
        "        inputs = torch.cat([latent_inputs, xyz.detach()], 1).cuda()      #Add .cuda() if you want to run on GPU\n",
        "        pred_sdf = self.decoder(inputs)\n",
        "\n",
        "        multipliers = -torch.matmul(dL_dx_i.unsqueeze(1), normals.unsqueeze(-1)).squeeze(-1)\n",
        "        loss_backward = torch.sum(multipliers * pred_sdf)\n",
        "        print(\"multipliers: \", torch.norm(multipliers), \n",
        "              \"backward loss: \", loss_backward)\n",
        "        \n",
        "        # artificial loss\n",
        "        # Soft-constraints\n",
        "        distances, indeces = LATENT_KD_TREE.query(latent.cpu().detach(), k=self.num_neignours_constr)\n",
        "        apenalty = self.alpha_penalty * torch.sum((latent - latent_vectors[indeces.squeeze()]) ** 2, dim=2).mean()\n",
        "        loss_backward += apenalty\n",
        "        print(\"penalty: \", apenalty)\n",
        "        print(\"backward: final: \", loss_backward)\n",
        "        # Backpropagate\n",
        "        loss_backward.backward()\n",
        "        \n",
        "        return latent.grad"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wZ85ST4pieWl",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}