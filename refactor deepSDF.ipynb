{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Copyright 2004-present Facebook. All Rights Reserved.\n",
    "#  python optim.py -s example1/synth_test.json -e example1\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import deep_sdf\n",
    "import deep_sdf.workspace as ws\n",
    "\n",
    "import pdb\n",
    "\n",
    "from library.optimiser import *\n",
    "from library.objective_function import *\n",
    "from library.post_analysis import *\n",
    "from library.experiments import *\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(initial_lr, optimizer, num_iterations, decreased_by, adjust_lr_every):\n",
    "    lr = initial_lr * ((1 / decreased_by) ** (num_iterations // adjust_lr_every))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "\n",
    "def chamfer_distance(p1, p2):\n",
    "    '''\n",
    "    Calculate Chamfer Distance between two point sets\n",
    "    '''\n",
    "    p1 = p1.unsqueeze(0)\n",
    "    p2 = p2.unsqueeze(0)\n",
    "\n",
    "    p1 = p1.repeat(p2.size(1), 1, 1)\n",
    "    p1 = p1.transpose(0, 1)\n",
    "\n",
    "    p2 = p2.repeat(p1.size(0), 1, 1)\n",
    "\n",
    "    # compute distance tensor\n",
    "    dist = torch.add(p1, torch.neg(p2))\n",
    "    dist = torch.norm(dist, 2, dim=2)\n",
    "\n",
    "    dist1, _ = torch.min(dist, dim = 1)\n",
    "    dist2, _ = torch.min(dist, dim = 0)\n",
    "\n",
    "    return torch.mean(dist1) + torch.mean(dist2)\n",
    "\n",
    "class argms:\n",
    "    def __init__(self):\n",
    "        self.experiment_directory = \"example1\"\n",
    "        self.checkpoint = \"latest\"\n",
    "        self.iterations = 100\n",
    "        self.split_filename = \"example1/synth_test.json\"\n",
    "        self.logfile = None\n",
    "        self.debug = False\n",
    "        self.quiet = False\n",
    "args = argms()\n",
    "\n",
    "def getLatentSourceAndTarget(args, source_id, target_id):\n",
    "    # pick initialization and samples\n",
    "    # Load collection of all latent codes\n",
    "    all_codes_path = os.path.join(\n",
    "        args.experiment_directory,\n",
    "        ws.latent_codes_subdir,\n",
    "        'latest.pth')\n",
    "    all_codes = torch.load(all_codes_path)['latent_codes']['weight']\n",
    "    ## sphere\n",
    "    source_id = 999 # zywvjkvz2492e6xpq4hd1jzy2r9lht        # This will be the source shape (ie starting point)\n",
    "    latent = all_codes[source_id].unsqueeze(0).detach()#.cuda()   #Add .cuda() if you want to run on GPU\n",
    "    latent.requires_grad = True\n",
    "\n",
    "    # This is be the target shape (ie objective)\n",
    "    latent_target = all_codes[target_id].unsqueeze(0).detach()#.cuda()   #Add .cuda() if you want to run on GPU\n",
    "    return latent, latent_target\n",
    "\n",
    "def constructDecoder(args):\n",
    "    specs_filename = os.path.join(args.experiment_directory, \"specs.json\")\n",
    "    specs = json.load(open(specs_filename))\n",
    "    arch = __import__(\"networks.\" + specs[\"NetworkArch\"], fromlist=[\"Decoder\"])\n",
    "    latent_size = specs[\"CodeLength\"]\n",
    "    # Load decoder: this is our black box function\n",
    "    decoder = arch.Decoder(latent_size, **specs[\"NetworkSpecs\"])\n",
    "    decoder = torch.nn.DataParallel(decoder)\n",
    "    saved_model_state = torch.load(\n",
    "        os.path.join(\n",
    "            args.experiment_directory, ws.model_params_subdir, args.checkpoint + \".pth\"\n",
    "        ),\n",
    "        map_location=torch.device('cpu') # Remove this if you want to run on GPU\n",
    "    )\n",
    "    decoder.load_state_dict(saved_model_state[\"model_state_dict\"])\n",
    "    # Optionally: put decoder on GPU\n",
    "    #decoder = decoder.module.cuda()\n",
    "    return decoder\n",
    "\n",
    "class decoder_obj(objective_func):\n",
    "    def __init__(self, latent_target, decoder):\n",
    "        self.N_MARCHING_CUBE = 64\n",
    "        self.l2reg= True\n",
    "        self.regl2 = 1e-3\n",
    "        self.iter = 0\n",
    "        self.quick = False\n",
    "        \n",
    "        self.latent_target = latent_target\n",
    "        self.decoder = decoder\n",
    "        self.optimum = 0\n",
    "        self.optimal = latent_target\n",
    "        \n",
    "        # Get a mesh representation of the target shape\n",
    "        self.verts_target, faces_target = deep_sdf.mesh.create_mesh_optim(\n",
    "            decoder, latent_target, N=self.N_MARCHING_CUBE, max_batch=int(2 ** 18)\n",
    "        )\n",
    "    \n",
    "        \n",
    "    def func(self, latent):\n",
    "        # from latent to xyz\n",
    "        verts, faces = deep_sdf.mesh.create_mesh_optim(self.decoder, latent, N=self.N_MARCHING_CUBE, max_batch=int(2 ** 18))\n",
    "        verts = verts[torch.randperm(verts.shape[0])]\n",
    "        verts = verts[0:20000, :]\n",
    "        self.xyz_upstream = torch.tensor(verts.astype(float), requires_grad = True, dtype=torch.float32)#, device=torch.device('cuda:0')) # For GPU,\n",
    "       \n",
    "        # from latent_traget to xyz_target\n",
    "        verts_target_sample = self.verts_target[torch.randperm(self.verts_target.shape[0])]\n",
    "        verts_target_sample = verts_target_sample[0:20000, :]\n",
    "        xyz_target = torch.tensor(verts_target_sample.astype(float), requires_grad = False, dtype=torch.float32) # For GPU, add: , device=torch.device('cuda:0'))\n",
    "\n",
    "        # compare difference\n",
    "        loss = chamfer_distance(self.xyz_upstream, xyz_target)\n",
    "        self.last_loss = loss;\n",
    "        self.last_latent = latent;\n",
    "        return loss\n",
    "    \n",
    "    def dfunc(self, latent):\n",
    "        \n",
    "        if latent.grad is not None:\n",
    "            latent.grad.detach_()\n",
    "            latent.grad.zero_()\n",
    "        \n",
    "        # step 1\n",
    "        if self.quick and torch.norm(latent - self.last_latent):\n",
    "            loss = self.last_loss\n",
    "        else:\n",
    "            loss = self.func(latent)\n",
    "        decoder.eval()\n",
    "        loss.backward()\n",
    "        dL_dx_i = self.xyz_upstream.grad\n",
    "        \n",
    "        # step 2\n",
    "        # use vertices to compute full backward pass\n",
    "        xyz = self.xyz_upstream.clone().detach()\n",
    "        xyz.requires_grad = True\n",
    "        latent_inputs = latent.expand(xyz.shape[0], -1)\n",
    "        inputs = torch.cat([latent_inputs, xyz], 1)#.cuda()      #Add .cuda() if you want to run on GPU\n",
    "        #first compute normals\n",
    "        pred_sdf = self.decoder(inputs)\n",
    "        loss_normals = torch.sum(pred_sdf)\n",
    "        loss_normals.backward(retain_graph = True)\n",
    "        normals = xyz.grad/torch.norm(xyz.grad, 2, 1).unsqueeze(-1)\n",
    "                \n",
    "        # step 3\n",
    "        # now assemble inflow derivative\n",
    "        latent.grad.detach_()\n",
    "        latent.grad.zero_()\n",
    "        dL_ds_i_fast = -torch.matmul(dL_dx_i.unsqueeze(1), normals.unsqueeze(-1)).squeeze(-1)\n",
    "        loss_backward = torch.sum(dL_ds_i_fast * pred_sdf)\n",
    "        if l2reg and self.iter % 20 == 0 and self.iter > 0:\n",
    "            self.regl2 = self.regl2/2\n",
    "        if l2reg:\n",
    "            loss_backward += self.regl2 * torch.mean(latent.pow(2))\n",
    "        # Backpropagate\n",
    "        loss_backward.backward()\n",
    "        \n",
    "        return latent.grad\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    torch.manual_seed(0)\n",
    "    # 0 Initialization\n",
    "    N_MARCHING_CUBE = 64\n",
    "    lr= 8e-3\n",
    "    l2reg= True\n",
    "    regl2 = 1e-3\n",
    "    decreased_by = 1.5\n",
    "    adjust_lr_every = 50\n",
    "    \n",
    "    # 1 prepare data\n",
    "    ## sphere\n",
    "    source_id = 999 # zywvjkvz2492e6xpq4hd1jzy2r9lht        # This will be the source shape (ie starting point)\n",
    "    ## torus\n",
    "    target_id = 2 # 0bucd9ryckhaqtqvbiagilujeqzek4  \n",
    "    latent, latent_target = getLatentSourceAndTarget(args, source_id, target_id)\n",
    "    \n",
    "    # 2 prepare model\n",
    "    decoder = constructDecoder(args)\n",
    "    # 3 prepare optimiser\n",
    "    optimizer = torch.optim.Adam([latent], lr=lr)\n",
    "\n",
    "    losses = []\n",
    "    lambdas = []\n",
    "    \n",
    "\n",
    "    objectiveDe = decoder_obj(latent_target, decoder)\n",
    "\n",
    "    # Use Adam optimizer, with source as starting point, and a loss defined on meshes\n",
    "    # latent is the input of our function\n",
    "    print(\"Starting optimization:\")\n",
    "    for e in range(int(args.iterations)):\n",
    "        print(\"latent: \", latent.detach().numpy())\n",
    "        \n",
    "        loss = objectiveDe.func(latent)\n",
    "        losses.append(loss.detach().cpu().numpy()) \n",
    "        print(\"loss: \", loss.detach().numpy())\n",
    "        \n",
    "        grad = objectiveDe.dfunc(latent)\n",
    "        print(\"latent grad: \", grad.detach().numpy())\n",
    "\n",
    "        adjust_learning_rate(lr, optimizer, e, decreased_by, adjust_lr_every)\n",
    "        optimizer.step()\n",
    "        print(e, \"th iteration\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verts_target, faces_target = deep_sdf.mesh.create_mesh_optim(\n",
    "    decoder, latent_target, N=N_MARCHING_CUBE, max_batch=int(2 ** 18)\n",
    ")\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    torch.manual_seed(0)\n",
    "    # Initialization\n",
    "    N_MARCHING_CUBE = 64\n",
    "    lr= 8e-3\n",
    "    l2reg= True\n",
    "    regl2 = 1e-3\n",
    "    decreased_by = 1.5\n",
    "    adjust_lr_every = 50\n",
    "    \n",
    "\n",
    "    \n",
    "    # pick initialization and samples\n",
    "    # Load collection of all latent codes\n",
    "    all_codes_path = os.path.join(\n",
    "        args.experiment_directory,\n",
    "        ws.latent_codes_subdir,\n",
    "        'latest.pth')\n",
    "    all_codes = torch.load(all_codes_path)['latent_codes']['weight']\n",
    "    ## sphere\n",
    "    source_id = 999 # zywvjkvz2492e6xpq4hd1jzy2r9lht        # This will be the source shape (ie starting point)\n",
    "    latent = all_codes[source_id].unsqueeze(0).detach()#.cuda()   #Add .cuda() if you want to run on GPU\n",
    "    latent.requires_grad = True\n",
    "\n",
    "    ## torus\n",
    "    target_id = 2 # 0bucd9ryckhaqtqvbiagilujeqzek4          # This is be the target shape (ie objective)\n",
    "    latent_target = all_codes[target_id].unsqueeze(0).detach()#.cuda()   #Add .cuda() if you want to run on GPU\n",
    "\n",
    "     \n",
    "    \n",
    "    specs_filename = os.path.join(args.experiment_directory, \"specs.json\")\n",
    "    specs = json.load(open(specs_filename))\n",
    "    arch = __import__(\"networks.\" + specs[\"NetworkArch\"], fromlist=[\"Decoder\"])\n",
    "    latent_size = specs[\"CodeLength\"]\n",
    "    # Load decoder: this is our black box function\n",
    "    decoder = arch.Decoder(latent_size, **specs[\"NetworkSpecs\"])\n",
    "    decoder = torch.nn.DataParallel(decoder)\n",
    "    saved_model_state = torch.load(\n",
    "        os.path.join(\n",
    "            args.experiment_directory, ws.model_params_subdir, args.checkpoint + \".pth\"\n",
    "        ),\n",
    "        map_location=torch.device('cpu') # Remove this if you want to run on GPU\n",
    "    )\n",
    "    decoder.load_state_dict(saved_model_state[\"model_state_dict\"])\n",
    "    # Optionally: put decoder on GPU\n",
    "    #decoder = decoder.module.cuda()\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.Adam([latent], lr=lr)\n",
    "\n",
    "    losses2 = []\n",
    "    lambdas = []\n",
    "    \n",
    "    objective = decoder_obj(latent_target, decoder)\n",
    "\n",
    "    # Use Adam optimizer, with source as starting point, and a loss defined on meshes\n",
    "    # latent is the input of our function\n",
    "    print(\"Starting optimization:\")\n",
    "    for e in range(int(args.iterations)):\n",
    "\n",
    "        if latent.grad is not None:\n",
    "            latent.grad.detach_()\n",
    "            latent.grad.zero_()\n",
    "\n",
    "        verts, faces = deep_sdf.mesh.create_mesh_optim(decoder, latent, N=N_MARCHING_CUBE, max_batch=int(2 ** 18))\n",
    "\n",
    "        \n",
    "        # subsample vertices for gradients computations\n",
    "        verts = verts[torch.randperm(verts.shape[0])]\n",
    "        verts = verts[0:20000, :]\n",
    "        # forward pass within loss layer\n",
    "        xyz_upstream = torch.tensor(verts.astype(float), requires_grad = True, dtype=torch.float32)#, device=torch.device('cuda:0')) # For GPU,\n",
    "        # Get a point cloud sampling of the target shape\n",
    "        verts_target_sample = verts_target[torch.randperm(verts_target.shape[0])]\n",
    "        verts_target_sample = verts_target_sample[0:20000, :]\n",
    "        xyz_target = torch.tensor(verts_target_sample.astype(float), requires_grad = False, dtype=torch.float32) # For GPU, add: , device=torch.device('cuda:0'))\n",
    "        # At this point we have 2 outputs for decoder: the target xyz_target, and the current value xyz_upstream\n",
    "        # The following lines compute a loss and backpropagate\n",
    "        # compute loss function: Chamfer between current guess (xyz_upstream) and objective (xyz_target)\n",
    "        loss = chamfer_distance(xyz_upstream, xyz_target)\n",
    "        print(\"Loss at iter\", e, \":\", loss.item(), \", latent norm: \", torch.norm(latent))\n",
    "        \n",
    "        \n",
    "        losses2.append(loss.detach().cpu().numpy())                                  ## Loss value\n",
    "        lambdas.append(torch.norm(latent_target-latent).detach().cpu().numpy())     ## Distance in the domain\n",
    "        decoder.eval()\n",
    "        loss.backward()\n",
    "        dL_dx_i = xyz_upstream.grad\n",
    "        \n",
    "        # use vertices to compute full backward pass\n",
    "        xyz = torch.tensor(verts.astype(float), requires_grad = True, dtype=torch.float32)#, device=torch.device('cuda:0')) # For GPU,\n",
    "        latent_inputs = latent.expand(xyz.shape[0], -1)\n",
    "        inputs = torch.cat([latent_inputs, xyz], 1)#.cuda()      #Add .cuda() if you want to run on GPU\n",
    "        #first compute normals\n",
    "        pred_sdf = decoder(inputs)\n",
    "        loss_normals = torch.sum(pred_sdf)\n",
    "        loss_normals.backward(retain_graph = True)\n",
    "        normals = xyz.grad/torch.norm(xyz.grad, 2, 1).unsqueeze(-1)\n",
    "        \n",
    "        # now assemble inflow derivative\n",
    "        latent.grad.detach_()\n",
    "        latent.grad.zero_()\n",
    "        dL_ds_i_fast = -torch.matmul(dL_dx_i.unsqueeze(1), normals.unsqueeze(-1)).squeeze(-1)\n",
    "        loss_backward = torch.sum(dL_ds_i_fast * pred_sdf)\n",
    "        if e % 20 == 0 and e > 0:\n",
    "            regl2 = regl2/2\n",
    "        if l2reg:\n",
    "            loss_backward += regl2* torch.mean(latent.pow(2))\n",
    "        # Backpropagate\n",
    "        loss_backward.backward()\n",
    "\n",
    "        \n",
    "       # print(\"time to backward:\", end-start)\n",
    "        # update latent\n",
    "        # Explicit gradient is accessible via latent.grad\n",
    "        \n",
    "        adjust_learning_rate(lr, optimizer, e, decreased_by, adjust_lr_every)\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # 1. objective function value\n",
    "        print(\"loss backward:\", loss_backward.item())\n",
    "        # 2. its derivative function value on current arguments \n",
    "        print(\"\\n\")\n",
    "        #print(latent.grad)\n",
    "        #print(\"shape of verts_target, faces_target: \", verts_target.shape, faces_target.shape, xyz_target.shape)\n",
    "        #raise Exception(\"Stop\");\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cma_es(adjust_optimizer):\n",
    "    def __init__(self, dim=2):\n",
    "        self.dim = dim\n",
    "        paras = {'x0': torch.zeros((dim,)),\n",
    "                 'std': torch.ones((dim,)) * 3, \n",
    "                 'tol': 1e-5, \n",
    "                 'adjust_func': do_nothing(), \n",
    "                 'record': False, \n",
    "                 'verbose': False}\n",
    "        self.set_parameters(paras)\n",
    "    def set_parameters(self, paras):\n",
    "        self.paras = paras\n",
    "        self.x0 = paras['x0'] \n",
    "        self.std = paras['std']\n",
    "        self.tol = paras['tol']\n",
    "        self.adjust_func = paras['adjust_func']\n",
    "        self.max_iter = 400 if 'max_iter' not in paras.keys() else paras['max_iter']\n",
    "        # set none to use default value \n",
    "        self.cluster_size = None if 'cluster_size' not in paras.keys() else paras['cluster_size']\n",
    "        self.survival_size = None if 'survival_size' not in paras.keys() else paras['survival_size']\n",
    "        self.record = True if 'record' not in paras.keys() else paras['record']\n",
    "        self.verbose = True if 'verbose' not in paras.keys() else paras['verbose']\n",
    "    def optimise(self, obj):\n",
    "        '''\n",
    "        @param obj: objective function class instance\n",
    "        return arg: found minimum arguments\n",
    "               val: found minimum value\n",
    "               stats: collection of recorded statistics for post-analysis\n",
    "        '''                  \n",
    "        def update_mean(x):\n",
    "            return (weights @ x).reshape(dim, 1)\n",
    "        def update_ps(ps, sigma, C, mean, mean_old):\n",
    "            return (1 - cs) * ps + torch.sqrt(cs * (2 - cs) * mueff) * invsqrtC @ (mean - mean_old) / sigma \n",
    "        def update_pc(pc, sigma, ps, mean, mean_old):\n",
    "            hsig = (torch.norm(ps) / torch.sqrt(1 - (1 - cs)**(2 * iter_/lambda_)) / chiN < 1.4 + 2/(dim + 1)).int()\n",
    "            return (1 - cc) * pc + hsig * torch.sqrt(cc * (2 - cc) * mueff) * (mean - mean_old) / sigma\n",
    "        def update_C(C, pc, x, mean_old, sigma):\n",
    "            hsig = (torch.norm(ps) / torch.sqrt(1 - (1 - cs)**(2 * iter_/lambda_)) / chiN < (1.4 + 2/(dim + 1))).int()\n",
    "            artmp = (1 / sigma) * (x - mean_old.reshape(1, dim))\n",
    "            return (1 - c1 - cmu) * C + c1 * (pc * pc.T + (1 - hsig) * cc * (2 - cc) * C) + cmu * artmp.T @ torch.diag(weights) @ artmp\n",
    "        def update_sigma(sigma, ps):\n",
    "            return sigma * torch.exp((cs / damps) * (torch.norm(ps)/ chiN - 1))\n",
    "        def is_not_moving(arg, val, pre_arg, pre_val, tol):\n",
    "            dis_arg = torch.norm(arg - pre_arg, dim=1).mean()\n",
    "            dis_val = torch.abs(val - pre_val).mean()\n",
    "            return (dis_arg < tol and dis_val < tol) \n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"\\n\\n*******starting optimisation from intitial mean: \", self.x0.squeeze().detach().numpy())\n",
    "        # User defined input parameters \n",
    "        dim = self.dim\n",
    "        sigma = 0.3\n",
    "        D = self.std / sigma\n",
    "        mean = self.x0.reshape(dim, 1)\n",
    "        # the size of solutions group\n",
    "        lambda_ = 4 + int(3 * np.log(dim)) if self.cluster_size == None else self.cluster_size  \n",
    "        # only best \"mu\" solutions are used to generate iterations\n",
    "        mu = int(lambda_ / 2) if self.survival_size == None else self.survival_size\n",
    "        # used to combine best \"mu\" solutions                                               \n",
    "        weights = np.log(mu + 1/2) - torch.log(torch.arange(mu, dtype=torch.float) + 1) \n",
    "        weights = (weights / torch.sum(weights)).float()    \n",
    "        mueff = 1 / torch.sum(weights**2) \n",
    "\n",
    "        # Strategy parameter setting: Adaptation\n",
    "        # time constant for cumulation for C\n",
    "        cc = (4 + mueff / dim) / (dim + 4 + 2 * mueff / dim)  \n",
    "        # t-const for cumulation for sigma control\n",
    "        cs = (mueff + 2) / (dim + mueff + 5)  \n",
    "        # learning rate for rank-one update of C\n",
    "        c1 = 2 / ((dim + 1.3)**2 + mueff)    \n",
    "        # and for rank-mu update\n",
    "        cmu = min(1 - c1, 2 * (mueff - 2 + 1 / mueff) / ((dim + 2)**2 + mueff))  \n",
    "        # damping for sigma, usually close to 1  \n",
    "        damps = 1 + 2 * max(0, np.sqrt((mueff - 1)/( dim + 1)) - 1) + cs                                                                 \n",
    "\n",
    "        # Initialize dynamic (internal) strategy parameters and constants\n",
    "        # evolution paths for C and sigma\n",
    "        pc = torch.zeros((dim, 1))     \n",
    "        ps = torch.zeros((dim, 1)) \n",
    "        # B defines the coordinate system\n",
    "        B = torch.eye(int(dim))       \n",
    "        # covariance matrix C\n",
    "        C = B * torch.diag(D**2) * B.T \n",
    "        # C^-1/2 \n",
    "        invsqrtC = B * torch.diag(D**-1) * B.T   \n",
    "        # expectation of ||N(0,I)|| == norm(randn(N,1)) \n",
    "        chiN = dim**0.5 * (1 - 1/(4 * dim) + 1 / (21 * dim**2))  \n",
    "\n",
    "        # --------------------  Initialization --------------------------------  \n",
    "        x, x_old, f = torch.zeros((lambda_, dim)), torch.zeros((lambda_, dim)), torch.zeros((lambda_,))\n",
    "        stats = {}\n",
    "        inner_stats = {}\n",
    "        stats['inner'] = []\n",
    "        stats['val'], stats['arg'] = [], []\n",
    "        stats['x_adjust'] = []\n",
    "        iter_eval, stats['evals_per_iter'] = torch.zeros((lambda_, )), []\n",
    "        inner_stats = [{}] * lambda_\n",
    "        stats['mean'], stats['std'] = [], []\n",
    "        stats['status'] = None\n",
    "        iter_, eval_ = 0, 0\n",
    "        # initial data in record\n",
    "        for i in range(lambda_):\n",
    "            x[i,:] = (mean + 0.1 * torch.randn(dim, 1)).squeeze()\n",
    "            #f[i] = obj.func(x[i])\n",
    "            f[i] = torch.tensor([10])\n",
    "        idx = torch.argsort(f.detach())\n",
    "        x_ascending = x[idx]\n",
    "        if self.record:\n",
    "            stats['inner'].append(inner_stats.detach().numpy())\n",
    "            stats['arg'].append(x_ascending.detach().numpy())\n",
    "            stats['val'].append(f[idx].detach().numpy())\n",
    "            stats['mean'].append(mean.detach().numpy())\n",
    "            stats['std'].append(sigma * B @ torch.diag(D))\n",
    "            stats['evals_per_iter'].append(torch.ones((lambda_,)).detach().numpy())\n",
    "            stats['x_adjust'].append(np.vstack((x.T.clone().detach().numpy(), x.T.clone().detach().numpy())))\n",
    "        arg = x_ascending\n",
    "        val = f[idx]\n",
    "        pre_arg = x_ascending\n",
    "        pre_val = f[idx]\n",
    "        best_val = 1e4\n",
    "        best_arg = None\n",
    "        \n",
    "        # optimise by iterations\n",
    "        try:\n",
    "            while iter_ < self.max_iter:\n",
    "                iter_ += 1\n",
    "                # generate candidate solutions with some stochastic elements\n",
    "                for i in range(lambda_):\n",
    "                    x[i] = (mean + sigma * B @ torch.diag(D) @ torch.randn(dim, 1)).squeeze()\n",
    "                    x_old[i] = x[i]\n",
    "                    \n",
    "                    x[i], f[i], inner_stats[i] = self.adjust_func.adjust(x[i].clone().detach().requires_grad_(True), obj)\n",
    "                    eval_ += inner_stats[i]['evals']\n",
    "                    iter_eval[i] = inner_stats[i]['evals']\n",
    "                # sort the value and positions of solutions \n",
    "                idx = torch.argsort(f.detach())\n",
    "                x_ascending = x[idx]\n",
    "\n",
    "                # update the parameter for next iteration\n",
    "                mean_old = mean\n",
    "                mean = update_mean(x_ascending[:mu])\n",
    "                ps =   update_ps(ps, sigma, C, mean, mean_old)\n",
    "                pc =   update_pc(pc, sigma, ps, mean, mean_old)\n",
    "                sigma = update_sigma(sigma, ps)\n",
    "                C =    update_C(C, pc, x_ascending[:mu], mean_old, sigma)\n",
    "                C = torch.triu(C) + torch.triu(C, 1).T\n",
    "                D, B = torch.eig(C, eigenvectors=True)\n",
    "                D = torch.sqrt(D[:,0])\n",
    "                invsqrtC = B @ torch.diag(D**-1) @ B\n",
    "                arg = x_ascending\n",
    "                val = f[idx]\n",
    "                if self.verbose:\n",
    "                    print(\"iter: \", iter_)\n",
    "                    print(\"loss: \", val[0].detach().numpy())\n",
    "                    print(\"latent: \", x_ascending[0].detach().numpy())\n",
    "                    print(\"\\n\")\n",
    "                # record data during process for post analysis\n",
    "                if self.record:\n",
    "                    stats['inner'].append(inner_stats.clone().detach().numpy())\n",
    "                    stats['arg'].append(x_ascending.detach().numpy())\n",
    "                    stats['val'].append(f[idx].detach().numpy())\n",
    "                    stats['mean'].append(mean.detach().numpy())\n",
    "                    stats['std'].append((sigma * B @ np.diag(D)).detach().numpy())\n",
    "                    stats['evals_per_iter'].append(iter_eval.clone().detach().numpy())\n",
    "                    stats['x_adjust'].append(np.vstack((x_old.T.clone().detach().numpy(), x.T.clone().detach().numpy())))\n",
    "                # stopping condition    \n",
    "                if best_val > val[0]:\n",
    "                    best_val = val[0]\n",
    "                    best_arg = arg[0]              \n",
    "                # check the stop condition\n",
    "                if torch.max(D) > (torch.min(D) * 1e6):\n",
    "                    stats['status'] = 'diverge'\n",
    "                    print('diverge, concentrate in low dimension manifold')\n",
    "                    break\n",
    "                if is_not_moving(arg, val, pre_arg, pre_val, self.tol) :\n",
    "                    break\n",
    "                pre_arg = arg\n",
    "                pre_val = val\n",
    "        except np.linalg.LinAlgError as err:\n",
    "            stats['status'] = 'diverge'\n",
    "            print('diverge, raise LinAlgError!')\n",
    "        finally:\n",
    "            if self.verbose:\n",
    "                print('eigenvalue of variance = {}'.format(D))\n",
    "                print('total iterations = {}, total evaluatios = {}'.format(iter_, eval_))\n",
    "                print('found minimum position = {}, found minimum = {}'.format(best_arg.detach().numpy(), best_val.detach().numpy()))\n",
    "\n",
    "        # carry statistics info before quit\n",
    "        if self.record:\n",
    "            stats['arg'] = np.array(stats['arg'])\n",
    "            stats['val'] = np.array(stats['val'])\n",
    "            stats['mean'] = np.array(stats['mean'])\n",
    "            stats['std'] = np.array(stats['std'])\n",
    "            stats['evals_per_iter'] = np.array(stats['evals_per_iter'])\n",
    "            stats['x_adjust'] = np.array(stats['x_adjust'])\n",
    "        stats['evals'] = eval_\n",
    "        return best_arg, best_val, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class adam(adjust_optimizer):\n",
    "    def __init__(self, alpha=0.01, verbose=False, dim=2):\n",
    "        self.alpha = 0.01\n",
    "        self.beta_1 = 0.9\n",
    "        self.beta_2 = 0.999\n",
    "        self.epsilon = 1e-11\n",
    "        self.max_iter = 10000\n",
    "        self.tol = 1e-3\n",
    "        self.verbose = verbose\n",
    "        self.record = False\n",
    "        self.x0 = torch.zeros((dim,))\n",
    "        \n",
    "    def set_parameters(self, paras):\n",
    "        self.paras = paras\n",
    "        self.x0 = paras['x0']\n",
    "        self.alpha = paras['alpha']\n",
    "        self.beta_1 = paras['beta_1']\n",
    "        self.beta_2 = paras['beta_2']\n",
    "        self.epsilon = paras['epsilon']\n",
    "        self.max_iter = paras['max_iter']\n",
    "        self.tol = paras['tol']\n",
    "        self.verbose = True if 'verbose' not in paras.keys() else paras['verbose']\n",
    "        self.record = False if 'record' not in paras.keys() else paras['record']\n",
    "        \n",
    "    def optimise(self, obj):\n",
    "        m_t = 0 \n",
    "        v_t = 0 \n",
    "        eval_cnt = 0\n",
    "        x = self.x0\n",
    "        stats = {}\n",
    "        stats['status'] = None\n",
    "        stats['gradient_before_after'] = []\n",
    "        stats['arg'] = []\n",
    "        stats['val'] = []\n",
    "        if self.record:\n",
    "            stats['arg'].append(x.clone().detach().numpy())\n",
    "            stats['val'].append(obj.func(x).detach().numpy())\n",
    "            stats['gradient_before_after'].append([obj.dfunc(x).detach().numpy(), obj.dfunc(x).detach().numpy()])\n",
    "        if self.verbose:\n",
    "            print(\"\\n\\n*******starting optimisation from intitial point: \", self.x0.squeeze().detach().numpy())\n",
    "        while eval_cnt < self.max_iter:\t\t\t\t\t#till it gets converged\n",
    "            eval_cnt += 1\n",
    "            x = x.clone().detach().requires_grad_(True)\n",
    "            loss = obj.func(x)\n",
    "            g_t = obj.dfunc(x)\t\t#computes the gradient of the stochastic function\n",
    "            m_t = self.beta_1*m_t + (1-self.beta_1)*g_t\t#updates the moving averages of the gradient\n",
    "            v_t = self.beta_2*v_t + (1-self.beta_2)*(g_t*g_t)\t#updates the moving averages of the squared gradient\n",
    "            m_cap = m_t/(1-(self.beta_1**eval_cnt))\t\t#calculates the bias-corrected estimates\n",
    "            v_cap = v_t/(1-(self.beta_2**eval_cnt))\t\t#calculates the bias-corrected estimates\n",
    "            x_prev = x.clone()\t\t\t\t\t\t\t\t\n",
    "            est_df = (m_cap)/(torch.sqrt(v_cap)+self.epsilon)\n",
    "            with torch.no_grad():\n",
    "                x -= self.alpha * est_df \t#updates the parameters\n",
    "            if self.verbose:\n",
    "                print(\"iter: \", eval_cnt)\n",
    "                print(\"loss: \", loss.detach().numpy())\n",
    "                print(\"gradient: \", g_t.detach().numpy())\n",
    "                print(\"latent: \", x.detach().numpy())\n",
    "                print(\"\\n\")\n",
    "            if self.record:\n",
    "                stats['arg'].append(x.clone().detach().numpy())\n",
    "                stats['val'].append(obj.func(x).detach().numpy())\n",
    "                stats['gradient_before_after'].append([g_t.detach().numpy(), est_df.detach().numpy()])\n",
    "            if(torch.norm(x-x_prev) < self.tol):\t\t#checks if it is converged or not\n",
    "                break\n",
    "        if self.verbose:\n",
    "            print('total evaluatios = {}'.format(eval_cnt))\n",
    "            print('gradient at stop position = {},\\nmodified graident = {}'.format(g_t, est_df))\n",
    "            print('found minimum position = {}, found minimum = {}'.format(x.detach().numpy(), obj.func(x).detach().numpy()))\n",
    "        stats['arg'] = np.array(stats['arg'])\n",
    "        stats['val'] = np.array(stats['val'])\n",
    "        stats['gradient_before_after'] = np.array(stats['gradient_before_after'])\n",
    "        stats['evals'] = eval_cnt\n",
    "        return x, obj.func(x), stats\n",
    "    \n",
    "class line_search(adjust_optimizer):\n",
    "    def __init__(self, alpha=1, beta=0.1):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.max_iter = 100\n",
    "        self.tol = 1e-2\n",
    "        self.verbose = False\n",
    "        self.record = False\n",
    "     \n",
    "    def set_parameters(self, paras):\n",
    "        self.paras = paras\n",
    "        self.x0 = paras['x0']\n",
    "        self.alpha = paras['alpha']\n",
    "        self.beta = paras['beta']\n",
    "        self.max_iter = paras['max_iter']\n",
    "        self.tol = paras['tol']\n",
    "        self.verbose = True if 'verbose' not in paras.keys() else paras['verbose']\n",
    "        self.record = True if 'record' not in paras.keys() else paras['record']\n",
    "    def optimise(self, obj):\n",
    "        '''\n",
    "        @param x0: initial point position\n",
    "        @param alpha: initial step size\n",
    "        @param beta: control the armijo condition\n",
    "        @return x: point position after moving to local minimum\n",
    "        '''\n",
    "        x = self.x0\n",
    "        alpha_ = self.alpha\n",
    "        tao = 0.5\n",
    "        fx = obj.func(x)\n",
    "        p = - obj.dfunc(x)\n",
    "        fnx = obj.func(x + alpha_ * p)\n",
    "        eval_cnt = 3\n",
    "        stats = {}\n",
    "        stats['status'] = None\n",
    "        stats['gradient'] = []\n",
    "        stats['arg'] = []\n",
    "        stats['val'] = []\n",
    "        if self.record:\n",
    "            stats['arg'].append(x.clone().detach().numpy())\n",
    "            stats['val'].append(fx.detach().numpy())\n",
    "            stats['gradient'].append(-p.detach().numpy())\n",
    "        if self.verbose:\n",
    "            print(\"\\n*******starting optimisation from intitial point: \", self.x0.squeeze().detach().numpy())\n",
    "        for k in range(self.max_iter):\n",
    "            while fnx > fx + alpha_ * self.beta * (-p @ p):\n",
    "                alpha_ *= tao\n",
    "                fnx = obj.func(x + alpha_ * p)\n",
    "                eval_cnt += 1\n",
    "            with torch.no_grad():\n",
    "                x += alpha_ * p\n",
    "            fx = fnx\n",
    "            x = x.clone().detach().requires_grad_(True)\n",
    "            p = -obj.dfunc(x)\n",
    "            fnx = obj.func(x + alpha_ * p)\n",
    "            eval_cnt += 2\n",
    "            if self.record:\n",
    "                stats['arg'].append(x.clone().detach().numpy())\n",
    "                #print(eval_cnt, stats['arg'])\n",
    "                stats['val'].append(fx.detach().numpy())\n",
    "                stats['gradient'].append(-p.detach().numpy())\n",
    "            if torch.norm(p) < self.tol:\n",
    "                break\n",
    "        stats['evals'] = eval_cnt\n",
    "        if self.verbose:\n",
    "            print('total evaluatios = {}'.format(eval_cnt))\n",
    "            print('gradient at stop position = {}'.format(-p.detach().numpy()))\n",
    "            print('found minimum position = {}, found minimum = {}'.format(x.detach().numpy(), fx.detach().numpy()))\n",
    "        stats['arg'] = np.array(stats['arg'])\n",
    "        stats['val'] = np.array(stats['val'])\n",
    "        stats['gradient'] = np.array(stats['gradient'])\n",
    "        return x, fnx, stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init and setup one experiment\n",
    "exp = single_experiment()\n",
    "# One experiment: setup objective function\n",
    "objectiveDe = decoder_obj(latent_target, decoder)\n",
    "exp.set_objective(objectiveDe)\n",
    "opt = adam(dim=8)\n",
    "optParas = {\n",
    "         'x0': latent,\n",
    "         'alpha': 0.001,\n",
    "         'beta_1': 0.9, \n",
    "         'beta_2': 0.999, \n",
    "         'epsilon': 1e-11, \n",
    "         'max_iter': 100,\n",
    "         'tol': 1e-6,              \n",
    "         'verbose': True,\n",
    "         'record': False }\n",
    "opt.set_parameters(optParas)\n",
    "exp.set_optimizer(opt)\n",
    "exp.do()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init and setup one experiment\n",
    "exp = single_experiment()\n",
    "# One experiment: setup objective function\n",
    "objectiveDe = decoder_obj(latent_target, decoder)\n",
    "exp.set_objective(objectiveDe)\n",
    "opt = adam(dim=8)\n",
    "optParas = {\n",
    "         'x0': latent,\n",
    "         'alpha': 0.001,\n",
    "         'beta_1': 0.9, \n",
    "         'beta_2': 0.999, \n",
    "         'epsilon': 1e-11, \n",
    "         'max_iter': 100,\n",
    "         'tol': 1e-6,              \n",
    "         'verbose': True,\n",
    "         'record': False }\n",
    "opt.set_parameters(optParas)\n",
    "exp.set_optimizer(opt)\n",
    "exp.do()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "# 0 Initialization\n",
    "N_MARCHING_CUBE = 64\n",
    "lr= 8e-3\n",
    "l2reg= True\n",
    "regl2 = 1e-3\n",
    "decreased_by = 1.5\n",
    "adjust_lr_every = 50\n",
    "\n",
    "# 1 prepare data\n",
    "## sphere\n",
    "source_id = 999 # zywvjkvz2492e6xpq4hd1jzy2r9lht        # This will be the source shape (ie starting point)\n",
    "## torus\n",
    "target_id = 2 # 0bucd9ryckhaqtqvbiagilujeqzek4  \n",
    "latent, latent_target = getLatentSourceAndTarget(args, source_id, target_id)\n",
    "\n",
    "# 2 prepare model\n",
    "decoder = constructDecoder(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*******starting optimisation from intitial mean:  [-0.34123307 -0.47586045 -0.15240912  0.02312643  0.14862083  0.3323207\n",
      " -0.16005228 -0.12385195]\n",
      "iter:  1\n",
      "loss:  10.0\n",
      "latent:  [-0.3266223  -0.41677397 -0.12227314 -0.003187    0.17221963  0.277801\n",
      " -0.15744506 -0.09102364]\n",
      "\n",
      "\n",
      "iter:  2\n",
      "loss:  0.42412472\n",
      "latent:  [-0.31690824 -0.41054645 -0.10236986 -0.06019158  0.16955937  0.29262227\n",
      " -0.17669874 -0.09724864]\n",
      "\n",
      "\n",
      "iter:  3\n",
      "loss:  0.40884984\n",
      "latent:  [-0.30315596 -0.42242423 -0.14098278 -0.05712719  0.13897526  0.28454265\n",
      " -0.21493274 -0.06458235]\n",
      "\n",
      "\n",
      "iter:  4\n",
      "loss:  0.40071818\n",
      "latent:  [-0.26251101 -0.4189985  -0.07259955 -0.07264411  0.15222563  0.280191\n",
      " -0.22376229 -0.02680844]\n",
      "\n",
      "\n",
      "iter:  5\n",
      "loss:  0.36837342\n",
      "latent:  [-0.3184425  -0.35868132 -0.09833866 -0.1175178   0.10599276  0.2539092\n",
      " -0.18219654 -0.03696901]\n",
      "\n",
      "\n",
      "iter:  6\n",
      "loss:  0.3465619\n",
      "latent:  [-0.24295753 -0.3386955  -0.07619581 -0.09698155  0.18401587  0.20837115\n",
      " -0.24368806 -0.0746818 ]\n",
      "\n",
      "\n",
      "iter:  7\n",
      "loss:  0.32039294\n",
      "latent:  [-0.20645308 -0.2590712  -0.07499315 -0.15255357  0.13487476  0.21427545\n",
      " -0.19814855  0.00095421]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# init and setup one experiment\n",
    "exp = single_experiment()\n",
    "# One experiment: setup objective function\n",
    "objectiveDe = decoder_obj(latent_target, decoder)\n",
    "exp.set_objective(objectiveDe)\n",
    "opt = cma_es(dim=8)\n",
    "optParas ={'x0': latent,\n",
    "           'std': torch.ones((8,)) * 0.03, \n",
    "           'tol': 1e-3, \n",
    "           'adjust_func': do_nothing(), \n",
    "           'record': False, \n",
    "           'verbose': True}\n",
    "opt.set_parameters(optParas)\n",
    "exp.set_optimizer(opt)\n",
    "exp.do()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3412, -0.4759, -0.1524,  0.0231,  0.1486,  0.3323, -0.1601, -0.1239]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-ada] *",
   "language": "python",
   "name": "conda-env-miniconda3-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
