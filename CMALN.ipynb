{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import skimage.measure\n",
    "import plyfile\n",
    "from plyfile import PlyData\n",
    "from sklearn.neighbors import KDTree\n",
    "import trimesh\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import (NNConv, GMMConv, GraphConv, Set2Set)\n",
    "from torch_geometric.nn import (SplineConv, graclus, max_pool, max_pool_x, global_mean_pool)\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pressure_predictor(load_directory):\n",
    "    model = SplineCNN8Residuals(3)\n",
    "    model.load_state_dict(torch.load(load_directory + \"/cfdModel.nn\"))\n",
    "    model = model.to(\"cuda:0\").eval()\n",
    "    return model\n",
    "\n",
    "def load_latent_vectors(load_directory, checkpoint):\n",
    "    filename = os.path.join(\n",
    "        load_directory, checkpoint + \".pth\"\n",
    "    )\n",
    "    if not os.path.isfile(filename):\n",
    "        raise Exception(\n",
    "            \"The experiment directory ({}) does not include a latent code file\"\n",
    "            + \" for checkpoint '{}'\".format(load_directory, checkpoint)\n",
    "        )\n",
    "    data = torch.load(filename)\n",
    "    return data[\"latent_codes\"].cuda()\n",
    "\n",
    "def load_decoder(load_directory, checkpoint):\n",
    "    specs_filename = os.path.join(load_directory, \"specs.json\")\n",
    "    if not os.path.isfile(specs_filename):\n",
    "        raise Exception(\n",
    "            'The experiment directory does not include specifications file \"specs.json\"'\n",
    "        )\n",
    "    specs = json.load(open(specs_filename))\n",
    "    latent_size = specs[\"CodeLength\"]\n",
    "    decoder = Decoder(latent_size, **specs[\"NetworkSpecs\"])\n",
    "    decoder = torch.nn.DataParallel(decoder)\n",
    "    saved_model_state = torch.load(os.path.join(load_directory, checkpoint +\".pth\"))\n",
    "    decoder.load_state_dict(saved_model_state[\"model_state_dict\"])\n",
    "    decoder = decoder.module.cuda()\n",
    "    decoder.eval()\n",
    "    return decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class objective_func(ABC):\n",
    "    @abstractmethod\n",
    "    def func(self, x):\n",
    "        pass\n",
    "    def dfunc(self, x):\n",
    "        out = self.func(x)\n",
    "        out.backward()\n",
    "        return x.grad\n",
    "    def get_optimal(self):\n",
    "        return self.optimal\n",
    "    def get_optimum(self):\n",
    "        return self.optimum\n",
    "    \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        latent_size,\n",
    "        dims,\n",
    "        dropout=None,\n",
    "        dropout_prob=0.0,\n",
    "        norm_layers=(),\n",
    "        latent_in=(),\n",
    "        weight_norm=False,\n",
    "        xyz_in_all=None,\n",
    "        use_tanh=False,\n",
    "        latent_dropout=False,\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        def make_sequence():\n",
    "            return []\n",
    "\n",
    "        dims = [latent_size + 3] + dims + [1]\n",
    "\n",
    "        self.num_layers = len(dims)\n",
    "        self.norm_layers = norm_layers\n",
    "        self.latent_in = latent_in\n",
    "        self.latent_dropout = latent_dropout\n",
    "        if self.latent_dropout:\n",
    "            self.lat_dp = nn.Dropout(0.2)\n",
    "\n",
    "        self.xyz_in_all = xyz_in_all\n",
    "        self.weight_norm = weight_norm\n",
    "\n",
    "        for layer in range(0, self.num_layers - 1):\n",
    "            if layer + 1 in latent_in:\n",
    "                out_dim = dims[layer + 1] - dims[0]\n",
    "            else:\n",
    "                out_dim = dims[layer + 1]\n",
    "                if self.xyz_in_all and layer != self.num_layers - 2:\n",
    "                    out_dim -= 3\n",
    "\n",
    "            if weight_norm and layer in self.norm_layers:\n",
    "                setattr(\n",
    "                    self,\n",
    "                    \"lin\" + str(layer),\n",
    "                    nn.utils.weight_norm(nn.Linear(dims[layer], out_dim)),\n",
    "                )\n",
    "            else:\n",
    "                setattr(self, \"lin\" + str(layer), nn.Linear(dims[layer], out_dim))\n",
    "\n",
    "            if (\n",
    "                (not weight_norm)\n",
    "                and self.norm_layers is not None\n",
    "                and layer in self.norm_layers\n",
    "            ):\n",
    "                setattr(self, \"bn\" + str(layer), nn.LayerNorm(out_dim))\n",
    "\n",
    "        self.use_tanh = use_tanh\n",
    "        if use_tanh:\n",
    "            self.tanh = nn.Tanh()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.dropout = dropout\n",
    "        self.th = nn.Tanh()\n",
    "\n",
    "    # input: N x (L+3)\n",
    "    def forward(self, input):\n",
    "        xyz = input[:, -3:]\n",
    "\n",
    "        if input.shape[1] > 3 and self.latent_dropout:\n",
    "            latent_vecs = input[:, :-3]\n",
    "            latent_vecs = F.dropout(latent_vecs, p=0.2, training=self.training)\n",
    "            x = torch.cat([latent_vecs, xyz], 1)\n",
    "        else:\n",
    "            x = input\n",
    "\n",
    "        for layer in range(0, self.num_layers - 1):\n",
    "            lin = getattr(self, \"lin\" + str(layer))\n",
    "            if layer in self.latent_in:\n",
    "                x = torch.cat([x, input], 1)\n",
    "            elif layer != 0 and self.xyz_in_all:\n",
    "                x = torch.cat([x, xyz], 1)\n",
    "            x = lin(x)\n",
    "            # last layer Tanh\n",
    "            if layer == self.num_layers - 2 and self.use_tanh:\n",
    "                x = self.tanh(x)\n",
    "            if layer < self.num_layers - 2:\n",
    "                if (\n",
    "                    self.norm_layers is not None\n",
    "                    and layer in self.norm_layers\n",
    "                    and not self.weight_norm\n",
    "                ):\n",
    "                    bn = getattr(self, \"bn\" + str(layer))\n",
    "                    x = bn(x)\n",
    "                x = self.relu(x)\n",
    "                if self.dropout is not None and layer in self.dropout:\n",
    "                    x = F.dropout(x, p=self.dropout_prob, training=self.training)\n",
    "\n",
    "        if hasattr(self, \"th\"):\n",
    "            x = self.th(x)\n",
    "\n",
    "        return x\n",
    "class SplineBlock(nn.Module):\n",
    "    def __init__(self, num_in_features, num_outp_features, mid_features, kernel=3, dim=3, batchnorm1=True):\n",
    "        super(SplineBlock, self).__init__()\n",
    "        self.batchnorm1 = batchnorm1\n",
    "        self.conv1 = SplineConv(num_in_features, mid_features, dim, kernel, is_open_spline=False)\n",
    "        if self.batchnorm1:\n",
    "            self.batchnorm1 = torch.nn.BatchNorm1d(mid_features)\n",
    "        self.conv2 = SplineConv(mid_features, 2 * mid_features, dim, kernel, is_open_spline=False)\n",
    "        self.batchnorm2 = torch.nn.BatchNorm1d(2 * mid_features)\n",
    "        self.conv3 = SplineConv(2 * mid_features + 3, num_outp_features, dim, kernel, is_open_spline=False)\n",
    "  \n",
    "    def forward(self, res, data):\n",
    "        if self.batchnorm1:\n",
    "            res = F.elu(self.batchnorm1(self.conv1(res, data['edge_index'], data['edge_attr'])))\n",
    "        else:\n",
    "            res = F.elu(self.conv1(res, data['edge_index'], data['edge_attr']))\n",
    "        res = F.elu(self.batchnorm2(self.conv2(res, data['edge_index'], data['edge_attr'])))\n",
    "#         res = F.elu(self.conv2(res, data.edge_index, data.edge_attr))\n",
    "        res = torch.cat([res, data['x']], dim=1)\n",
    "        res = self.conv3(res, data['edge_index'], data['edge_attr'])\n",
    "        return res\n",
    "\n",
    "class SplineCNN8Residuals(nn.Module):\n",
    "    def __init__(self, num_features, kernel=3, dim=3):\n",
    "        super(SplineCNN8Residuals, self).__init__()\n",
    "        self.block1 = SplineBlock(num_features, 16, 8, kernel, dim)\n",
    "        self.block2 = SplineBlock(16, 64, 32, kernel, dim)\n",
    "        self.block3 = SplineBlock(64, 64, 128, kernel, dim)\n",
    "        self.block4 = SplineBlock(64, 8, 16, kernel, dim)\n",
    "        self.block5 = SplineBlock(11, 32, 16, kernel, dim)\n",
    "        self.block6 = SplineBlock(32, 64, 32, kernel, dim)\n",
    "        self.block7 = SplineBlock(64, 64, 128, kernel, dim)\n",
    "        self.block8 = SplineBlock(75, 4, 16, kernel, dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        res = data['x']\n",
    "        res = self.block1(res, data)\n",
    "        res = self.block2(res, data)\n",
    "        res = self.block3(res, data)\n",
    "        res4 = self.block4(res, data)\n",
    "        res = torch.cat([res4, data['x']], dim=1)\n",
    "        res = self.block5(res, data)\n",
    "        res = self.block6(res, data)\n",
    "        res = self.block7(res, data)\n",
    "        res = torch.cat([res, res4, data['x']], dim=1)\n",
    "        res = self.block8(res, data)\n",
    "        return res\n",
    "\n",
    "def create_mesh(\n",
    "    decoder, latent_vec, filename='', N=256, max_batch=32 ** 3, offset=None, scale=None\n",
    "):\n",
    "    ply_filename = filename\n",
    "\n",
    "    decoder.eval()\n",
    "\n",
    "    # NOTE: the voxel_origin is actually the (bottom, left, down) corner, not the middle\n",
    "    voxel_origin = [-1, -1, -1]\n",
    "    voxel_size = 2.0 / (N - 1)\n",
    "\n",
    "    overall_index = torch.arange(0, N ** 3, 1, out=torch.LongTensor())\n",
    "    samples = torch.zeros(N ** 3, 4)\n",
    "\n",
    "    # transform first 3 columns\n",
    "    # to be the x, y, z index\n",
    "    samples[:, 2] = overall_index % N\n",
    "    samples[:, 1] = (overall_index.long() // N) % N\n",
    "    samples[:, 0] = ((overall_index.long() // N) // N) % N\n",
    "\n",
    "    # transform first 3 columns\n",
    "    # to be the x, y, z coordinate\n",
    "    samples[:, 0] = (samples[:, 0] * voxel_size) + voxel_origin[2]\n",
    "    samples[:, 1] = (samples[:, 1] * voxel_size) + voxel_origin[1]\n",
    "    samples[:, 2] = (samples[:, 2] * voxel_size) + voxel_origin[0]\n",
    "\n",
    "    num_samples = N ** 3\n",
    "\n",
    "    samples.requires_grad = False\n",
    "    head = 0\n",
    "\n",
    "    while head < num_samples:\n",
    "        sample_subset = samples[head : min(head + max_batch, num_samples), 0:3].cuda()\n",
    "        num_subsample = min(max_batch, num_samples-head)\n",
    "        latent_repeat = latent_vec.expand(num_subsample, -1)\n",
    "        inputs = torch.cat([latent_repeat, sample_subset], 1)\n",
    "        samples[head : min(head + max_batch, num_samples), 3] = \\\n",
    "                decoder(inputs).squeeze(1).detach().cpu()\n",
    "        head += max_batch\n",
    "        \n",
    "    sdf_values = samples[:, 3].reshape(N, N, N).data.cpu()\n",
    "\n",
    "    return convert_sdf_samples_to_ply(\n",
    "        sdf_values,\n",
    "        voxel_origin,\n",
    "        voxel_size,\n",
    "        ply_filename + \".ply\",\n",
    "        offset,\n",
    "        scale,\n",
    "    )\n",
    "\n",
    "def convert_sdf_samples_to_ply(\n",
    "    pytorch_3d_sdf_tensor,\n",
    "    voxel_grid_origin,\n",
    "    voxel_size,\n",
    "    ply_filename_out,\n",
    "    offset=None,\n",
    "    scale=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Convert sdf samples to .ply\n",
    "\n",
    "    :param pytorch_3d_sdf_tensor: a torch.FloatTensor of shape (n,n,n)\n",
    "    :voxel_grid_origin: a list of three floats: the bottom, left, down origin of the voxel grid\n",
    "    :voxel_size: float, the size of the voxels\n",
    "    :ply_filename_out: string, path of the filename to save to\n",
    "\n",
    "    This function adapted from: https://github.com/RobotLocomotion/spartan\n",
    "    \"\"\"\n",
    "\n",
    "    numpy_3d_sdf_tensor = pytorch_3d_sdf_tensor.numpy()\n",
    "\n",
    "    verts, faces, normals, values = skimage.measure.marching_cubes_lewiner(\n",
    "        numpy_3d_sdf_tensor, level=0.0, spacing=[voxel_size] * 3\n",
    "    )\n",
    "\n",
    "    # transform from voxel coordinates to camera coordinates\n",
    "    # note x and y are flipped in the output of marching_cubes\n",
    "    mesh_points = np.zeros_like(verts)\n",
    "    mesh_points[:, 0] = voxel_grid_origin[0] + verts[:, 0]\n",
    "    mesh_points[:, 1] = voxel_grid_origin[1] + verts[:, 1]\n",
    "    mesh_points[:, 2] = voxel_grid_origin[2] + verts[:, 2]\n",
    "\n",
    "    # apply additional offset and scale\n",
    "    if scale is not None:\n",
    "        mesh_points = mesh_points / scale\n",
    "    if offset is not None:\n",
    "        mesh_points = mesh_points - offset\n",
    "\n",
    "    # try writing to the ply file\n",
    "\n",
    "    num_verts = verts.shape[0]\n",
    "    num_faces = faces.shape[0]\n",
    "\n",
    "    verts_tuple = np.zeros((num_verts,), dtype=[(\"x\", \"f4\"), (\"y\", \"f4\"), (\"z\", \"f4\")])\n",
    "    norms_tuple = np.zeros((num_verts,), dtype=[(\"x\", \"f4\"), (\"y\", \"f4\"), (\"z\", \"f4\")])\n",
    "\n",
    "    for i in range(0, num_verts):\n",
    "        verts_tuple[i] = tuple(mesh_points[i, :])\n",
    "        norms_tuple[i] = tuple(normals[i, :])\n",
    "\n",
    "    faces_building = []\n",
    "    for i in range(0, num_faces):\n",
    "        faces_building.append(((faces[i, :].tolist(),)))\n",
    "    faces_tuple = np.array(faces_building, dtype=[(\"vertex_indices\", \"i4\", (3,))])\n",
    "\n",
    "    el_verts = plyfile.PlyElement.describe(verts_tuple, \"vertex\")\n",
    "    el_faces = plyfile.PlyElement.describe(faces_tuple, \"face\")\n",
    "    el_norms = plyfile.PlyElement.describe(norms_tuple, \"normals\")\n",
    "\n",
    "    ply_data = plyfile.PlyData([el_verts, el_faces, el_norms])\n",
    "    return ply_data\n",
    "\n",
    "def compute_lift_faces_diff(mesh, preds):\n",
    "    pressures = torch.mean(preds[mesh['face'], 0], axis=0)\n",
    "\n",
    "    # TODO: cahnge to x if needed\n",
    "    pos = mesh['x']\n",
    "    cross_prod = (pos[mesh['face'][1]] - pos[mesh['face'][0]]).cross(\n",
    "                  pos[mesh['face'][2]] - pos[mesh['face'][0]])\n",
    "    area = -cross_prod[:, 0] / 2\n",
    "    lift = torch.mul(pressures, area)\n",
    "    return torch.sum(lift[~torch.isnan(lift)])\n",
    "\n",
    "def boundsLoss(points, box=[(-1, 1, 0)]):\n",
    "    loss = 0\n",
    "    for l, r, i in box:\n",
    "        loss +=  torch.mean(F.relu(-points[:, i] + l))  \\\n",
    "               + torch.mean(F.relu( points[:, i] - r))\n",
    "    return loss\n",
    "\n",
    "def innerBoundsLoss(points, r=1, center=(0, 0, 0)):\n",
    "    radiuses = torch.sum( (points - torch.Tensor(center).to('cuda:0')) ** 2 , dim=1)\n",
    "    return torch.mean(F.relu(r - radiuses))\n",
    "\n",
    "def calculate_loss(mesh, local_preds, constraint_rad=0.1):\n",
    "    loss = compute_lift_faces_diff(mesh, local_preds)\n",
    "    first = loss.clone().detach().cpu().numpy()\n",
    "    loss += boundsLoss(mesh['x'], box=[(-0.6, 0.6, 0)])\n",
    "    second = loss.clone().detach().cpu().numpy()\n",
    "    loss += innerBoundsLoss(mesh['x'], r=constraint_rad**2, center=(-0.05, 0.05, 0))  \\\n",
    "          + innerBoundsLoss(mesh['x'], r=(constraint_rad / 2)**2, center=(0.3, 0, 0))\n",
    "    print(\"three parts (321) of loss: %.3f, %.3f, %.3f\"%(loss.detach().cpu().numpy() - second, second-first, first))\n",
    "    return loss\n",
    "\n",
    "def transformPoints(points, AvgTransform):\n",
    "    matrix = torch.cuda.FloatTensor(AvgTransform)\n",
    "    column = torch.zeros((len(points), 1), device=\"cuda:0\") + 1\n",
    "    stacked = torch.cat([points, column], dim=1)\n",
    "    transformed = torch.matmul(matrix, stacked.t()).t()[:, :3]\n",
    "    return transformed\n",
    "\n",
    "def transform_mesh(points, ply_mesh, AvgTransform):\n",
    "    transformed_points = transformPoints(points, AvgTransform)\n",
    "    \n",
    "    edges = trimesh.geometry.faces_to_edges(ply_mesh['face']['vertex_indices'])\n",
    "    np_points = transformed_points.cpu().detach().numpy()\n",
    "    edge_attr = [np_points[a] - np_points[b] for a, b in edges]\n",
    "    mesh = {'x': transformed_points, \n",
    "        'face':torch.tensor(ply_mesh['face']['vertex_indices'], dtype=torch.long).to('cuda:0').t(),\n",
    "        'edge_attr':torch.tensor(edge_attr, dtype=torch.float).to('cuda:0'),\n",
    "        'edge_index':torch.tensor(edges, dtype=torch.long).t().contiguous().to('cuda:0')\n",
    "        }\n",
    "    return mesh\n",
    "\n",
    "\n",
    "def decode_sdf(decoder, latent_vector, queries):\n",
    "    num_samples = queries.shape[0]\n",
    "\n",
    "    if latent_vector is None:\n",
    "        inputs = queries\n",
    "    else:\n",
    "        latent_repeat = latent_vector.expand(num_samples, -1)\n",
    "        inputs = torch.cat([latent_repeat, queries], 1)\n",
    "\n",
    "    sdf = decoder(inputs)\n",
    "\n",
    "    return sdf\n",
    "class single_experiment:\n",
    "    def __init__(self, tol=0.1):\n",
    "        self.tol = tol\n",
    "    def set_objective(self, objective_func):\n",
    "        self.objective_func = objective_func\n",
    "\n",
    "    def set_optimizer(self, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def do(self):\n",
    "        optimal, optimum, statistics = self.optimizer.optimise(self.objective_func)\n",
    "        dist_arg = np.linalg.norm(optimal.detach().cpu().numpy() - self.objective_func.get_optimal())\n",
    "        dist_val = np.linalg.norm(optimum.detach().cpu().numpy() - self.objective_func.get_optimum())\n",
    "        if  dist_arg < self.tol \\\n",
    "        or  dist_val < self.tol:\n",
    "            statistics['status'] = 'global minimum'\n",
    "        elif statistics['status'] != 'diverge':\n",
    "            statistics['status'] = 'local minimum'\n",
    "        print(\"distance domain, codomain: \", dist_arg, dist_val)\n",
    "        if self.optimizer.verbose:\n",
    "            print(\"Result: \", statistics['status'])\n",
    "            print(\"found minimum: {}, minimum position: {}, evals: {}\".format(optimum, torch.norm(optimal).item(), statistics['evals']))\n",
    "        if self.optimizer.record == False:\n",
    "            return statistics['status'], optimum, optimal, statistics['evals']\n",
    "        else:\n",
    "            statistics['optimal'] = self.objective_func.get_optimal()\n",
    "            statistics['optimum'] = self.objective_func.get_optimum()\n",
    "            statistics['found_optimal'] = optimal\n",
    "            statistics['found_optimum'] = optimum\n",
    "            return statistics\n",
    "            \n",
    "\n",
    "def get_trimesh_from_torch_geo_with_colors(mesh, preds, vmin=-8, vmax=8):\n",
    "    norm = mpl.colors.Normalize(vmin= vmin, vmax=vmax)\n",
    "    cmap = cm.hot\n",
    "    m = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "    \n",
    "    verticies = mesh['x'].cpu().detach()\n",
    "    faces = mesh['face'].t().cpu().detach()\n",
    "    return trimesh.Trimesh(vertices=verticies, faces=faces, \n",
    "                           vertex_colors=list(map(lambda c: m.to_rgba(c),  preds[:, 0].cpu().detach())))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_to_load_data = 'starting_data'\n",
    "experiment_directory = \"data_for_this_experiments\"\n",
    "\n",
    "predictor = load_pressure_predictor(DIR_to_load_data)\n",
    "\n",
    "decoder = load_decoder(DIR_to_load_data, \"decoderModel\")\n",
    "\n",
    "latent_vectors = load_latent_vectors(DIR_to_load_data, \"latentCodes\").detach()\n",
    "\n",
    "AvgTransform = np.load(DIR_to_load_data + \"/avg_trans_matrix.npy\") #computeAvgTransform()\n",
    "\n",
    "LATENT_TO_OPTIMIZE = latent_vectors[32]\n",
    "LATENT_KD_TREE = KDTree(np.array([lv.cpu().detach().numpy()[0] for lv in latent_vectors]))\n",
    "# /cvlabdata2/home/artem/Data/cars_remeshed_dsdf/transforms/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual_Mesh(ilatent, N):\n",
    "    ply_mesh = create_mesh(decoder,\n",
    "                        ilatent,\n",
    "                        N=N,\n",
    "                        max_batch=int(2 ** 8))\n",
    "    points = torch.cuda.FloatTensor(np.hstack(( ply_mesh['vertex']['x'][:, None], \n",
    "                                            ply_mesh['vertex']['y'][:, None], \n",
    "                                            ply_mesh['vertex']['z'][:, None])))\n",
    "    scaled_mesh = transform_mesh(points, ply_mesh, AvgTransform)\n",
    "    pressure_field = predictor(scaled_mesh)\n",
    "    loss = compute_lift_faces_diff(scaled_mesh, pressure_field)  \n",
    "    print(\"latent loss. %f \"%(loss))\n",
    "    return get_trimesh_from_torch_geo_with_colors(scaled_mesh, pressure_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class cma_es():\n",
    "    def __init__(self, dim=2):\n",
    "        self.dim = dim\n",
    "        paras = {'x0': torch.zeros((dim,)),\n",
    "                 'std': torch.ones((dim,)) * 3, \n",
    "                 'tol': 1e-5, \n",
    "                 'adjust_func': None, \n",
    "                 'record': False, \n",
    "                 'verbose': False}\n",
    "        self.set_parameters(paras)\n",
    "    def set_parameters(self, paras):\n",
    "        self.paras = paras\n",
    "        self.x0 = paras['x0'] \n",
    "        self.std = paras['std']\n",
    "        self.tol = paras['tol']\n",
    "        self.adjust_func = paras['adjust_func']\n",
    "        self.max_iter = 400 if 'max_iter' not in paras.keys() else paras['max_iter']\n",
    "        # set none to use default value \n",
    "        self.cluster_size = None if 'cluster_size' not in paras.keys() else paras['cluster_size']\n",
    "        self.survival_size = None if 'survival_size' not in paras.keys() else paras['survival_size']\n",
    "        self.record = True if 'record' not in paras.keys() else paras['record']\n",
    "        self.verbose = True if 'verbose' not in paras.keys() else paras['verbose']\n",
    "    def optimise(self, obj):\n",
    "        '''\n",
    "        @param obj: objective function class instance\n",
    "        return arg: found minimum arguments\n",
    "               val: found minimum value\n",
    "               stats: collection of recorded statistics for post-analysis\n",
    "        '''                  \n",
    "        def update_mean(x):\n",
    "            return (weights @ x).reshape(dim, 1)\n",
    "        def update_ps(ps, sigma, C, mean, mean_old):\n",
    "            return (1 - cs) * ps + torch.sqrt(cs * (2 - cs) * mueff) * invsqrtC @ (mean - mean_old) / sigma \n",
    "        def update_pc(pc, sigma, ps, mean, mean_old):\n",
    "            hsig = (torch.norm(ps) / torch.sqrt(1 - (1 - cs)**(2 * iter_/lambda_)) / chiN < 1.4 + 2/(dim + 1)).int()\n",
    "            return (1 - cc) * pc + hsig * torch.sqrt(cc * (2 - cc) * mueff) * (mean - mean_old) / sigma\n",
    "        def update_C(C, pc, x, mean_old, sigma):\n",
    "            hsig = (torch.norm(ps) / torch.sqrt(1 - (1 - cs)**(2 * iter_/lambda_)) / chiN < (1.4 + 2/(dim + 1))).int()\n",
    "            artmp = (1 / sigma) * (x - mean_old.reshape(1, dim))\n",
    "            return (1 - c1 - cmu) * C + c1 * (pc * pc.transpose(1,0) + (1 - hsig) * cc * (2 - cc) * C) + cmu * artmp.transpose(1,0) @ torch.diag(weights) @ artmp\n",
    "        def update_sigma(sigma, ps):\n",
    "            return sigma * torch.exp((cs / damps) * (torch.norm(ps)/ chiN - 1))\n",
    "        def is_not_moving(arg, val, pre_arg, pre_val, tol):\n",
    "            dis_arg = torch.norm(arg - pre_arg, dim=1).mean()\n",
    "            dis_val = torch.abs(val - pre_val).mean()\n",
    "            return (dis_arg < tol and dis_val < tol) \n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"\\n\\n*******starting optimisation from intitial mean: \", torch.norm(self.x0).detach().cpu().numpy())\n",
    "        # User defined input parameters \n",
    "        dim = self.dim\n",
    "        sigma = 0.3\n",
    "        D = self.std / sigma\n",
    "        mean = self.x0.reshape(dim, 1).detach()\n",
    "        # the size of solutions group\n",
    "        lambda_ = 4 + int(3 * np.log(dim)) if self.cluster_size == None else self.cluster_size  \n",
    "        # only best \"mu\" solutions are used to generate iterations\n",
    "        mu = int(lambda_ / 2) if self.survival_size == None else self.survival_size\n",
    "        # used to combine best \"mu\" solutions                                               \n",
    "        weights = torch.log(mu + 1/2) - torch.log(torch.arange(mu, dtype=torch.float) + 1) \n",
    "        weights = (weights / torch.sum(weights)).cuda()   \n",
    "        mueff = 1 / torch.sum(weights**2) \n",
    "\n",
    "        # Strategy parameter setting: Adaptation\n",
    "        # time constant for cumulation for C\n",
    "        cc = (4 + mueff / dim) / (dim + 4 + 2 * mueff / dim)  \n",
    "        # t-const for cumulation for sigma control\n",
    "        cs = (mueff + 2) / (dim + mueff + 5)  \n",
    "        # learning rate for rank-one update of C\n",
    "        c1 = 2 / ((dim + 1.3)**2 + mueff)    \n",
    "        # and for rank-mu update\n",
    "        cmu = min(1 - c1, 2 * (mueff - 2 + 1 / mueff) / ((dim + 2)**2 + mueff))  \n",
    "        # damping for sigma, usually close to 1  \n",
    "        damps = 1 + 2 * max(0, torch.sqrt((mueff - 1)/( dim + 1)) - 1) + cs     \n",
    "\n",
    "\n",
    "        # Initialize dynamic (internal) strategy parameters and constants\n",
    "        # evolution paths for C and sigma\n",
    "        pc = torch.zeros((dim, 1), device=torch.device('cuda:0'))     \n",
    "        ps = torch.zeros((dim, 1), device=torch.device('cuda:0')) \n",
    "        # B defines the coordinate system\n",
    "        B = torch.eye(int(dim), device=torch.device('cuda:0'))       \n",
    "        # covariance matrix C\n",
    "        C = B * torch.diag(D**2) * B.transpose(1, 0)\n",
    "        # C^-1/2 \n",
    "        invsqrtC = B * torch.diag(D**-1) * B.transpose(1, 0)\n",
    "        # expectation of ||N(0,I)|| == norm(randn(N,1)) \n",
    "        chiN = dim**0.5 * (1 - 1/(4 * dim) + 1 / (21 * dim**2))  \n",
    "\n",
    "        # --------------------  Initialization --------------------------------  \n",
    "        x, x_old, fs = torch.zeros((lambda_, dim), device=torch.device('cuda:0')),  \\\n",
    "                        torch.zeros((lambda_, dim), device=torch.device('cuda:0')), \\\n",
    "                        torch.zeros((lambda_,), device=torch.device('cuda:0'))\n",
    "        stats = {}\n",
    "        inner_stats = {}\n",
    "        stats['inner'] = []\n",
    "        stats['val'], stats['arg'] = [], []\n",
    "        stats['x_adjust'] = []\n",
    "        iter_eval, stats['evals'] = torch.zeros((lambda_,)), []\n",
    "        inner_stats = [{}] * lambda_\n",
    "        stats['mean'], stats['std'] = [], []\n",
    "        stats['status'] = None\n",
    "        iter_, eval_ = 0, 0\n",
    "        # initial data in record\n",
    "        cand = mean.squeeze()\n",
    "        #fs[0] = obj.func(cand)\n",
    "        for i in range(lambda_):\n",
    "            fs[i] = 100\n",
    "            x[i] = cand\n",
    "            x_old[i] = cand\n",
    "        idx = 0\n",
    "        x_ascending = x[idx]\n",
    "        arg = None\n",
    "        val = fs[idx]\n",
    "        pre_arg = x_ascending\n",
    "        pre_val = fs[idx]\n",
    "        best_val = fs[0] + 1e2\n",
    "        best_arg = x[0,:]\n",
    "        sum_eval = 0\n",
    "        # optimise by iterations\n",
    "        while iter_ < self.max_iter:\n",
    "            iter_ += 1\n",
    "            # generate candidate solutions with some stochastic elements\n",
    "            for i in range(lambda_):\n",
    "                candidate_old = (mean + sigma * B @ torch.diag(D) @ torch.randn(dim, 1).cuda()).reshape(1,-1)\n",
    "                print(\"candidate: \", candidate_old.shape)\n",
    "                print(candidate_old)\n",
    "                candidate_new, val, inner_stats[i] = obj.func(candidate_old.requires_grad_(True))\n",
    "                del candidate_old \n",
    "                x[i] = candidate_new.detach()\n",
    "                fs[i] = val.detach()\n",
    "\n",
    "                eval_ += inner_stats[i]['evals']\n",
    "                iter_eval[i] = inner_stats[i]['evals']\n",
    "           # sort the value and positions of solutions \n",
    "            idx = torch.argsort(fs)\n",
    "            x_ascending = x[idx]\n",
    "\n",
    "            # update the parameter for next iteration\n",
    "            mean_old = mean\n",
    "            mean = update_mean(x_ascending[:mu])\n",
    "            # print(\"mean old and new: \", mean_old, mean)\n",
    "            ps =   update_ps(ps, sigma, C, mean, mean_old)\n",
    "            pc =   update_pc(pc, sigma, ps, mean, mean_old)\n",
    "            sigma = update_sigma(sigma, ps)\n",
    "\n",
    "            C =    update_C(C, pc, x_ascending[:mu], mean_old, sigma)\n",
    "            C = (torch.triu(C) + torch.triu(C, 1).transpose(1,0))\n",
    "            D, B = torch.eig(C, eigenvectors=True)\n",
    "            D = torch.sqrt(D[:,0])\n",
    "            invsqrtC = B @ torch.diag(D**-1) @ B.transpose(1,0)\n",
    "            arg = x_ascending\n",
    "            val = fs[idx]\n",
    "            if self.verbose:\n",
    "                print(\"**************** cma iter: \", iter_, \"**********************\")\n",
    "                print(\"loss: %.5f\"%val[0].item())\n",
    "                print(\"evals: \", iter_eval.sum())\n",
    "                #print(\"latent: \", x_ascending[0].cpu().numpy())\n",
    "                #print(\"mean: \", mean)\n",
    "                #print(\"sigma: \", sigma)\n",
    "                #print(\"std: \", D)\n",
    "                print(\"\\n\")\n",
    "            # record data during process for post analysis\n",
    "            if self.record:\n",
    "                #stats['inner'].append(inner_stats)\n",
    "                stats['arg'].append(x_ascending[0].cpu().numpy())\n",
    "                stats['val'].append(fs[idx].detach().cpu().numpy())\n",
    "                #stats['mean'].append(mean.cpu().numpy())\n",
    "                #stats['std'].append((sigma * B @ torch.diag(D)).cpu().numpy())\n",
    "                sum_eval += iter_eval.sum() \n",
    "                stats['evals'].append(sum_eval)\n",
    "                #stats['x_adjust'].append(np.vstack((x.transpose(1,0).cpu().numpy(), x_old.transpose(1,0).cpu().numpy())))\n",
    "            # stopping condition  \n",
    "            if best_val > val[0]:\n",
    "                best_val = val[0]\n",
    "                best_arg = arg[0]              \n",
    "            # check the stop condition\n",
    "            if torch.max(D) > (torch.min(D) * 1e4):\n",
    "                stats['status'] = 'diverge'\n",
    "                print('diverge, concentrate in low dimension manifold')\n",
    "                break\n",
    "            if is_not_moving(arg, val, pre_arg, pre_val, self.tol) :\n",
    "                break\n",
    "            pre_arg = arg\n",
    "            pre_val = val\n",
    "        if self.verbose:\n",
    "            #print('eigenvalue of variance = {}'.format(D))\n",
    "            print('total iterations = {}, total evaluatios = {}'.format(iter_, eval_))\n",
    "            print('found minimum position = {}, found minimum = {}'.format(best_arg.detach().cpu().numpy()[:10], best_val.detach().cpu().numpy()))\n",
    "\n",
    "        # carry statistics info before quit\n",
    "        if self.record:\n",
    "            stats['arg'] = np.array(stats['arg'])\n",
    "            stats['val'] = np.array(stats['val'])\n",
    "            #stats['mean'] = np.array(stats['mean'])\n",
    "            #stats['std'] = np.array(stats['std'])\n",
    "            stats['evals'] = np.array(stats['evals'])\n",
    "            #stats['x_adjust'] = np.array(stats['x_adjust'])\n",
    "        stats['evals'] = eval_\n",
    "        return best_arg, best_val, stats\n",
    "    \n",
    "    def update_mean(self, xs):\n",
    "        print(self.WEIGHTS.shape, xs.shape)\n",
    "        return (self.WEIGHTS @ xs).reshape(self.DIM, 1)\n",
    "    def update_ps(self):\n",
    "        return (1 - self.CS) * self.ps + torch.sqrt(self.CS * (2 - self.CS) * self.MUEFF) * self.invsqrtC @ (self.mean - self.mean_old) / self.sigma \n",
    "    def update_pc(self, iter_):\n",
    "        hsig = (torch.norm(self.ps) / torch.sqrt(1 - (1 - self.CS)**(2 * iter_/self.LAMBDA_)) / self.chiN < 1.4 + 2/(self.DIM + 1)).int()\n",
    "        return (1 - self.CC) * self.pc + hsig * torch.sqrt(self.CC * (2 - self.CC) * self.MUEFF) * (self.mean - self.mean_old) / self.sigma\n",
    "    def update_C(self, iter_, xs):\n",
    "        hsig = (torch.norm(self.ps) / torch.sqrt(1 - (1 - self.CS)**(2 * iter_/self.LAMBDA_)) / self.chiN < (1.4 + 2/(self.DIM + 1))).int()\n",
    "        artmp = (1 / self.sigma) * (xs - self.mean_old.reshape(1, self.DIM))\n",
    "        return (1 - self.C1 - self.CMU) * self.c + self.C1 *  \\\n",
    "                (self.pc * self.pc.transpose(1,0) + (1 - hsig) * self.CC * \\\n",
    "                (2 - self.CC) * self.c) + self.CMU * artmp.transpose(1,0) @  \\\n",
    "                torch.diag(self.WEIGHTS) @ artmp\n",
    "    def update_sigma(self):\n",
    "        return self.sigma * torch.exp((self.CS / self.DAMPS) * (torch.norm(self.ps)/ self.chiN - 1))\n",
    "\n",
    "    def init(self, latent):\n",
    "\n",
    "        # User defined input parameters \n",
    "        self.DIM = latent.shape[1]\n",
    "        # the size of solutions group\n",
    "        self.LAMBDA_ = 4 + int(3 * torch.log(torch.tensor(self.DIM))) if self.cluster_size == None else self.cluster_size  \n",
    "        # only best \"mu\" solutions are used to generate iterations\n",
    "        self.MU = int(self.LAMBDA_ / 2) if self.survival_size == None else self.survival_size\n",
    "        # used to combine best \"mu\" solutions                                               \n",
    "        self.WEIGHTS = torch.log(torch.tensor(self.MU + 1/2)) - torch.log(torch.arange(self.MU, dtype=torch.float) + 1) \n",
    "        self.WEIGHTS = (self.WEIGHTS / torch.sum(self.WEIGHTS)).cuda()   \n",
    "        self.MUEFF = 1 / torch.sum(self.WEIGHTS**2) \n",
    "\n",
    "        # Strategy parameter setting: Adaptation\n",
    "        # time constant for cumulation for C\n",
    "        self.CC = (4 +self. MUEFF / self.DIM) / (self.DIM + 4 + 2 * self.MUEFF / self.DIM)  \n",
    "        # t-const for cumulation for sigma control\n",
    "        self.CS = (self.MUEFF + 2) / (self.DIM + self.MUEFF + 5)  \n",
    "        # learning rate for rank-one update of C\n",
    "        self.C1 = 2 / ((self.DIM + 1.3)**2 + self.MUEFF)    \n",
    "        # and for rank-mu update\n",
    "        self.CMU = min(1 - self.C1, 2 * (self.MUEFF - 2 + 1 / self.MUEFF) / ((self.DIM + 2)**2 + self.MUEFF))  \n",
    "        # damping for sigma, usually close to 1  \n",
    "        self.DAMPS = 1 + 2 * max(0, torch.sqrt((self.MUEFF - 1)/( self.DIM + 1)) - 1) + self.CS     \n",
    "        self.chiN = self.DIM**0.5 * (1 - 1/(4 * self.DIM) + 1 / (21 * self.DIM**2))  \n",
    "\n",
    "        \n",
    "        self.sigma = 0.3\n",
    "        self.d = self.std / self.sigma\n",
    "        self.mean = latent.detach().reshape(self.DIM,1)\n",
    "        # Initialize dynamic (internal) strategy parameters and constants\n",
    "        # evolution paths for C and sigma\n",
    "        self.pc = torch.zeros((self.DIM, 1), device=torch.device('cuda:0'))     \n",
    "        self.ps = torch.zeros((self.DIM, 1), device=torch.device('cuda:0')) \n",
    "        # B defines the coordinate system\n",
    "        self.b = torch.eye(int(self.DIM), device=torch.device('cuda:0'))       \n",
    "        # covariance matrix C\n",
    "        self.c = self.b * torch.diag(self.d**2) * self.b.transpose(1, 0)\n",
    "        # C^-1/2 \n",
    "        self.invsqrtC = self.b * torch.diag(self.d**-1) * self.b.transpose(1, 0)\n",
    "        # expectation of ||N(0,I)|| == norm(randn(N,1)) \n",
    "        self.xs = []\n",
    "        self.fs = []\n",
    "        self.j = 0\n",
    "    def add_inner(self, opt):\n",
    "        self.opt = opt\n",
    "    def step(self, i, x, val, grad):\n",
    "        if i % self.LAMBDA_ != 0 or i == 0 :\n",
    "            self.xs.append(x.detach().squeeze())\n",
    "            self.fs.append(val)\n",
    "            #old = x.clone()\n",
    "            #opt = torch.optim.SGD([x], lr=0.2)\n",
    "            #opt.step()\n",
    "            #return x\n",
    "        else:\n",
    "            idx = torch.argsort(torch.tensor(self.fs))\n",
    "            x_ascending = torch.stack(self.xs)[idx]\n",
    "\n",
    "            # update the parameter for next iteration\n",
    "            self.mean_old = self.mean\n",
    "            self.mean = self.update_mean(x_ascending[:self.MU])\n",
    "            self.ps = self.update_ps()\n",
    "            self.pc = self.update_pc(i)\n",
    "            self.sigma = self.update_sigma()\n",
    "            self.c = self.update_C(i, x_ascending[:self.MU])\n",
    "            self.c = (torch.triu(self.c) + torch.triu(self.c, 1).transpose(1,0))\n",
    "            self.d, self.b = torch.eig(self.c, eigenvectors=True)\n",
    "            self.d = torch.sqrt(self.d[:,0])\n",
    "            self.invsqrtC = self.b @ torch.diag(self.d**-1) @ self.b.transpose(1,0)\n",
    "            self.xs = []\n",
    "            self.fs = []\n",
    "            self.j += 1\n",
    "            print(\"********** %d th iter of CMA completed************\"%(self.j))\n",
    "        x_new = (self.mean + self.sigma * self.b @ torch.diag(self.d)  \\\n",
    "                 @ torch.randn(self.DIM, 1).cuda()).reshape(1,-1)\n",
    "        return x_new.reshape(1, self.DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def method4_to_arbitatry_loss(points, ply_mesh, model, constraint_rad=0.1, axis=0):\n",
    "    initial_dir = points.grad.clone()\n",
    "    points.grad.data.zero_()\n",
    "    #points.requires_grad_(False)\n",
    "    mesh = transform_mesh(points, ply_mesh, AvgTransform)\n",
    "    #mesh['x'] = mesh['x'].detach().requires_grad_(True)\n",
    "    local_preds = model(mesh)\n",
    "    loss = calculate_loss(mesh, local_preds, constraint_rad=constraint_rad)\n",
    "    loss.backward()\n",
    "\n",
    "    sign = [-p1.dot(p2) for p1, p2 in zip(initial_dir, points.grad)]\n",
    "    \n",
    "    return sign, loss, local_preds, mesh\n",
    "\n",
    "def optimize_shape_deepSDF(decoder, latent, initial_points=None, num_points=None, \n",
    "                           num_iters=100, point_iters=100, num_neignours_constr=10,\n",
    "                           lr=0.2, decreased_by=2, adjust_lr_every=10, alpha_penalty=0.05,\n",
    "                           multiplier_func=method4_to_arbitatry_loss, verbose=None, save_to_dir=None, N=256):\n",
    "\n",
    "    def adjust_learning_rate(\n",
    "        initial_lr, optimizer, num_iterations, decreased_by, adjust_lr_every\n",
    "    ):\n",
    "        lr = initial_lr * ((1 / decreased_by) ** (num_iterations // adjust_lr_every)) \\\n",
    "                        * ((punch_lr_at_reindex_by) ** (num_iterations // reindex_latent_each))\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "            \n",
    "        return lr\n",
    "    \n",
    "    if not os.path.exists(os.path.join(save_to_dir, 'meshes')):\n",
    "        os.makedirs(os.path.join(save_to_dir, 'meshes'))\n",
    "    if not os.path.exists(os.path.join(save_to_dir, 'predictions')):\n",
    "        os.makedirs(os.path.join(save_to_dir, 'predictions'))\n",
    "\n",
    "    ref_latent = latent.clone().detach()\n",
    "    decoder.eval()\n",
    "    latent = latent.clone()\n",
    "    latent.requires_grad = True\n",
    "    optimizer = cma_es(dim=256)\n",
    "    optParas ={'x0': latent,\n",
    "           'std': torch.ones((256,), device=torch.device('cuda:0')) * 0.03, \n",
    "           'tol': 1e-6, \n",
    "           'adjust_func': None, \n",
    "           'record': True, \n",
    "           'max_iter': 50,\n",
    "           'cluster_size': 6,\n",
    "           'verbose': True}\n",
    "    optimizer.set_parameters(optParas)\n",
    "    optimizer.init(latent)\n",
    "    #inner_opt = torch.optim.SGD([latent], lr=lr)\n",
    "    #inner_opt.zero_grad()\n",
    "    #optimizer.add_inner(inner_opt)\n",
    "    loss_plot = []\n",
    "    penalty_plot = []\n",
    "    latent_plot = []\n",
    "    math_loss_plot = []\n",
    "    for i in range(num_iters):\n",
    "        time_start = time.time()\n",
    "        \n",
    "        #cur_rl = adjust_learning_rate(lr, inner_opt, i, decreased_by, adjust_lr_every)\n",
    "\n",
    "            \n",
    "        start = time.time()\n",
    "        with torch.no_grad():\n",
    "            ply_mesh = create_mesh( decoder,\n",
    "                                    latent,\n",
    "                                    N=N,\n",
    "                                    max_batch=int(2 ** 18),\n",
    "                                    offset=None,\n",
    "                                    scale=None)\n",
    "        end = time.time()\n",
    "        print(\"mesh time: %.1f \"%(end-start))\n",
    "\n",
    "        points = torch.tensor(np.hstack(( ply_mesh['vertex']['x'][:, None], \n",
    "                                                    ply_mesh['vertex']['y'][:, None], \n",
    "                                                    ply_mesh['vertex']['z'][:, None]))).cuda(0)\n",
    "        \n",
    "        points.requires_grad = True\n",
    "\n",
    "        sdf_value = decode_sdf(decoder, latent, points)\n",
    "        sdf_value.backward(torch.ones([len(points), 1], dtype=torch.float32).cuda(0))\n",
    "\n",
    "        mults, loss_value, preds, transformed_mesh = multiplier_func(points, ply_mesh)         \n",
    "        multipliers = torch.cuda.FloatTensor(mults).cuda(0)\n",
    "    \n",
    "        latent.grad.zero_()\n",
    "        sdf_value = torch.squeeze(decode_sdf(decoder, latent, points.detach()))\n",
    "    \n",
    "        final_loss = torch.sum(sdf_value * multipliers)\n",
    "        \n",
    "        \n",
    "        #final_loss.backward()\n",
    "       # first_deri = torch.norm(latent.grad).item()\n",
    "        first_d = latent.grad.clone().squeeze()\n",
    "        \n",
    "\n",
    "        # Soft-constraints\n",
    "        distances, indeces = LATENT_KD_TREE.query(latent.cpu().detach(), k=num_neignours_constr)\n",
    "        apenalty = alpha_penalty * torch.sum((latent - latent_vectors[indeces.squeeze()]) ** 2, dim=2).mean()\n",
    "        #apenalty.backward()\n",
    "        #sum_d = latent.grad.squeeze()\n",
    "        #second_d = sum_d - first_d\n",
    "        #second_deri = torch.norm(second_d).item()\n",
    "\n",
    "\n",
    "\n",
    "        math_loss = (apenalty + final_loss).detach()\n",
    "\n",
    "        #save_path = os.path.join(save_to_dir, 'meshes/' + str(i).zfill(5) + \".ply\")\n",
    "        #preds_save_path = os.path.join(save_to_dir, 'predictions/' + str(i).zfill(5) + \".npy\")\n",
    "        #tri_mesh = get_trimesh_from_torch_geo_with_colors(transformed_mesh, preds)\n",
    "        #tri_mesh.export(save_path)\n",
    "        #np.save(preds_save_path, preds[:,0].cpu().detach().numpy())\n",
    "        #np.save(os.path.join(save_to_dir, \"latent_plot.npy\"), latent_plot)    \n",
    "\n",
    "        \n",
    "        latent = optimizer.step(i, latent, math_loss.item(), None)\n",
    "        latent = latent.detach().requires_grad_(True)\n",
    "        \n",
    "        #inner_opt.step()\n",
    "        end_end = time.time()\n",
    "        \n",
    "        print(\"backward time: %.2f\"%(end_end-end))\n",
    "        \n",
    "        math_loss_plot.append(math_loss)\n",
    "        penalty_plot.append(apenalty)\n",
    "        loss_plot.append(loss_value.cpu().detach().numpy())\n",
    "        latent_plot.append(latent.clone())\n",
    "        np.save(os.path.join(save_to_dir, \"phy_loss_plot.npy\"), loss_plot)    \n",
    "        np.save(os.path.join(save_to_dir, \"latent_series.npy\"), latent_plot)    \n",
    "        np.save(os.path.join(save_to_dir, \"math_loss_plot.npy\"), math_loss_plot)   \n",
    "\n",
    "        \n",
    "        if verbose is not None and i % verbose == 0:\n",
    "            print('Iter ', i)\n",
    "            #print(\"gradient: first %.3f second %.3f, full %.3f, angle %.2f \"%(first_deri, second_deri, \\\n",
    "            #    torch.norm(sum_d), 90 / np.pi * torch.acos(first_d.dot(second_d) / torch.norm(first_d)/torch.norm(second_d))))\n",
    "            print('phys Loss: %.5f'%loss_value.item())\n",
    "            print('apenality: %.4f'%apenalty.item())\n",
    "            print(\"math_loss: %.4f\"%((apenalty + final_loss).item()))\n",
    "        print(\"\\n\")\n",
    "        if i > 2 and np.abs(loss_plot[-1] - loss_plot[-2]) < 1e-4:\n",
    "            print(\"one time of low progress!\")\n",
    "            #break\n",
    "        \n",
    "    return loss_plot, math_loss_plot, penalty_plot, latent_plot\n",
    "\n",
    "\n",
    "\n",
    "def make_full_transformation(initial_latent, experiment_name, \n",
    "                             decoder, model, alpha_penalty=0.05, constraint_rad=0.1, axis=0, **kwargs):\n",
    "    '''\n",
    "    kwargs:\n",
    "        num_iters=1000, \n",
    "        adjust_lr_every=10, \n",
    "        decreased_by=1.2,\n",
    "        lr=0.005\n",
    "        verbose=10,\n",
    "    '''\n",
    "\n",
    "    #ref_points = get_points_from_latent(decoder, ref_latent, N=128)\n",
    "    save_to_dir = experiment_name\n",
    "    if not os.path.exists(save_to_dir):\n",
    "        os.makedirs(save_to_dir)\n",
    "\n",
    "    #np.save(os.path.join(save_to_dir, \"target_verts.npy\"), ref_points)\n",
    "\n",
    "    return optimize_shape_deepSDF(decoder, initial_latent, initial_points=None,\n",
    "                                           alpha_penalty=alpha_penalty,\n",
    "                                           num_points=None, point_iters=2,\n",
    "                                           multiplier_func=lambda x, y: \n",
    "                                               method4_to_arbitatry_loss(x, y, model, \n",
    "                                                                         constraint_rad=constraint_rad, \n",
    "                                                                         axis=axis),\n",
    "                                           save_to_dir=save_to_dir, **kwargs)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mesh time: 11.4 \n",
      "three parts (321) of loss: 0.000, 0.000, 0.097\n",
      "backward time: 35.28\n",
      "Iter  0\n",
      "phys Loss: 0.09732\n",
      "apenality: 0.0797\n",
      "math_loss: 0.0796\n",
      "\n",
      "\n",
      "mesh time: 11.3 \n",
      "three parts (321) of loss: 0.000, 0.000, 0.070\n",
      "backward time: 34.80\n",
      "Iter  1\n",
      "phys Loss: 0.07042\n",
      "apenality: 0.1166\n",
      "math_loss: 0.1165\n",
      "\n",
      "\n",
      "mesh time: 11.2 \n",
      "three parts (321) of loss: 0.000, 0.000, 0.087\n",
      "backward time: 32.49\n",
      "Iter  2\n",
      "phys Loss: 0.08696\n",
      "apenality: 0.1405\n",
      "math_loss: 0.1404\n",
      "\n",
      "\n",
      "mesh time: 11.1 \n",
      "three parts (321) of loss: 0.000, 0.000, 0.093\n",
      "backward time: 35.62\n",
      "Iter  3\n",
      "phys Loss: 0.09329\n",
      "apenality: 0.1251\n",
      "math_loss: 0.1250\n",
      "\n",
      "\n",
      "mesh time: 11.3 \n",
      "three parts (321) of loss: 0.000, 0.000, 0.093\n",
      "backward time: 36.37\n",
      "Iter  4\n",
      "phys Loss: 0.09337\n",
      "apenality: 0.1249\n",
      "math_loss: 0.1248\n",
      "\n",
      "\n",
      "one time of low progress!\n",
      "mesh time: 11.1 \n",
      "three parts (321) of loss: 0.000, 0.000, 0.085\n",
      "backward time: 33.73\n",
      "Iter  5\n",
      "phys Loss: 0.08528\n",
      "apenality: 0.1239\n",
      "math_loss: 0.1239\n",
      "\n",
      "\n",
      "mesh time: 11.2 \n",
      "three parts (321) of loss: 0.000, 0.000, 0.106\n",
      "torch.Size([3]) torch.Size([3, 256])\n",
      "********** 1 th iter of CMA completed************\n",
      "backward time: 33.87\n",
      "Iter  6\n",
      "phys Loss: 0.10636\n",
      "apenality: 0.1299\n",
      "math_loss: 0.1300\n",
      "\n",
      "\n",
      "mesh time: 11.2 \n",
      "three parts (321) of loss: 0.000, 0.000, 0.054\n",
      "backward time: 31.23\n",
      "Iter  7\n",
      "phys Loss: 0.05422\n",
      "apenality: 0.1487\n",
      "math_loss: 0.1487\n",
      "\n",
      "\n",
      "mesh time: 11.2 \n",
      "three parts (321) of loss: 0.000, 0.000, 0.092\n",
      "backward time: 36.74\n",
      "Iter  8\n",
      "phys Loss: 0.09229\n",
      "apenality: 0.1132\n",
      "math_loss: 0.1131\n",
      "\n",
      "\n",
      "mesh time: 11.4 \n",
      "three parts (321) of loss: 0.000, 0.000, 0.071\n",
      "backward time: 34.25\n",
      "Iter  9\n",
      "phys Loss: 0.07141\n",
      "apenality: 0.1148\n",
      "math_loss: 0.1147\n",
      "\n",
      "\n",
      "mesh time: 11.3 \n",
      "three parts (321) of loss: 0.000, 0.000, 0.081\n",
      "backward time: 35.12\n",
      "Iter  10\n",
      "phys Loss: 0.08076\n",
      "apenality: 0.1162\n",
      "math_loss: 0.1161\n",
      "\n",
      "\n",
      "mesh time: 11.1 \n",
      "three parts (321) of loss: 0.000, 0.000, 0.060\n",
      "backward time: 37.02\n",
      "Iter  11\n",
      "phys Loss: 0.06023\n",
      "apenality: 0.1220\n",
      "math_loss: 0.1218\n",
      "\n",
      "\n",
      "mesh time: 11.2 \n",
      "three parts (321) of loss: 0.000, 0.000, 0.062\n",
      "torch.Size([3]) torch.Size([3, 256])\n",
      "********** 2 th iter of CMA completed************\n",
      "backward time: 31.81\n",
      "Iter  12\n",
      "phys Loss: 0.06215\n",
      "apenality: 0.1205\n",
      "math_loss: 0.1206\n",
      "\n",
      "\n",
      "mesh time: 11.4 \n",
      "three parts (321) of loss: 0.000, 0.000, 0.088\n",
      "backward time: 37.84\n",
      "Iter  13\n",
      "phys Loss: 0.08805\n",
      "apenality: 0.1372\n",
      "math_loss: 0.1373\n",
      "\n",
      "\n",
      "mesh time: 11.3 \n",
      "three parts (321) of loss: 0.000, 0.000, 0.075\n"
     ]
    }
   ],
   "source": [
    "LATENT_TO_OPTIMIZE = latent_vectors[32]\n",
    "DIR_for_dump_data = './cma'\n",
    "punch_lr_at_reindex_by=1\n",
    "reindex_latent_each = 10000\n",
    "\n",
    "np.random.seed(101)\n",
    "torch.manual_seed(0)\n",
    "%time resCMA = make_full_transformation(LATENT_TO_OPTIMIZE.detach(), \\\n",
    "                         experiment_name=DIR_for_dump_data, decoder=decoder, model=predictor, \\\n",
    "                         alpha_penalty=0.2, axis=0, \\\n",
    "                         constraint_rad=0.05,  \\\n",
    "                         num_iters=360,  \\\n",
    "                         adjust_lr_every=20,  \\\n",
    "                         decreased_by=1.1,  \\\n",
    "                         lr=0.2,  \\\n",
    "                         verbose=1, \\\n",
    "                         N=256,  \\\n",
    "                         num_neignours_constr=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
