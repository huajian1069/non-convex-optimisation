{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "conv1.weight \t torch.Size([6, 3, 5, 5])\n",
      "conv1.bias \t torch.Size([6])\n",
      "conv2.weight \t torch.Size([16, 6, 5, 5])\n",
      "conv2.bias \t torch.Size([16])\n",
      "fc1.weight \t torch.Size([120, 400])\n",
      "fc1.bias \t torch.Size([120])\n",
      "fc2.weight \t torch.Size([84, 120])\n",
      "fc2.bias \t torch.Size([84])\n",
      "fc3.weight \t torch.Size([10, 84])\n",
      "fc3.bias \t torch.Size([10])\n",
      "Optimizer's state_dict:\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 0.001, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [4814476144, 4814476304, 4599200656, 4814476224, 4814476384, 4814476464, 4814476544, 4814476624, 4814476704, 4814476784]}]\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "class TheModelClass(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TheModelClass, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "model = TheModelClass()\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.01352374, dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(model.state_dict()['conv1.weight'][0,1,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1588, -0.3376,  0.0932, -0.0402,  0.1580,  0.2382, -0.0432,\n",
       "          -0.0909]]], requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH399 = \"/Users/huajian/Downloads/MinimalDeepSDF/example1/Optimizations/750/Codes/399.pth\"\n",
    "code399 = torch.load(PATH399)\n",
    "code399"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/huajian/Downloads/MinimalDeepSDF/example1/Optimizations/750/Codes'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.dirname(PATH399)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3412, -0.4759, -0.1524,  0.0231,  0.1486,  0.3323, -0.1601,\n",
       "          -0.1239]]], requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH0 = \"/Users/huajian/Downloads/MinimalDeepSDF/example1/Optimizations/750/Codes/0.pth\"\n",
    "code0 = torch.load(PATH0)\n",
    "code0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 8, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code0.unsqueeze(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3412, -0.4759, -0.1524,  0.0231,  0.1486,  0.3323, -0.1601,\n",
       "          -0.1239, -0.3412, -0.4759, -0.1524,  0.0231,  0.1486,  0.3323,\n",
       "          -0.1601, -0.1239]],\n",
       "\n",
       "        [[-0.3412, -0.4759, -0.1524,  0.0231,  0.1486,  0.3323, -0.1601,\n",
       "          -0.1239, -0.3412, -0.4759, -0.1524,  0.0231,  0.1486,  0.3323,\n",
       "          -0.1601, -0.1239]]], grad_fn=<RepeatBackward>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code0.repeat(2,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3412]],\n",
       "\n",
       "        [[-0.4759]],\n",
       "\n",
       "        [[-0.1524]],\n",
       "\n",
       "        [[ 0.0231]],\n",
       "\n",
       "        [[ 0.1486]],\n",
       "\n",
       "        [[ 0.3323]],\n",
       "\n",
       "        [[-0.1601]],\n",
       "\n",
       "        [[-0.1239]]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code0.transpose(2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr= 8e-3\n",
    "l2reg= True\n",
    "decreased_by = 1.5    \n",
    "adjust_lr_every = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$lr_t = lr_0  (\\frac{1}{\\alpha})^{\\frac{t}{t_p}}$\n",
    "\n",
    "$lr_t = 8e^{-3} (\\frac{1}{1.5})^{\\frac{t}{50}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Copyright 2004-present Facebook. All Rights Reserved.\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import deep_sdf\n",
    "import deep_sdf.workspace as ws\n",
    "\n",
    "import pdb\n",
    "\n",
    "def adjust_learning_rate(initial_lr, optimizer, num_iterations, decreased_by, adjust_lr_every):\n",
    "    lr = initial_lr * ((1 / decreased_by) ** (num_iterations // adjust_lr_every))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "\n",
    "def chamfer_distance(p1, p2):\n",
    "    '''\n",
    "    Calculate Chamfer Distance between two point sets\n",
    "    '''\n",
    "\n",
    "    p1 = p1.unsqueeze(0)\n",
    "    p2 = p2.unsqueeze(0)\n",
    "\n",
    "    p1 = p1.repeat(p2.size(1), 1, 1)\n",
    "    p1 = p1.transpose(0, 1)\n",
    "\n",
    "    p2 = p2.repeat(p1.size(0), 1, 1)\n",
    "\n",
    "    # compute distance tensor\n",
    "    dist = torch.add(p1, torch.neg(p2))\n",
    "    dist = torch.norm(dist, 2, dim=2)\n",
    "\n",
    "    dist1, _ = torch.min(dist, dim = 1)\n",
    "    dist2, _ = torch.min(dist, dim = 0)\n",
    "\n",
    "    return torch.mean(dist1) + torch.mean(dist2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    arg_parser = argparse.ArgumentParser(\n",
    "        description=\"Use a trained DeepSDF decoder to reconstruct a shape given SDF \"\n",
    "        + \"samples.\"\n",
    "    )\n",
    "    arg_parser.add_argument(\n",
    "        \"--experiment\",\n",
    "        \"-e\",\n",
    "        dest=\"experiment_directory\",\n",
    "        required=True,\n",
    "        help=\"The experiment directory which includes specifications and saved model \"\n",
    "        + \"files to use for reconstruction\",\n",
    "    )\n",
    "    arg_parser.add_argument(\n",
    "        \"--checkpoint\",\n",
    "        \"-c\",\n",
    "        dest=\"checkpoint\",\n",
    "        default=\"latest\",\n",
    "        help=\"The checkpoint weights to use. This can be a number indicating an epoch \"\n",
    "        + \"or 'latest' for the latest weights (this is the default)\",\n",
    "    )\n",
    "    arg_parser.add_argument(\n",
    "        \"--split\",\n",
    "        \"-s\",\n",
    "        dest=\"split_filename\",\n",
    "        required=True,\n",
    "        help=\"The split to reconstruct.\",\n",
    "    )\n",
    "    arg_parser.add_argument(\n",
    "        \"--iters\",\n",
    "        dest=\"iterations\",\n",
    "        default=400,\n",
    "        help=\"The number of iterations of latent code optimization to perform.\",\n",
    "    )\n",
    "    \n",
    "    ## read by here\n",
    "\n",
    "    # Initialization\n",
    "    N_MARCHING_CUBE = 64\n",
    "    deep_sdf.add_common_args(arg_parser)\n",
    "    args = arg_parser.parse_args()\n",
    "    deep_sdf.configure_logging(args)\n",
    "\n",
    "    specs_filename = os.path.join(args.experiment_directory, \"specs.json\")\n",
    "\n",
    "    if not os.path.isfile(specs_filename):\n",
    "        raise Exception(\n",
    "            'The experiment directory does not include specifications file \"specs.json\"'\n",
    "        )\n",
    "\n",
    "    specs = json.load(open(specs_filename))\n",
    "    arch = __import__(\"networks.\" + specs[\"NetworkArch\"], fromlist=[\"Decoder\"])\n",
    "    latent_size = specs[\"CodeLength\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Load decoder: this is our black box function\n",
    "    decoder = arch.Decoder(latent_size, **specs[\"NetworkSpecs\"])\n",
    "    decoder = torch.nn.DataParallel(decoder)\n",
    "    saved_model_state = torch.load(\n",
    "        os.path.join(\n",
    "            args.experiment_directory, ws.model_params_subdir, args.checkpoint + \".pth\"\n",
    "        ),\n",
    "        map_location=torch.device('cpu') # Remove this if you want to run on GPU\n",
    "    )\n",
    "    saved_model_epoch = saved_model_state[\"epoch\"]\n",
    "    decoder.load_state_dict(saved_model_state[\"model_state_dict\"])\n",
    "    # Optionally: put decoder on GPU\n",
    "    #decoder = decoder.module.cuda()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    with open(args.split_filename, \"r\") as f:\n",
    "        split = json.load(f)\n",
    "\n",
    "    logging.debug(decoder)\n",
    "    optimization_dir = os.path.join(\n",
    "        args.experiment_directory, ws.optimizations_subdir, str(saved_model_epoch)\n",
    "    )\n",
    "\n",
    "    if not os.path.isdir(optimization_dir):\n",
    "        os.makedirs(optimization_dir)\n",
    "\n",
    "    optimization_meshes_dir = os.path.join(\n",
    "        optimization_dir, ws.optimizations_meshes_subdir\n",
    "    )\n",
    "    if not os.path.isdir(optimization_meshes_dir):\n",
    "        os.makedirs(optimization_meshes_dir)\n",
    "\n",
    "    optimization_codes_dir = os.path.join(\n",
    "        optimization_dir, ws.optimizations_codes_subdir\n",
    "    )\n",
    "    if not os.path.isdir(optimization_codes_dir):\n",
    "        os.makedirs(optimization_codes_dir)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    torch.manual_seed(0)\n",
    "    lr= 8e-3\n",
    "    l2reg= True\n",
    "    decreased_by = 1.5\n",
    "    adjust_lr_every = 50\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # pick initialization and samples\n",
    "    # Load collection of all latent codes\n",
    "    all_codes_path = os.path.join(\n",
    "        args.experiment_directory,\n",
    "        ws.latent_codes_subdir,\n",
    "        'latest.pth')\n",
    "    all_codes = torch.load(all_codes_path)['latent_codes']['weight']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    source_id = 999 # zywvjkvz2492e6xpq4hd1jzy2r9lht        # This will be the source shape (ie starting point)\n",
    "    latent = all_codes[source_id].unsqueeze(0).detach()#.cuda()   #Add .cuda() if you want to run on GPU\n",
    "    latent.requires_grad = True\n",
    "\n",
    "    target_id = 2 # 0bucd9ryckhaqtqvbiagilujeqzek4          # This is be the target shape (ie objective)\n",
    "    latent_target = all_codes[target_id].unsqueeze(0).detach()#.cuda()   #Add .cuda() if you want to run on GPU\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Get a mesh representation of the target shape\n",
    "    verts_target, faces_target = deep_sdf.mesh.create_mesh_optim(\n",
    "        decoder, latent_target, N=N_MARCHING_CUBE, max_batch=int(2 ** 18)\n",
    "    )\n",
    "\n",
    "    # Store the mesh\n",
    "    mesh_filename = os.path.join(optimization_meshes_dir, \"target\") + \".ply\"\n",
    "    if not os.path.exists(os.path.dirname(mesh_filename)):\n",
    "        os.makedirs(os.path.dirname(mesh_filename))\n",
    "    deep_sdf.mesh.write_verts_faces_to_file(verts_target, faces_target, mesh_filename)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.Adam([latent], lr=lr)\n",
    "\n",
    "    losses = []\n",
    "    lambdas = []\n",
    "\n",
    "    verts_target_sample = verts_target[torch.randperm(verts_target.shape[0])]\n",
    "    verts_target_sample = verts_target_sample[0:20000, :]\n",
    "    np.save(os.path.join(optimization_meshes_dir, \"target_verts.npy\"), verts_target_sample)\n",
    "\n",
    "    regl2 = 1e-3\n",
    "\n",
    "\n",
    "\n",
    "    # first show latent interpolation form source to target for reference\n",
    "    for i in range(21):\n",
    "        alpha = i/20\n",
    "        print(\"interpolate at:\", alpha)\n",
    "        latent_interp = alpha*latent_target + (1-alpha)*latent\n",
    "        verts, faces = deep_sdf.mesh.create_mesh_optim(\n",
    "            decoder, latent_interp, N=N_MARCHING_CUBE, max_batch=int(2 ** 18)\n",
    "        )\n",
    "        mesh_filename = os.path.join(optimization_meshes_dir, \"interpolation_\" + str(i)) + \".ply\"\n",
    "        if not os.path.exists(os.path.dirname(mesh_filename)):\n",
    "            os.makedirs(os.path.dirname(mesh_filename))\n",
    "        deep_sdf.mesh.write_verts_faces_to_file(verts, faces, mesh_filename)\n",
    "\n",
    "\n",
    "\n",
    "    # Use Adam optimizer, with source as starting point, and a loss defined on meshes\n",
    "    # latent is the input of our function\n",
    "    print(\"Starting optimization:\")\n",
    "    for e in range(args.iterations):\n",
    "\n",
    "\n",
    "        # Get a point cloud sampling of the target shape\n",
    "        verts_target_sample = verts_target[torch.randperm(verts_target.shape[0])]\n",
    "        verts_target_sample = verts_target_sample[0:20000, :]\n",
    "        xyz_target = torch.tensor(verts_target_sample.astype(float), requires_grad = False, dtype=torch.float32) # For GPU, add: , device=torch.device('cuda:0'))\n",
    "\n",
    "        decoder.eval()\n",
    "        adjust_learning_rate(lr, optimizer, e, decreased_by, adjust_lr_every)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        start = time.time()\n",
    "        # Get a mesh representation of our current guess: decoder is evaluated at position latent\n",
    "        # first create mesh running full forward pass\n",
    "        verts, faces = deep_sdf.mesh.create_mesh_optim(\n",
    "            decoder, latent, N=N_MARCHING_CUBE, max_batch=int(2 ** 18)\n",
    "        )\n",
    "        end = time.time()\n",
    "        print(\"time to mesh:\", end-start)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # store the current mesh for visualization\n",
    "        mesh_filename   = os.path.join(optimization_meshes_dir, str(e) + \".ply\")\n",
    "        latent_filename = os.path.join(optimization_codes_dir,  str(e) + \".pth\")\n",
    "\n",
    "\n",
    "        if not os.path.exists(os.path.dirname(mesh_filename)):\n",
    "            os.makedirs(os.path.dirname(mesh_filename))\n",
    "        deep_sdf.mesh.write_verts_faces_to_file(verts, faces, mesh_filename)\n",
    "\n",
    "        if not os.path.exists(os.path.dirname(latent_filename)):\n",
    "            os.makedirs(os.path.dirname(latent_filename))\n",
    "        torch.save(latent.unsqueeze(0), latent_filename)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # subsample vertices for gradients computations\n",
    "        verts = verts[torch.randperm(verts.shape[0])]\n",
    "        verts = verts[0:20000, :]\n",
    "        start = time.time()\n",
    "        # forward pass within loss layer\n",
    "        xyz_upstream = torch.tensor(verts.astype(float), requires_grad = True, dtype=torch.float32)#, device=torch.device('cuda:0')) # For GPU,\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "        # At this point we have 2 outputs for decoder: the target xyz_target, and the current value xyz_upstream\n",
    "        # The following lines compute a loss and backpropagate\n",
    "\n",
    "        # compute loss function: Chamfer between current guess (xyz_upstream) and objective (xyz_target)\n",
    "        loss = chamfer_distance(xyz_upstream, xyz_target)\n",
    "        print(\"Loss at iter\", e, \":\", loss.item(), \", latent norm: \", torch.norm(latent) )\n",
    "        losses.append(loss.detach().cpu().numpy())                                  ## Loss value\n",
    "        #np.save(os.path.join(optimization_meshes_dir, \"log.npy\"), losses)\n",
    "        lambdas.append(torch.norm(latent_target-latent).detach().cpu().numpy())     ## Distance in the domain\n",
    "        #np.save(os.path.join(optimization_meshes_dir, \"lambda.npy\"), lambdas)\n",
    "        \n",
    "\n",
    "\n",
    "        # now store upstream gradients\n",
    "        loss.backward()\n",
    "        dL_dx_i = xyz_upstream.grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # use vertices to compute full backward pass\n",
    "        xyz = torch.tensor(verts.astype(float), requires_grad = True,dtype=torch.float32)#, device=torch.device('cuda:0')) # For GPU,\n",
    "        latent_inputs = latent.expand(xyz.shape[0], -1)\n",
    "        inputs = torch.cat([latent_inputs, xyz], 1)#.cuda()      #Add .cuda() if you want to run on GPU\n",
    "        #first compute normals\n",
    "        pred_sdf = decoder(inputs)\n",
    "\n",
    "\n",
    "\n",
    "        loss_normals = torch.sum(pred_sdf)\n",
    "        loss_normals.backward(retain_graph = True)\n",
    "        normals = xyz.grad/torch.norm(xyz.grad, 2, 1).unsqueeze(-1)\n",
    "        # now assemble inflow derivative\n",
    "        optimizer.zero_grad()\n",
    "        dL_ds_i_fast = -torch.matmul(dL_dx_i.unsqueeze(1), normals.unsqueeze(-1)).squeeze(-1)\n",
    "        loss_backward = torch.sum(dL_ds_i_fast * pred_sdf)\n",
    "\n",
    "\n",
    "\n",
    "        if e % 20 == 0 and e > 0:\n",
    "            regl2 = regl2/2\n",
    "        if l2reg:\n",
    "            loss_backward+= regl2* torch.mean(latent.pow(2))\n",
    "\n",
    "\n",
    "        # Backpropagate\n",
    "        loss_backward.backward()\n",
    "\n",
    "        end = time.time()\n",
    "        print(\"time to backward:\", end-start)\n",
    "\n",
    "        # update latent\n",
    "        # Explicit gradient is accessible via latent.grad\n",
    "        optimizer.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-ada] *",
   "language": "python",
   "name": "conda-env-miniconda3-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
