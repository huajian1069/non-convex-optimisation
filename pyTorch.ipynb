{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## toy neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "conv1.weight \t torch.Size([6, 3, 5, 5])\n",
      "conv1.bias \t torch.Size([6])\n",
      "conv2.weight \t torch.Size([16, 6, 5, 5])\n",
      "conv2.bias \t torch.Size([16])\n",
      "fc1.weight \t torch.Size([120, 400])\n",
      "fc1.bias \t torch.Size([120])\n",
      "fc2.weight \t torch.Size([84, 120])\n",
      "fc2.bias \t torch.Size([84])\n",
      "fc3.weight \t torch.Size([10, 84])\n",
      "fc3.bias \t torch.Size([10])\n",
      "Optimizer's state_dict:\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 0.001, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [4875503568, 4875501808, 4873249888, 4873249088, 4873249648, 4874232848, 4873015456, 4874298864, 4870762976, 4870766016]}]\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "class TheModelClass(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TheModelClass, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "model = TheModelClass()\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TheModelClass(\n",
       "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.01352374, dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(model.state_dict()['conv1.weight'][0,1,2,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensor manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['epoch', 'latent_codes'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## neural network weights\n",
    "all_codes = torch.load(\"../MinimalDeepSDF/example1/LatentCodes/latest.pth\", map_location=torch.device(\"cpu\"))\n",
    "all_codes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "750"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_codes['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3412, -0.4759, -0.1524,  0.0231,  0.1486,  0.3323, -0.1601, -0.1239])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sphere\n",
    "sphere = all_codes['latent_codes']['weight'][999]\n",
    "sphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sphere.unsqueeze(0).detach().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0562, -0.2882,  0.1230,  0.2264,  0.0941,  0.1983,  0.1402, -0.1180])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torsus\n",
    "torsus = all_codes['latent_codes']['weight'][2]\n",
    "torsus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([sphere.unsqueeze(0), torsus.reshape(1,-1)], 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1588, -0.3376,  0.0932, -0.0402,  0.1580,  0.2382, -0.0432, -0.0909],\n",
       "       grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best got by Adam \n",
    "PATH399 = \"/Users/huajian/Downloads/MinimalDeepSDF/example1/Optimizations/750/Codes/399.pth\"\n",
    "code399 = torch.load(PATH399).squeeze()\n",
    "code399"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3747, grad_fn=<NormBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distance between found optimum and initial guess\n",
    "(code399 - sphere).norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3535, grad_fn=<NormBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distance between found optimum and global minimum\n",
    "(code399 - torsus).norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4913, grad_fn=<NormBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(code399,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4841)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(torsus,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7349)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(sphere,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remark:\n",
    "direction is consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2851,  0.1876,  0.2754,  0.2032, -0.0545, -0.1340,  0.3002,  0.0058])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target direction\n",
    "torsus - sphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1824,  0.1383,  0.2456, -0.0633,  0.0094, -0.0941,  0.1168,  0.0330],\n",
       "       grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code399 - sphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1801, grad_fn=<DotBackward>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torsus - sphere).dot(code399 - sphere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/huajian/Downloads/MinimalDeepSDF/example1/Optimizations/750/Codes'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.dirname(PATH399)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3412, -0.4759, -0.1524,  0.0231,  0.1486,  0.3323, -0.1601,\n",
       "          -0.1239]]], requires_grad=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code of sphere\n",
    "PATH0 = \"/Users/huajian/Downloads/MinimalDeepSDF/example1/Optimizations/750/Codes/0.pth\"\n",
    "code0 = torch.load(PATH0, map_location=torch.device('cpu'))\n",
    "code0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 8, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code0.unsqueeze(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3412, -0.4759, -0.1524,  0.0231,  0.1486,  0.3323, -0.1601,\n",
       "          -0.1239, -0.3412, -0.4759, -0.1524,  0.0231,  0.1486,  0.3323,\n",
       "          -0.1601, -0.1239]],\n",
       "\n",
       "        [[-0.3412, -0.4759, -0.1524,  0.0231,  0.1486,  0.3323, -0.1601,\n",
       "          -0.1239, -0.3412, -0.4759, -0.1524,  0.0231,  0.1486,  0.3323,\n",
       "          -0.1601, -0.1239]]], grad_fn=<RepeatBackward>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code0.repeat(2,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3412, -0.4759, -0.1524,  0.0231,  0.1486,  0.3323, -0.1601,\n",
       "          -0.1239]],\n",
       "\n",
       "        [[-0.3412, -0.4759, -0.1524,  0.0231,  0.1486,  0.3323, -0.1601,\n",
       "          -0.1239]]], grad_fn=<ExpandBackward>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code0.expand(2,-1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3412]],\n",
       "\n",
       "        [[-0.4759]],\n",
       "\n",
       "        [[-0.1524]],\n",
       "\n",
       "        [[ 0.0231]],\n",
       "\n",
       "        [[ 0.1486]],\n",
       "\n",
       "        [[ 0.3323]],\n",
       "\n",
       "        [[-0.1601]],\n",
       "\n",
       "        [[-0.1239]]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code0.transpose(2,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MinimalDeepSDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'__main__'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python optim.py -s example1/synth_test.json -e example1\n",
    "__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = __import__(\"library.optimiser\", globals(), fromlist=[\"do_nothing\", \"cma_es\"])\n",
    "m.cma_es()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "750"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## neural network weights\n",
    "saved_model_state = torch.load(\"../MinimalDeepSDF/example1/ModelParameters/latest.pth\", map_location=torch.device(\"cpu\"))\n",
    "saved_model_state[\"epoch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['epoch', 'model_state_dict'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model_state.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(saved_model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## mass code representation of torus\n",
    "split = json.load(open(\"../MinimalDeepSDF/example1/synth_test.json\", \"r\"))\n",
    "type(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_codes = torch.load(\"../MinimalDeepSDF/example1/LatentCodes/latest.pth\")['latent_codes']['weight']\n",
    "type(all_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 8])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_codes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = all_codes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_codes[0].detach().requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.requires_grad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr= 8e-3\n",
    "l2reg= True\n",
    "decreased_by = 1.5    \n",
    "adjust_lr_every = 50\n",
    "\n",
    "N_MARCHING_CUBE = 64\n",
    "regl2 = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$lr_t = lr_0  (\\frac{1}{\\alpha})^{\\frac{t}{t_p}}$\n",
    "\n",
    "$lr_t = 8e^{-3} (\\frac{1}{1.5})^{\\frac{t}{50}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'deep_sdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-18590b70f4a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdeep_sdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeep_sdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkspace\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mws\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'deep_sdf'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Copyright 2004-present Facebook. All Rights Reserved.\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import deep_sdf\n",
    "import deep_sdf.workspace as ws\n",
    "\n",
    "import pdb\n",
    "\n",
    "def adjust_learning_rate(initial_lr, optimizer, num_iterations, decreased_by, adjust_lr_every):\n",
    "    lr = initial_lr * ((1 / decreased_by) ** (num_iterations // adjust_lr_every))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "\n",
    "def chamfer_distance(p1, p2):\n",
    "    '''\n",
    "    Calculate Chamfer Distance between two point sets\n",
    "    '''\n",
    "\n",
    "    p1 = p1.unsqueeze(0)\n",
    "    p2 = p2.unsqueeze(0)\n",
    "\n",
    "    p1 = p1.repeat(p2.size(1), 1, 1)\n",
    "    p1 = p1.transpose(0, 1)\n",
    "\n",
    "    p2 = p2.repeat(p1.size(0), 1, 1)\n",
    "\n",
    "    # compute distance tensor\n",
    "    dist = torch.add(p1, torch.neg(p2))\n",
    "    dist = torch.norm(dist, 2, dim=2)\n",
    "\n",
    "    dist1, _ = torch.min(dist, dim = 1)\n",
    "    dist2, _ = torch.min(dist, dim = 0)\n",
    "\n",
    "    return torch.mean(dist1) + torch.mean(dist2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'deep_sdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-aa9b41b70352>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# Initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mN_MARCHING_CUBE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mdeep_sdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_common_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_parser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mdeep_sdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigure_logging\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'deep_sdf' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    arg_parser = argparse.ArgumentParser(\n",
    "        description=\"Use a trained DeepSDF decoder to reconstruct a shape given SDF samples.\"\n",
    "    )\n",
    "    arg_parser.add_argument(\n",
    "        \"--experiment\",\n",
    "        \"-e\",\n",
    "        dest=\"experiment_directory\",\n",
    "        required=True,\n",
    "        help=\"The experiment directory which includes specifications and saved model \"\n",
    "        + \"files to use for reconstruction\",\n",
    "    )\n",
    "    arg_parser.add_argument(\n",
    "        \"--checkpoint\",\n",
    "        \"-c\",\n",
    "        dest=\"checkpoint\",\n",
    "        default=\"latest\",\n",
    "        help=\"The checkpoint weights to use. This can be a number indicating an epoch \"\n",
    "        + \"or 'latest' for the latest weights (this is the default)\",\n",
    "    )\n",
    "    arg_parser.add_argument(\n",
    "        \"--split\",\n",
    "        \"-s\",\n",
    "        dest=\"split_filename\",\n",
    "        required=True,\n",
    "        help=\"The split to reconstruct.\",\n",
    "    )\n",
    "    arg_parser.add_argument(\n",
    "        \"--iters\",\n",
    "        dest=\"iterations\",\n",
    "        default=400,\n",
    "        help=\"The number of iterations of latent code optimization to perform.\",\n",
    "    )\n",
    "    \n",
    "\n",
    "\n",
    "    # Initialization\n",
    "    N_MARCHING_CUBE = 64\n",
    "    deep_sdf.add_common_args(arg_parser)\n",
    "    args = arg_parser.parse_args()\n",
    "    deep_sdf.configure_logging(args)\n",
    "\n",
    "    specs_filename = os.path.join(args.experiment_directory, \"specs.json\")\n",
    "\n",
    "    if not os.path.isfile(specs_filename):\n",
    "        raise Exception(\n",
    "            'The experiment directory does not include specifications file \"specs.json\"'\n",
    "        )\n",
    "\n",
    "    specs = json.load(open(specs_filename))\n",
    "    arch = __import__(\"networks.\" + specs[\"NetworkArch\"], fromlist=[\"Decoder\"])\n",
    "    latent_size = specs[\"CodeLength\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Load decoder: this is our black box function\n",
    "    decoder = arch.Decoder(latent_size, **specs[\"NetworkSpecs\"])\n",
    "    decoder = torch.nn.DataParallel(decoder)\n",
    "    saved_model_state = torch.load(\n",
    "        os.path.join(\n",
    "            args.experiment_directory, ws.model_params_subdir, args.checkpoint + \".pth\"\n",
    "        ),\n",
    "        map_location=torch.device('cpu') # Remove this if you want to run on GPU\n",
    "    )\n",
    "    saved_model_epoch = saved_model_state[\"epoch\"]\n",
    "    decoder.load_state_dict(saved_model_state[\"model_state_dict\"])\n",
    "    # Optionally: put decoder on GPU\n",
    "    #decoder = decoder.module.cuda()\n",
    "\n",
    "\n",
    "    ## read by here\n",
    "\n",
    "    logging.debug(decoder)\n",
    "    optimization_dir = os.path.join(\n",
    "        args.experiment_directory, ws.optimizations_subdir, str(saved_model_epoch)\n",
    "    )\n",
    "\n",
    "    if not os.path.isdir(optimization_dir):\n",
    "        os.makedirs(optimization_dir)\n",
    "\n",
    "    optimization_meshes_dir = os.path.join(\n",
    "        optimization_dir, ws.optimizations_meshes_subdir\n",
    "    )\n",
    "    if not os.path.isdir(optimization_meshes_dir):\n",
    "        os.makedirs(optimization_meshes_dir)\n",
    "\n",
    "    optimization_codes_dir = os.path.join(\n",
    "        optimization_dir, ws.optimizations_codes_subdir\n",
    "    )\n",
    "    if not os.path.isdir(optimization_codes_dir):\n",
    "        os.makedirs(optimization_codes_dir)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    torch.manual_seed(0)\n",
    "    lr= 8e-3\n",
    "    l2reg= True\n",
    "    decreased_by = 1.5\n",
    "    adjust_lr_every = 50\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # pick initialization and samples\n",
    "    # Load collection of all latent codes\n",
    "    all_codes_path = os.path.join(\n",
    "        args.experiment_directory,\n",
    "        ws.latent_codes_subdir,\n",
    "        'latest.pth')\n",
    "    all_codes = torch.load(all_codes_path)['latent_codes']['weight']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    source_id = 999 # zywvjkvz2492e6xpq4hd1jzy2r9lht        # This will be the source shape (ie starting point)\n",
    "    latent = all_codes[source_id].unsqueeze(0).detach()#.cuda()   #Add .cuda() if you want to run on GPU\n",
    "    latent.requires_grad = True\n",
    "\n",
    "    target_id = 2 # 0bucd9ryckhaqtqvbiagilujeqzek4          # This is be the target shape (ie objective)\n",
    "    latent_target = all_codes[target_id].unsqueeze(0).detach()#.cuda()   #Add .cuda() if you want to run on GPU\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Get a mesh representation of the target shape\n",
    "    verts_target, faces_target = deep_sdf.mesh.create_mesh_optim(\n",
    "        decoder, latent_target, N=N_MARCHING_CUBE, max_batch=int(2 ** 18)\n",
    "    )\n",
    "\n",
    "    # Store the mesh\n",
    "    mesh_filename = os.path.join(optimization_meshes_dir, \"target\") + \".ply\"\n",
    "    if not os.path.exists(os.path.dirname(mesh_filename)):\n",
    "        os.makedirs(os.path.dirname(mesh_filename))\n",
    "    deep_sdf.mesh.write_verts_faces_to_file(verts_target, faces_target, mesh_filename)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.Adam([latent], lr=lr)\n",
    "\n",
    "    losses = []\n",
    "    lambdas = []\n",
    "\n",
    "    verts_target_sample = verts_target[torch.randperm(verts_target.shape[0])]\n",
    "    verts_target_sample = verts_target_sample[0:20000, :]\n",
    "    np.save(os.path.join(optimization_meshes_dir, \"target_verts.npy\"), verts_target_sample)\n",
    "\n",
    "    regl2 = 1e-3\n",
    "\n",
    "\n",
    "\n",
    "    # first show latent interpolation form source to target for reference\n",
    "    for i in range(21):\n",
    "        alpha = i/20\n",
    "        print(\"interpolate at:\", alpha)\n",
    "        latent_interp = alpha*latent_target + (1-alpha)*latent\n",
    "        verts, faces = deep_sdf.mesh.create_mesh_optim(\n",
    "            decoder, latent_interp, N=N_MARCHING_CUBE, max_batch=int(2 ** 18)\n",
    "        )\n",
    "        mesh_filename = os.path.join(optimization_meshes_dir, \"interpolation_\" + str(i)) + \".ply\"\n",
    "        if not os.path.exists(os.path.dirname(mesh_filename)):\n",
    "            os.makedirs(os.path.dirname(mesh_filename))\n",
    "        deep_sdf.mesh.write_verts_faces_to_file(verts, faces, mesh_filename)\n",
    "\n",
    "\n",
    "\n",
    "    # Use Adam optimizer, with source as starting point, and a loss defined on meshes\n",
    "    # latent is the input of our function\n",
    "    print(\"Starting optimization:\")\n",
    "    for e in range(args.iterations):\n",
    "\n",
    "\n",
    "        # Get a point cloud sampling of the target shape\n",
    "        verts_target_sample = verts_target[torch.randperm(verts_target.shape[0])]\n",
    "        verts_target_sample = verts_target_sample[0:20000, :]\n",
    "        xyz_target = torch.tensor(verts_target_sample.astype(float), requires_grad = False, dtype=torch.float32) # For GPU, add: , device=torch.device('cuda:0'))\n",
    "\n",
    "        decoder.eval()\n",
    "        adjust_learning_rate(lr, optimizer, e, decreased_by, adjust_lr_every)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        start = time.time()\n",
    "        # Get a mesh representation of our current guess: decoder is evaluated at position latent\n",
    "        # first create mesh running full forward pass\n",
    "        verts, faces = deep_sdf.mesh.create_mesh_optim(\n",
    "            decoder, latent, N=N_MARCHING_CUBE, max_batch=int(2 ** 18)\n",
    "        )\n",
    "        end = time.time()\n",
    "        print(\"time to mesh:\", end-start)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # store the current mesh for visualization\n",
    "        mesh_filename   = os.path.join(optimization_meshes_dir, str(e) + \".ply\")\n",
    "        latent_filename = os.path.join(optimization_codes_dir,  str(e) + \".pth\")\n",
    "\n",
    "\n",
    "        if not os.path.exists(os.path.dirname(mesh_filename)):\n",
    "            os.makedirs(os.path.dirname(mesh_filename))\n",
    "        deep_sdf.mesh.write_verts_faces_to_file(verts, faces, mesh_filename)\n",
    "\n",
    "        if not os.path.exists(os.path.dirname(latent_filename)):\n",
    "            os.makedirs(os.path.dirname(latent_filename))\n",
    "        torch.save(latent.unsqueeze(0), latent_filename)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # subsample vertices for gradients computations\n",
    "        verts = verts[torch.randperm(verts.shape[0])]\n",
    "        verts = verts[0:20000, :]\n",
    "        start = time.time()\n",
    "        # forward pass within loss layer\n",
    "        xyz_upstream = torch.tensor(verts.astype(float), requires_grad = True, dtype=torch.float32)#, device=torch.device('cuda:0')) # For GPU,\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "        # At this point we have 2 outputs for decoder: the target xyz_target, and the current value xyz_upstream\n",
    "        # The following lines compute a loss and backpropagate\n",
    "\n",
    "        # compute loss function: Chamfer between current guess (xyz_upstream) and objective (xyz_target)\n",
    "        loss = chamfer_distance(xyz_upstream, xyz_target)\n",
    "        print(\"Loss at iter\", e, \":\", loss.item(), \", latent norm: \", torch.norm(latent) )\n",
    "        losses.append(loss.detach().cpu().numpy())                                  ## Loss value\n",
    "        #np.save(os.path.join(optimization_meshes_dir, \"log.npy\"), losses)\n",
    "        lambdas.append(torch.norm(latent_target-latent).detach().cpu().numpy())     ## Distance in the domain\n",
    "        #np.save(os.path.join(optimization_meshes_dir, \"lambda.npy\"), lambdas)\n",
    "        \n",
    "\n",
    "\n",
    "        # now store upstream gradients\n",
    "        loss.backward()\n",
    "        dL_dx_i = xyz_upstream.grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # use vertices to compute full backward pass\n",
    "        xyz = torch.tensor(verts.astype(float), requires_grad = True,dtype=torch.float32)#, device=torch.device('cuda:0')) # For GPU,\n",
    "        latent_inputs = latent.expand(xyz.shape[0], -1)\n",
    "        inputs = torch.cat([latent_inputs, xyz], 1)#.cuda()      #Add .cuda() if you want to run on GPU\n",
    "        #first compute normals\n",
    "        pred_sdf = decoder(inputs)\n",
    "\n",
    "\n",
    "\n",
    "        loss_normals = torch.sum(pred_sdf)\n",
    "        loss_normals.backward(retain_graph = True)\n",
    "        normals = xyz.grad/torch.norm(xyz.grad, 2, 1).unsqueeze(-1)\n",
    "        # now assemble inflow derivative\n",
    "        optimizer.zero_grad()\n",
    "        dL_ds_i_fast = -torch.matmul(dL_dx_i.unsqueeze(1), normals.unsqueeze(-1)).squeeze(-1)\n",
    "        loss_backward = torch.sum(dL_ds_i_fast * pred_sdf)\n",
    "\n",
    "\n",
    "\n",
    "        if e % 20 == 0 and e > 0:\n",
    "            regl2 = regl2/2\n",
    "        if l2reg:\n",
    "            loss_backward+= regl2* torch.mean(latent.pow(2))\n",
    "\n",
    "\n",
    "        # Backpropagate\n",
    "        loss_backward.backward()\n",
    "\n",
    "        end = time.time()\n",
    "        print(\"time to backward:\", end-start)\n",
    "\n",
    "        # update latent\n",
    "        # Explicit gradient is accessible via latent.grad\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-ada] *",
   "language": "python",
   "name": "conda-env-miniconda3-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
