{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-wgCgJ6AieVb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import skimage.measure\n",
    "import plyfile\n",
    "from plyfile import PlyData\n",
    "from sklearn.neighbors import KDTree\n",
    "import trimesh\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import (NNConv, GMMConv, GraphConv, Set2Set)\n",
    "from torch_geometric.nn import (SplineConv, graclus, max_pool, max_pool_x, global_mean_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "DtBoNNSMieVi",
    "outputId": "86d328bb-69b8-4e45-baf7-d2e03557e517"
   },
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "f6JlQ7MWzsDx",
    "outputId": "ce243b5e-97f2-4d59-f35e-3a4a7fa026eb"
   },
   "outputs": [],
   "source": [
    "! nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "3oINuP6Sz5B8",
    "outputId": "05dae263-cb5f-4e55-9d75-27d49680e9f5"
   },
   "outputs": [],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zuy6igvMnjMZ"
   },
   "outputs": [],
   "source": [
    "! mkdir networks\n",
    "! mv ../deep_sdf_decoder.py networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZQDXwZHwieV4"
   },
   "outputs": [],
   "source": [
    "model_params_subdir = \"ModelParameters\"\n",
    "optimizer_params_subdir = \"OptimizerParameters\"\n",
    "latent_codes_subdir = \"LatentCodes\"\n",
    "logs_filename = \"Logs.pth\"\n",
    "reconstructions_subdir = \"Reconstructions\"\n",
    "reconstruction_meshes_subdir = \"Meshes\"\n",
    "reconstruction_codes_subdir = \"Codes\"\n",
    "specifications_filename = \"specs.json\"\n",
    "data_source_map_filename = \".datasources.json\"\n",
    "evaluation_subdir = \"Evaluation\"\n",
    "sdf_samples_subdir = \"SdfSamples\"\n",
    "surface_samples_subdir = \"SurfaceSamples\"\n",
    "normalization_param_subdir = \"NormalizationParameters\"\n",
    "training_meshes_subdir = \"TrainingMeshes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5vlOE0_iieV8"
   },
   "outputs": [],
   "source": [
    "def load_latent_vectors(experiment_directory, checkpoint):\n",
    "\n",
    "    filename = os.path.join(\n",
    "        experiment_directory, checkpoint + \".pth\"\n",
    "    )\n",
    "    if not os.path.isfile(filename):\n",
    "        raise Exception(\n",
    "            \"The experiment directory ({}) does not include a latent code file\"\n",
    "            + \" for checkpoint '{}'\".format(experiment_directory, checkpoint)\n",
    "        )\n",
    "    data = torch.load(filename)\n",
    "    return data[\"latent_codes\"].cuda()\n",
    "\n",
    "def load_model(experiment_directory, checkpoint):\n",
    "    specs_filename = os.path.join(experiment_directory, \"specs.json\")\n",
    "\n",
    "    if not os.path.isfile(specs_filename):\n",
    "        raise Exception(\n",
    "            'The experiment directory does not include specifications file \"specs.json\"'\n",
    "        )\n",
    "\n",
    "    specs = json.load(open(specs_filename))\n",
    "\n",
    "    arch = __import__(\"networks.\" + specs[\"NetworkArch\"], fromlist=[\"Decoder\"])\n",
    "\n",
    "    latent_size = specs[\"CodeLength\"]\n",
    "\n",
    "    decoder = arch.Decoder(latent_size, **specs[\"NetworkSpecs\"])\n",
    "\n",
    "    decoder = torch.nn.DataParallel(decoder)\n",
    "\n",
    "    saved_model_state = torch.load(\n",
    "        os.path.join(experiment_directory, checkpoint + \".pth\")\n",
    "    )\n",
    "\n",
    "    decoder.load_state_dict(saved_model_state[\"model_state_dict\"])\n",
    "\n",
    "    decoder = decoder.module.cuda()\n",
    "\n",
    "    decoder.eval()\n",
    "    \n",
    "    return decoder\n",
    "\n",
    "def create_mesh(decoder, latent_vec, filename='', N=256, max_batch=32 ** 3, offset=None, scale=None):\n",
    "    start = time.time()\n",
    "    ply_filename = filename\n",
    "\n",
    "    decoder.eval()\n",
    "\n",
    "    # NOTE: the voxel_origin is actually the (bottom, left, down) corner, not the middle\n",
    "    voxel_origin = [-1, -1, -1]\n",
    "    voxel_size = 2.0 / (N - 1)\n",
    "\n",
    "    overall_index = torch.arange(0, N ** 3, 1, out=torch.LongTensor())\n",
    "    samples = torch.zeros(N ** 3, 4)\n",
    "\n",
    "    # transform first 3 columns\n",
    "    # to be the x, y, z index\n",
    "    samples[:, 2] = overall_index % N\n",
    "    samples[:, 1] = (overall_index.long() / N) % N\n",
    "    samples[:, 0] = ((overall_index.long() / N) / N) % N\n",
    "\n",
    "    # transform first 3 columns\n",
    "    # to be the x, y, z coordinate\n",
    "    samples[:, 0] = (samples[:, 0] * voxel_size) + voxel_origin[2]\n",
    "    samples[:, 1] = (samples[:, 1] * voxel_size) + voxel_origin[1]\n",
    "    samples[:, 2] = (samples[:, 2] * voxel_size) + voxel_origin[0]\n",
    "\n",
    "    num_samples = N ** 3\n",
    "\n",
    "    samples.requires_grad = False\n",
    "\n",
    "    head = 0\n",
    "\n",
    "    while head < num_samples:\n",
    "        sample_subset = samples[head : min(head + max_batch, num_samples), 0:3].cuda()\n",
    "\n",
    "        samples[head : min(head + max_batch, num_samples), 3] = \\\n",
    "                decode_sdf(decoder, latent_vec, sample_subset).squeeze(1).detach().cpu()\n",
    "        head += max_batch\n",
    "\n",
    "    sdf_values = samples[:, 3]\n",
    "    sdf_values = sdf_values.reshape(N, N, N)\n",
    "\n",
    "    end = time.time()\n",
    "    #print(\"sampling takes: %f\" % (end - start))\n",
    "\n",
    "    return convert_sdf_samples_to_ply(\n",
    "        sdf_values.data.cpu(),\n",
    "        voxel_origin,\n",
    "        voxel_size,\n",
    "        ply_filename + \".ply\",\n",
    "        offset,\n",
    "        scale,\n",
    "    )\n",
    "\n",
    "def convert_sdf_samples_to_ply(\n",
    "    pytorch_3d_sdf_tensor,\n",
    "    voxel_grid_origin,\n",
    "    voxel_size,\n",
    "    ply_filename_out,\n",
    "    offset=None,\n",
    "    scale=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Convert sdf samples to .ply\n",
    "\n",
    "    :param pytorch_3d_sdf_tensor: a torch.FloatTensor of shape (n,n,n)\n",
    "    :voxel_grid_origin: a list of three floats: the bottom, left, down origin of the voxel grid\n",
    "    :voxel_size: float, the size of the voxels\n",
    "    :ply_filename_out: string, path of the filename to save to\n",
    "\n",
    "    This function adapted from: https://github.com/RobotLocomotion/spartan\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    numpy_3d_sdf_tensor = pytorch_3d_sdf_tensor.numpy()\n",
    "\n",
    "    verts, faces, normals, values = skimage.measure.marching_cubes_lewiner(\n",
    "        numpy_3d_sdf_tensor, level=0.0, spacing=[voxel_size] * 3\n",
    "    )\n",
    "\n",
    "    # transform from voxel coordinates to camera coordinates\n",
    "    # note x and y are flipped in the output of marching_cubes\n",
    "    mesh_points = np.zeros_like(verts)\n",
    "    mesh_points[:, 0] = voxel_grid_origin[0] + verts[:, 0]\n",
    "    mesh_points[:, 1] = voxel_grid_origin[1] + verts[:, 1]\n",
    "    mesh_points[:, 2] = voxel_grid_origin[2] + verts[:, 2]\n",
    "\n",
    "    # apply additional offset and scale\n",
    "    if scale is not None:\n",
    "        mesh_points = mesh_points / scale\n",
    "    if offset is not None:\n",
    "        mesh_points = mesh_points - offset\n",
    "\n",
    "    # try writing to the ply file\n",
    "\n",
    "    num_verts = verts.shape[0]\n",
    "    num_faces = faces.shape[0]\n",
    "\n",
    "    verts_tuple = np.zeros((num_verts,), dtype=[(\"x\", \"f4\"), (\"y\", \"f4\"), (\"z\", \"f4\")])\n",
    "    norms_tuple = np.zeros((num_verts,), dtype=[(\"x\", \"f4\"), (\"y\", \"f4\"), (\"z\", \"f4\")])\n",
    "\n",
    "    for i in range(0, num_verts):\n",
    "        verts_tuple[i] = tuple(mesh_points[i, :])\n",
    "        norms_tuple[i] = tuple(normals[i, :])\n",
    "\n",
    "    faces_building = []\n",
    "    for i in range(0, num_faces):\n",
    "        faces_building.append(((faces[i, :].tolist(),)))\n",
    "    faces_tuple = np.array(faces_building, dtype=[(\"vertex_indices\", \"i4\", (3,))])\n",
    "\n",
    "    el_verts = plyfile.PlyElement.describe(verts_tuple, \"vertex\")\n",
    "    el_faces = plyfile.PlyElement.describe(faces_tuple, \"face\")\n",
    "    el_norms = plyfile.PlyElement.describe(norms_tuple, \"normals\")\n",
    "\n",
    "    ply_data = plyfile.PlyData([el_verts, el_faces, el_norms])\n",
    "    return ply_data\n",
    "\n",
    "\n",
    "def make_mesh_from_points(points, ply_mesh):\n",
    "    transformed_points = transformPoints(points)\n",
    "    \n",
    "    edges = trimesh.geometry.faces_to_edges(ply_mesh['face']['vertex_indices'])\n",
    "    np_points = transformed_points.cpu().detach().numpy()\n",
    "    edge_attr = [np_points[a] - np_points[b] for a, b in edges]\n",
    "    mesh = {'x': transformed_points, \n",
    "        'face':torch.tensor(ply_mesh['face']['vertex_indices'], dtype=torch.long).to('cuda:0').t(),\n",
    "        'edge_attr':torch.tensor(edge_attr, dtype=torch.float).to('cuda:0'),\n",
    "        'edge_index':torch.tensor(edges, dtype=torch.long).t().contiguous().to('cuda:0')\n",
    "        }\n",
    "    return mesh\n",
    "\n",
    "def transformPoints(points):\n",
    "    matrix = torch.cuda.FloatTensor(AvgTransform)\n",
    "    column = torch.zeros((len(points), 1), device=\"cuda:0\") + 1\n",
    "    stacked = torch.cat([points, column], dim=1)\n",
    "    transformed = torch.matmul(matrix, stacked.t()).t()[:, :3]\n",
    "    return transformed\n",
    "\n",
    "def get_trimesh_from_torch_geo_with_colors(mesh, preds, vmin=-8, vmax=8):\n",
    "    norm = mpl.colors.Normalize(vmin= vmin, vmax=vmax)\n",
    "    cmap = cm.hot\n",
    "    m = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "    \n",
    "    verticies = mesh['x'].cpu().detach()\n",
    "    faces = mesh['face'].t().cpu().detach()\n",
    "    return trimesh.Trimesh(vertices=verticies, faces=faces, \n",
    "                           vertex_colors=list(map(lambda c: m.to_rgba(c),  preds[:, 0].cpu().detach())))\n",
    "\n",
    "\n",
    "def decode_sdf(decoder, latent_vector, queries):\n",
    "    num_samples = queries.shape[0]\n",
    "\n",
    "    if latent_vector is None:\n",
    "        inputs = queries\n",
    "    else:\n",
    "        latent_repeat = latent_vector.expand(num_samples, -1)\n",
    "        inputs = torch.cat([latent_repeat, queries], 1)\n",
    "\n",
    "    sdf = decoder(inputs)\n",
    "\n",
    "    return sdf\n",
    "def compute_lift_faces_diff(data_instance, answers, axis=0):\n",
    "    pressures = torch.mean(answers[data_instance['face'], 0], axis=0)\n",
    "\n",
    "    # TODO: cahnge to x if needed\n",
    "    pos = data_instance['x']\n",
    "    cross_prod = (pos[data_instance['face'][1]] - pos[data_instance['face'][0]]).cross(\n",
    "                  pos[data_instance['face'][2]] - pos[data_instance['face'][0]])\n",
    "    mult = -cross_prod[:, axis] / 2\n",
    "    lift = torch.mul(pressures, mult)\n",
    "    return torch.sum(lift[~torch.isnan(lift)])\n",
    "\n",
    "def boundsLoss(points, box=[(-1, 1, 0)]):\n",
    "    loss = 0\n",
    "    for l, r, i in box:\n",
    "        loss +=  torch.mean(F.relu(-points[:, i] + l))  \\\n",
    "               + torch.mean(F.relu( points[:, i] - r))\n",
    "    return loss\n",
    "\n",
    "def innerBoundsLoss(points, r=1, center=(0, 0, 0)):\n",
    "    radiuses = torch.sum( (points - torch.Tensor(center).to('cuda:0')) ** 2 , dim=1)\n",
    "    return torch.mean(F.relu(r - radiuses))\n",
    "\n",
    "def calculate_loss(mesh, local_preds, axis=0, constraint_rad=0.1):\n",
    "    loss =  (1 - axis) * compute_lift_faces_diff(mesh, local_preds, axis=0) + \\\n",
    "                  axis * compute_lift_faces_diff(mesh, local_preds, axis=1)\n",
    "    print(\"first part of loss: \", loss.detach().cpu().numpy())\n",
    "    first = loss.clone().detach().cpu().numpy()\n",
    "    loss += boundsLoss(mesh['x'], box=[(-0.6, 0.6, 0)])\n",
    "    print(\"second part of loss: \", loss.detach().cpu().numpy() - first)\n",
    "    second = loss.clone().detach().cpu().numpy()\n",
    "    loss += innerBoundsLoss(mesh['x'], r=constraint_rad**2, center=(-0.05, 0.05, 0))  \\\n",
    "          + innerBoundsLoss(mesh['x'], r=(constraint_rad / 2)**2, center=(0.3, 0, 0))\n",
    "    print(\"third part of loss: \", loss.detach().cpu().numpy() - second)\n",
    "    return loss\n",
    "\n",
    "def soft_constraints(latent, latent_vectors, num_neignours_constr, alpha_penalty):\n",
    "    # Soft-constraints\n",
    "    distances, indeces = LATENT_KD_TREE.query(latent.cpu().detach(), k=num_neignours_constr)\n",
    "    torch.sum((initial_la - latent_vectors[indeces.squeeze()]) ** 2, dim=2).mean()\n",
    "    return penalty * alpha_penalty\n",
    "\n",
    "def computeAvgTransform():\n",
    "    objects = list()\n",
    "    for (dirpath, dirnames, filenames) in os.walk(\"/cvlabdata2/home/artem/Data/cars_remeshed_dsdf/transforms/\"):\n",
    "        objects += [os.path.join(dirpath, file) for file in filenames if file[-4:] == '.npy']\n",
    "    \n",
    "    matricies = []\n",
    "    for obj in objects:\n",
    "        matricies.append(np.load(obj))\n",
    "    \n",
    "    return np.mean(np.array(matricies), axis=0)\n",
    "\n",
    "\n",
    "class SplineBlock(nn.Module):\n",
    "    def __init__(self, num_in_features, num_outp_features, mid_features, kernel=3, dim=3, batchnorm1=True):\n",
    "        super(SplineBlock, self).__init__()\n",
    "        self.batchnorm1 = batchnorm1\n",
    "        self.conv1 = SplineConv(num_in_features, mid_features, dim, kernel, is_open_spline=False)\n",
    "        if self.batchnorm1:\n",
    "            self.batchnorm1 = torch.nn.BatchNorm1d(mid_features)\n",
    "        self.conv2 = SplineConv(mid_features, 2 * mid_features, dim, kernel, is_open_spline=False)\n",
    "        self.batchnorm2 = torch.nn.BatchNorm1d(2 * mid_features)\n",
    "        self.conv3 = SplineConv(2 * mid_features + 3, num_outp_features, dim, kernel, is_open_spline=False)\n",
    "  \n",
    "    def forward(self, res, data):\n",
    "        if self.batchnorm1:\n",
    "            res = F.elu(self.batchnorm1(self.conv1(res, data['edge_index'], data['edge_attr'])))\n",
    "        else:\n",
    "            res = F.elu(self.conv1(res, data['edge_index'], data['edge_attr']))\n",
    "        res = F.elu(self.batchnorm2(self.conv2(res, data['edge_index'], data['edge_attr'])))\n",
    "#         res = F.elu(self.conv2(res, data.edge_index, data.edge_attr))\n",
    "        res = torch.cat([res, data['x']], dim=1)\n",
    "        res = self.conv3(res, data['edge_index'], data['edge_attr'])\n",
    "        return res\n",
    "\n",
    "class SplineCNN8Residuals(nn.Module):\n",
    "    def __init__(self, num_features, kernel=3, dim=3):\n",
    "        super(SplineCNN8Residuals, self).__init__()\n",
    "        self.block1 = SplineBlock(num_features, 16, 8, kernel, dim)\n",
    "        self.block2 = SplineBlock(16, 64, 32, kernel, dim)\n",
    "        self.block3 = SplineBlock(64, 64, 128, kernel, dim)\n",
    "        self.block4 = SplineBlock(64, 8, 16, kernel, dim)\n",
    "        self.block5 = SplineBlock(11, 32, 16, kernel, dim)\n",
    "        self.block6 = SplineBlock(32, 64, 32, kernel, dim)\n",
    "        self.block7 = SplineBlock(64, 64, 128, kernel, dim)\n",
    "        self.block8 = SplineBlock(75, 4, 16, kernel, dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        res = data['x']\n",
    "        res = self.block1(res, data)\n",
    "        res = self.block2(res, data)\n",
    "        res = self.block3(res, data)\n",
    "        res4 = self.block4(res, data)\n",
    "        res = torch.cat([res4, data['x']], dim=1)\n",
    "        res = self.block5(res, data)\n",
    "        res = self.block6(res, data)\n",
    "        res = self.block7(res, data)\n",
    "        res = torch.cat([res, res4, data['x']], dim=1)\n",
    "        res = self.block8(res, data)\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-JM2oaapieWC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-I_hhsIwieWF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zw1U5NI-ieWJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OwDmuXRZieWL"
   },
   "outputs": [],
   "source": [
    "DIR_for_dump_data = './starting_data'\n",
    "experiment_directory = DIR_for_dump_data\n",
    "\n",
    "\n",
    "\n",
    "decoder = load_model(experiment_directory, \"decoderModel\")\n",
    "latent_vectors = load_latent_vectors(experiment_directory, \"latentCodes\")\n",
    "latent_vectors = latent_vectors.detach()\n",
    "\n",
    "LATENT_TO_OPTIMIZE = latent_vectors[32]\n",
    "LATENT_KD_TREE = KDTree(np.array([lv.cpu().detach().numpy()[0] for lv in latent_vectors]))\n",
    "AvgTransform = np.load(DIR_for_dump_data + \"/avg_trans_matrix.npy\") #computeAvgTransform()\n",
    "\n",
    "model = SplineCNN8Residuals(3)\n",
    "model.load_state_dict(torch.load(experiment_directory + \"/cfdModel.nn\"))\n",
    "model = model.to(\"cuda:0\")\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w3z7xq1QieWR"
   },
   "outputs": [],
   "source": [
    "initial_la = latent_vectors[32]\n",
    "ply_mesh = create_mesh( decoder,\n",
    "                        initial_la,\n",
    "                        N=256,\n",
    "                        max_batch=int(2 ** 18),\n",
    "                        offset=None,\n",
    "                        scale=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1HL3A11cx3wm"
   },
   "outputs": [],
   "source": [
    "points = torch.cuda.FloatTensor(np.hstack(( ply_mesh['vertex']['x'][:, None], \n",
    "                                            ply_mesh['vertex']['y'][:, None], \n",
    "                                            ply_mesh['vertex']['z'][:, None])))\n",
    "\n",
    "# from mesh to pressure field\n",
    "points.requires_grad = True\n",
    "mesh = make_mesh_from_points(points, ply_mesh)\n",
    "#del ply_mesh, points\n",
    "local_preds = model(mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kQgi519pBA_L"
   },
   "outputs": [],
   "source": [
    "loss = calculate_loss(mesh, local_preds, axis=0, constraint_rad=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ayzvv36zI1S5"
   },
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "dL_dp = points.grad.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hr_1kLvqJTQ0"
   },
   "outputs": [],
   "source": [
    "points.grad.data.zero_()\n",
    "sdf_value = decode_sdf(decoder, initial_la, points)\n",
    "sdf_value.backward(torch.ones([len(points), 1], dtype=torch.float32).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BLPn3Y81J5Ab"
   },
   "outputs": [],
   "source": [
    "# assemble constant \n",
    "mults = [-p1.dot(p2) for p1, p2 in zip(dL_dp, points.grad)]       \n",
    "multipliers = torch.cuda.FloatTensor(mults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y7RtRhTUJewl"
   },
   "outputs": [],
   "source": [
    "points = points.detach()\n",
    "initial_la = initial_la.detach().requires_grad_(True)\n",
    "latent_inputs = initial_la.expand(points.shape[0], -1)\n",
    "inputs = torch.cat([latent_inputs, points], 1).cuda() \n",
    "sdf_value = decoder(inputs)\n",
    "final_loss = torch.sum(sdf_value.squeeze() * multipliers)\n",
    "final_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "colab_type": "code",
    "id": "n1cxfRK9ozBp",
    "outputId": "fba5cec2-6a64-4275-f58e-9752fbdebedb"
   },
   "outputs": [],
   "source": [
    "#initial_la.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "id": "T440V-0ao248",
    "outputId": "b0ddcc89-4900-4f2b-8403-228a8fea66f3"
   },
   "outputs": [],
   "source": [
    "#initial_la.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-3A3dHkwYmKs"
   },
   "outputs": [],
   "source": [
    "apenalty = soft_constraints(initial_la, latent_vectors, num_neignours_constr=10, alpha_penalty=0.2)\n",
    "apenalty.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 901
    },
    "colab_type": "code",
    "id": "MJJO70Umo7LV",
    "outputId": "4b78bd74-8ce6-4c60-f209-a7eafae12fa0"
   },
   "outputs": [],
   "source": [
    "#initial_la.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "aTs5dZL0CuEg",
    "outputId": "2ef245aa-ee5b-498c-bad0-054458a7631d"
   },
   "outputs": [],
   "source": [
    "final_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fu57UXaPofQ9",
    "outputId": "d943245e-229e-42a9-e299-264325375b4c"
   },
   "outputs": [],
   "source": [
    "penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "kpIdfkoR_P4K",
    "outputId": "8cc7c36f-56c3-4769-ac0b-5234b45d4ff9"
   },
   "outputs": [],
   "source": [
    "innerBoundsLoss(mesh['x'], r=(0.5 / 2)**2, center=(0.3, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "T-pitvgF_znG",
    "outputId": "395691c7-70f2-4a76-c31b-21d710e385e0"
   },
   "outputs": [],
   "source": [
    "boundsLoss(mesh['x'], box=[(-0.006, 0.006, 0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "dsOoQcbC1ai1",
    "outputId": "ec93170e-e62f-4933-dd4a-b5debd0f3c2a"
   },
   "outputs": [],
   "source": [
    "local_preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ccq6n-_m1h4f",
    "outputId": "efc9c605-a1c0-4b2d-e527-31090152df0f"
   },
   "outputs": [],
   "source": [
    "points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6y6WBozCieWT"
   },
   "outputs": [],
   "source": [
    "ply_mesh.write(\"data_for_this_experiments/mesh32.ply\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cfd_obj:\n",
    "    def __init__(self, decoder, p_predictor):\n",
    "        self.N_MARCHING_CUBE = 128\n",
    "        self.regl2 = 1e-3\n",
    "        self.iter = 0\n",
    "        self.quick = True\n",
    "        self.AvgTransform = computeAvgTransform()\n",
    "        self.decoder = decoder\n",
    "        self.pressure_pred = p_predictor\n",
    "        self.optimal = latent_target.detach().cpu().numpy()\n",
    "        self.constraint_rad = 0.05\n",
    "    \n",
    "        \n",
    "    def func(self, latent):\n",
    "        # from latent to xyz\n",
    "        mesh = create_mesh(decoder, latent, N=self.N_MARCHING_CUBE, max_batch=int(2 ** 18))\n",
    "        points = torch.cuda.FloatTensor(np.hstack(( ply_mesh['vertex']['x'][:, None], \n",
    "                                            ply_mesh['vertex']['y'][:, None], \n",
    "                                            ply_mesh['vertex']['z'][:, None])))\n",
    "        # from mesh to pressure field\n",
    "        self.xyz_upstream = points.detach().requires_grad_(True)\n",
    "        scaled_mesh = make_mesh_from_points(points, mesh)\n",
    "        pressure_field = model(scaled_mesh)\n",
    "        loss = calculate_loss(scaled_mesh, pressure_field, axis=0, constraint_rad=self.constraint_rad)\n",
    "        self.last_loss = loss\n",
    "        self.last_latent = latent\n",
    "        return loss\n",
    "    \n",
    "    def dfunc(self, latent):\n",
    "        if latent.grad is not None:\n",
    "            latent.grad.detach_()\n",
    "            latent.grad.zero_()\n",
    "        # step 1\n",
    "        if self.quick and self.last_latent is not None and torch.all(latent == self.last_latent):\n",
    "            loss = self.last_loss\n",
    "        else:\n",
    "            loss = self.func(latent)\n",
    "        loss.backward()\n",
    "        dL_dx_i = self.xyz_upstream.grad\n",
    "        \n",
    "        \n",
    "        # step 2\n",
    "        # calculate mesh normal\n",
    "        xyz = self.xyz_upstream.clone().detach()\n",
    "        xyz.requires_grad = True\n",
    "        \n",
    "        latent_inputs = latent.expand(xyz.shape[0], -1)\n",
    "        inputs = torch.cat([latent_inputs, xyz], 1).cuda()      #Add .cuda() if you want to run on GPU\n",
    "        #first compute normals\n",
    "        pred_sdf = self.decoder(inputs)\n",
    "        \n",
    "        loss_normals = torch.sum(pred_sdf)\n",
    "        loss_normals.backward(retain_graph = True)\n",
    "        normals = xyz.grad/torch.norm(xyz.grad, 2, 1).unsqueeze(-1)\n",
    "        \n",
    "        print(\"normal: \", normals.shape)\n",
    "        print(\"dl_dx_i: \", dL_dx_i.shape)\n",
    "        \n",
    "        # step 3\n",
    "        # now assemble inflow derivative\n",
    "        latent.grad.detach_()\n",
    "        latent.grad.zero_()\n",
    "        multipliers = -torch.matmul(dL_dx_i.unsqueeze(1), normals.unsqueeze(-1)).squeeze(-1)\n",
    "        loss_backward = torch.sum(multipliers * pred_sdf)\n",
    "        \n",
    "        # artificial loss\n",
    "        apenalty = soft_constraints(latent, latent_vectors, num_neignours_constr)\n",
    "        \n",
    "        loss_backward += apenalty\n",
    "        \n",
    "        # Backpropagate\n",
    "        loss_backward.backward()\n",
    "        \n",
    "        return latent.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UkBBg5DtieWV"
   },
   "outputs": [],
   "source": [
    "# from latent to mesh/point\n",
    "with torch.no_grad():\n",
    "    ply_mesh = create_mesh( decoder,\n",
    "                            latent,\n",
    "                            N=N,\n",
    "                            max_batch=int(2 ** 18),\n",
    "                            offset=None,\n",
    "                            scale=None)\n",
    "points = torch.cuda.FloatTensor(np.hstack(( ply_mesh['vertex']['x'][:, None], \n",
    "                                            ply_mesh['vertex']['y'][:, None], \n",
    "                                            ply_mesh['vertex']['z'][:, None])))\n",
    "\n",
    "# from mesh to pressure field\n",
    "points.requires_grad = True\n",
    "mesh = make_mesh_from_points(points, ply_mesh)\n",
    "local_preds = model(mesh)\n",
    "loss = calculate_loss(mesh, local_preds, axis=axis, constraint_rad=constraint_rad)\n",
    "\n",
    "\n",
    "\n",
    "loss.backward()\n",
    "dL_dp = points.grad.clone()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# calculate mesh normal\n",
    "points.grad.data.zero_()\n",
    "sdf_value = decode_sdf(decoder, latent, points)\n",
    "sdf_value.backward(torch.ones([len(points), 1], dtype=torch.float32).cuda())\n",
    "\n",
    "# assemble constant \n",
    "mults = [-p1.dot(p2) for p1, p2 in zip(dL_dp, points.grad)]       \n",
    "multipliers = torch.cuda.FloatTensor(mults)\n",
    "\n",
    "\n",
    "\n",
    "# get gradient of sdf w.r.t. latent\n",
    "#optimizer.zero_grad()\n",
    "latent.grad.data.zero_()\n",
    "sdf_value = torch.squeeze(.decode_sdf(decoder, latent, points))\n",
    "final_loss = torch.sum(sdf_value * multipliers)\n",
    "final_loss.backward()\n",
    "\n",
    "# artificial loss\n",
    "apenalty = soft_constraints(latent, latent_vectors, num_neignours_constr)\n",
    "apenalty.backward()\n",
    "\n",
    "#print(\"Latent grad penalized: \", torch.sum(latent.grad ** 2))\n",
    "\n",
    "#optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zovrDIQKieWZ"
   },
   "outputs": [],
   "source": [
    "def method4_to_arbitatry_loss(points, ply_mesh, model, constraint_rad=0.1, axis=0):\n",
    "    initial_dir = points.grad.clone()\n",
    "    points.grad.data.zero_()\n",
    "\n",
    "    mesh = make_mesh_from_points(points, ply_mesh)\n",
    "    #signs = compute_signs_for_loss(mesh, transformPoints(normals, AvgTransform))\n",
    "    local_preds = model(mesh)\n",
    "    loss = calculate_loss(mesh, local_preds, axis=axis, constraint_rad=constraint_rad)\n",
    "    loss.backward()\n",
    "\n",
    "    sign = [-p1.dot(p2) for p1, p2 in zip(initial_dir, points.grad)]\n",
    "    \n",
    "    return sign, loss, local_preds, mesh\n",
    "\n",
    "\n",
    "\n",
    "def optimize_shape_deepSDF(decoder, latent, initial_points=None, num_points=None, \n",
    "                           num_iters=100, point_iters=100, num_neignours_constr=10,\n",
    "                           lr=0.2, decreased_by=2, adjust_lr_every=10, alpha_penalty=0.05,\n",
    "                           multiplier_func=method4_to_arbitatry_loss, verbose=None, save_to_dir=None, N=256):\n",
    "\n",
    "    def adjust_learning_rate(\n",
    "        initial_lr, optimizer, num_iterations, decreased_by, adjust_lr_every\n",
    "    ):\n",
    "        lr = initial_lr * ((1 / decreased_by) ** (num_iterations // adjust_lr_every)) \\\n",
    "                        * ((punch_lr_at_reindex_by) ** (num_iterations // reindex_latent_each))\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "            \n",
    "        return lr\n",
    "    \n",
    "    if not os.path.exists(os.path.join(save_to_dir, 'meshes')):\n",
    "        os.makedirs(os.path.join(save_to_dir, 'meshes'))\n",
    "    if not os.path.exists(os.path.join(save_to_dir, 'predictions')):\n",
    "        os.makedirs(os.path.join(save_to_dir, 'predictions'))\n",
    "\n",
    "    ref_latent = latent.clone().detach()\n",
    "    decoder.eval()\n",
    "    latent = latent.clone()\n",
    "    latent.requires_grad = True\n",
    "    optimizer = torch.optim.SGD([latent], lr=lr)\n",
    "\n",
    "    loss_plot = []\n",
    "    latent_dist = []\n",
    "    lr_plot = []\n",
    "    latent_plot = []\n",
    "\n",
    "    for i in range(num_iters):\n",
    "\n",
    "        time_start = time.time()\n",
    "\n",
    "        save_path = os.path.join(save_to_dir, 'meshes/' + str(i).zfill(5) + \".ply\")\n",
    "        preds_save_path = os.path.join(save_to_dir, 'predictions/' + str(i).zfill(5) + \".npy\")\n",
    "\n",
    "\n",
    "        cur_rl = adjust_learning_rate(lr, optimizer, i, decreased_by, adjust_lr_every)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            ply_mesh = create_mesh( decoder,\n",
    "                                    latent,\n",
    "                                    N=N,\n",
    "                                    max_batch=int(2 ** 18),\n",
    "                                    offset=None,\n",
    "                                    scale=None)\n",
    "\n",
    "        points = torch.cuda.FloatTensor(np.hstack(( ply_mesh['vertex']['x'][:, None], \n",
    "                                                    ply_mesh['vertex']['y'][:, None], \n",
    "                                                    ply_mesh['vertex']['z'][:, None])))\n",
    "\n",
    "        points.requires_grad = True\n",
    "\n",
    "        sdf_value = decode_sdf(decoder, latent, points)\n",
    "        sdf_value.backward(torch.ones([len(points), 1], dtype=torch.float32).cuda())\n",
    "\n",
    "        mults, loss_value, preds, transformed_mesh = multiplier_func(points, ply_mesh)         \n",
    "        multipliers = torch.cuda.FloatTensor(mults)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        sdf_value = torch.squeeze(decode_sdf(decoder, latent, points))\n",
    "\n",
    "        final_loss = torch.sum(sdf_value * multipliers)\n",
    "        final_loss.backward()\n",
    "        print(\"backward loss: \", final_loss)\n",
    "\n",
    "        # Soft-constraints\n",
    "        distances, indeces = LATENT_KD_TREE.query(latent.cpu().detach(), k=num_neignours_constr)\n",
    "        penalty = torch.mean(\n",
    "                    torch.stack([torch.sum( \n",
    "                                    (latent - latent_vectors[indeces[0][i]]) ** 2\n",
    "                                 )\n",
    "                                 for i in range(len(indeces[0]))]\n",
    "                               )\n",
    "                    )\n",
    "        apenalty = penalty * alpha_penalty\n",
    "        apenalty.backward()\n",
    "        print(\"penality: \", apenalty)\n",
    "        print(\"together: \", apenalty + final_loss, \"\\n\")\n",
    "\n",
    "        optimizer.step()\n",
    "       \n",
    "\n",
    "        tri_mesh = get_trimesh_from_torch_geo_with_colors(transformed_mesh, preds)\n",
    "        tri_mesh.export(save_path)\n",
    "        #np.save(preds_save_path, preds.cpu().detach().numpy())\n",
    "\n",
    "        #if save_to_dir is not None:\n",
    "        #    plot_points_from_torch\n",
    "\n",
    "        loss_plot.append(loss_value.cpu().detach().numpy())\n",
    "        latent_dist.append(torch.sum((latent - ref_latent) ** 2 ).cpu().detach().numpy() )\n",
    "        latent_plot.append(latent.detach().cpu().numpy())\n",
    "        lr_plot.append(penalty)\n",
    "\n",
    "        time_end = time.time()\n",
    "\n",
    "        if verbose is not None and i % verbose == 0:\n",
    "            print('Iter ', i, 'Loss: ', loss_value.detach().cpu().numpy(), ' LD: ', lr_plot[-1])\n",
    "    \n",
    "        np.save(os.path.join(save_to_dir, \"latent_plot.npy\"), latent_plot)    \n",
    "        np.save(os.path.join(save_to_dir, \"loss_plot.npy\"), loss_plot)\n",
    "        np.save(os.path.join(save_to_dir, \"latent_dist.npy\"), latent_dist)\n",
    "        np.save(os.path.join(save_to_dir, \"lr_plot.npy\"), lr_plot)\n",
    "\n",
    "\n",
    "\n",
    "def make_full_transformation(initial_latent, experiment_name, \n",
    "                             decoder, model, alpha_penalty=0.05, constraint_rad=0.1, axis=0, **kwargs):\n",
    "    '''\n",
    "    kwargs:\n",
    "        num_iters=1000, \n",
    "        adjust_lr_every=10, \n",
    "        decreased_by=1.2,\n",
    "        lr=0.005\n",
    "        verbose=10,\n",
    "    '''\n",
    "\n",
    "    #ref_points = get_points_from_latent(decoder, ref_latent, N=128)\n",
    "    save_to_dir = experiment_name\n",
    "    if not os.path.exists(save_to_dir):\n",
    "        os.makedirs(save_to_dir)\n",
    "\n",
    "    #np.save(os.path.join(save_to_dir, \"target_verts.npy\"), ref_points)\n",
    "\n",
    "    optimize_shape_deepSDF(decoder, initial_latent, initial_points=None,\n",
    "                                           alpha_penalty=alpha_penalty,\n",
    "                                           num_points=None, point_iters=2,\n",
    "                                           multiplier_func=lambda x, y: \n",
    "                                               method4_to_arbitatry_loss(x, y, model, \n",
    "                                                                         constraint_rad=constraint_rad, \n",
    "                                                                         axis=axis),\n",
    "                                           save_to_dir=save_to_dir, **kwargs)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iJJyuM71ieWb"
   },
   "outputs": [],
   "source": [
    "\n",
    "DIR_for_dump_data = './data_for_this_experiments'\n",
    "punch_lr_at_reindex_by=1\n",
    "reindex_latent_each = 10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f_H2o92JieWf"
   },
   "outputs": [],
   "source": [
    "np.random.seed(101)\n",
    "make_full_transformation(LATENT_TO_OPTIMIZE.detach(),\n",
    "                         experiment_name=DIR_for_dump_data, decoder=decoder, model=model,\n",
    "                         alpha_penalty=0.2, axis=0,\n",
    "                         constraint_rad=0.05,\n",
    "                         num_iters=30,\n",
    "                         adjust_lr_every=20,\n",
    "                         decreased_by=1.1, \n",
    "                         lr=0.2,\n",
    "                         verbose=1,\n",
    "                         N=256,\n",
    "                         num_neignours_constr=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s-LdeJ1zieWi"
   },
   "outputs": [],
   "source": [
    "#preds = np.load(\"data_for_this_experiments/OptimizationPaper/predictions/00000.npy\")\n",
    "mesh = trimesh.load_mesh(\"data_for_this_experiments/meshes/00029.ply\")\n",
    "loss_plot = np.load(\"data_for_this_experiments/loss_plot.npy\", allow_pickle=True)\n",
    "lr_plot = np.load(\"data_for_this_experiments/lr_plot.npy\", allow_pickle=True)\n",
    "latent_plot = np.load(\"data_for_this_experiments/latent_plot.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(loss_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances, indeces = LATENT_KD_TREE.query(torch.tensor(latent_plot[-1]), k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indeces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newl = 0.1 * latent_vectors[indeces.squeeze()[0]] + 0.9 * latent_vectors[indeces.squeeze()[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visual_Mesh(torch.tensor(newl)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visual_Mesh(torch.tensor(latent_vectors[indeces.squeeze()[:2]].mean(axis=0))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visual_Mesh(torch.tensor(latent_vectors[indeces].mean(axis=0))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visual_Mesh(latent_vectors[267]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visual_Mesh(latent_vectors[350]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visual_Mesh(torch.tensor(latent_plot[-1]).cuda()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(lr_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = trimesh.load_mesh(\"data_for_this_experiments/meshes/00004.ply\")\n",
    "#mesh.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = trimesh.load_mesh(\"data_for_this_experiments/meshes/00029.ply\")\n",
    "#mesh.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances, indeces = LATENT_KD_TREE.query(LATENT_TO_OPTIMIZE.cpu().detach(), k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh['face'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.norm(latent_vectors[indeces.squeeze()[0]] - latent_vectors[indeces.squeeze()[8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.norm(latent_vectors[indeces.squeeze()[9]] - latent_vectors[indeces.squeeze()[8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.norm(latent_vectors[indeces.squeeze()[7]] - latent_vectors[indeces.squeeze()[8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visual_Mesh(ilatent):\n",
    "    ply_mesh = create_mesh(decoder,\n",
    "                        ilatent,\n",
    "                        N=256,\n",
    "                        max_batch=int(2 ** 18))\n",
    "    points = torch.cuda.FloatTensor(np.hstack(( ply_mesh['vertex']['x'][:, None], \n",
    "                                            ply_mesh['vertex']['y'][:, None], \n",
    "                                            ply_mesh['vertex']['z'][:, None])))\n",
    "    scaled_mesh = make_mesh_from_points(points, ply_mesh)\n",
    "    pressure_field = model(scaled_mesh)\n",
    "    loss = compute_lift_faces_diff(scaled_mesh, pressure_field, axis=0)  \n",
    "    print(\"latent loss. %f \"%(loss))\n",
    "    return get_trimesh_from_torch_geo_with_colors(scaled_mesh, pressure_field)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visual_Mesh(latent_vectors[26]).show() # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wZ85ST4pieWl"
   },
   "outputs": [],
   "source": [
    "mesh37 = trimesh.load_mesh(\"Expirements/OptimizationPaper/meshes/00037.ply\")\n",
    "mesh37.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cY9La8xqieWm"
   },
   "outputs": [],
   "source": [
    "mesh = {'point_pos': tansformed_points, \n",
    "        'edge_vec':torch.tensor(edge_attr, dtype=torch.float).to('cuda:0'),\n",
    "        'edge_index':torch.tensor(edges, dtype=torch.long).t().contiguous().to('cuda:0')\n",
    "        }"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "checkCFDloss.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
