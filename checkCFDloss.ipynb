{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-wgCgJ6AieVb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import skimage.measure\n",
    "import plyfile\n",
    "from plyfile import PlyData\n",
    "from sklearn.neighbors import KDTree\n",
    "import trimesh\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import (NNConv, GMMConv, GraphConv, Set2Set)\n",
    "from torch_geometric.nn import (SplineConv, graclus, max_pool, max_pool_x, global_mean_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "DtBoNNSMieVi",
    "outputId": "86d328bb-69b8-4e45-baf7-d2e03557e517"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.1+cu101\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 116,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "f6JlQ7MWzsDx",
    "outputId": "ce243b5e-97f2-4d59-f35e-3a4a7fa026eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2019 NVIDIA Corporation\n",
      "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
      "Cuda compilation tools, release 10.1, V10.1.243\n"
     ]
    }
   ],
   "source": [
    "! nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "3oINuP6Sz5B8",
    "outputId": "05dae263-cb5f-4e55-9d75-27d49680e9f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jul 31 14:39:41 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.51.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   73C    P0    76W / 149W |  11344MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                 ERR! |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zuy6igvMnjMZ"
   },
   "outputs": [],
   "source": [
    "! mkdir networks\n",
    "! mv ../deep_sdf_decoder.py networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_U0sLodQi_fS"
   },
   "outputs": [],
   "source": [
    "!pip install plyfile\n",
    "!pip install trimesh\n",
    "!pip install torch-scatter==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
    "!pip install torch-sparse==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
    "!pip install torch-cluster==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
    "!pip install torch-spline-conv==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
    "!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "f6caIMKgieVz",
    "outputId": "000355cc-790d-4c58-cddf-71829e6698ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([32.0000,  2.3000], device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([32,2.3]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZQDXwZHwieV4"
   },
   "outputs": [],
   "source": [
    "model_params_subdir = \"ModelParameters\"\n",
    "optimizer_params_subdir = \"OptimizerParameters\"\n",
    "latent_codes_subdir = \"LatentCodes\"\n",
    "logs_filename = \"Logs.pth\"\n",
    "reconstructions_subdir = \"Reconstructions\"\n",
    "reconstruction_meshes_subdir = \"Meshes\"\n",
    "reconstruction_codes_subdir = \"Codes\"\n",
    "specifications_filename = \"specs.json\"\n",
    "data_source_map_filename = \".datasources.json\"\n",
    "evaluation_subdir = \"Evaluation\"\n",
    "sdf_samples_subdir = \"SdfSamples\"\n",
    "surface_samples_subdir = \"SurfaceSamples\"\n",
    "normalization_param_subdir = \"NormalizationParameters\"\n",
    "training_meshes_subdir = \"TrainingMeshes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5vlOE0_iieV8"
   },
   "outputs": [],
   "source": [
    "def load_latent_vectors(experiment_directory, checkpoint):\n",
    "\n",
    "    filename = os.path.join(\n",
    "        experiment_directory, checkpoint + \".pth\"\n",
    "    )\n",
    "    if not os.path.isfile(filename):\n",
    "        raise Exception(\n",
    "            \"The experiment directory ({}) does not include a latent code file\"\n",
    "            + \" for checkpoint '{}'\".format(experiment_directory, checkpoint)\n",
    "        )\n",
    "    data = torch.load(filename)\n",
    "    return data[\"latent_codes\"].cuda()\n",
    "\n",
    "def load_model(experiment_directory, checkpoint):\n",
    "    specs_filename = os.path.join(experiment_directory, \"specs.json\")\n",
    "\n",
    "    if not os.path.isfile(specs_filename):\n",
    "        raise Exception(\n",
    "            'The experiment directory does not include specifications file \"specs.json\"'\n",
    "        )\n",
    "\n",
    "    specs = json.load(open(specs_filename))\n",
    "\n",
    "    arch = __import__(\"networks.\" + specs[\"NetworkArch\"], fromlist=[\"Decoder\"])\n",
    "\n",
    "    latent_size = specs[\"CodeLength\"]\n",
    "\n",
    "    decoder = arch.Decoder(latent_size, **specs[\"NetworkSpecs\"])\n",
    "\n",
    "    decoder = torch.nn.DataParallel(decoder)\n",
    "\n",
    "    saved_model_state = torch.load(\n",
    "        os.path.join(experiment_directory, checkpoint + \".pth\")\n",
    "    )\n",
    "\n",
    "    decoder.load_state_dict(saved_model_state[\"model_state_dict\"])\n",
    "\n",
    "    decoder = decoder.module.cuda()\n",
    "\n",
    "    decoder.eval()\n",
    "    \n",
    "    return decoder\n",
    "\n",
    "def create_mesh(\n",
    "    decoder, latent_vec, filename='', N=256, max_batch=32 ** 3, offset=None, scale=None\n",
    "):\n",
    "    start = time.time()\n",
    "    ply_filename = filename\n",
    "\n",
    "    decoder.eval()\n",
    "\n",
    "    # NOTE: the voxel_origin is actually the (bottom, left, down) corner, not the middle\n",
    "    voxel_origin = [-1, -1, -1]\n",
    "    voxel_size = 2.0 / (N - 1)\n",
    "\n",
    "    overall_index = torch.arange(0, N ** 3, 1, out=torch.LongTensor())\n",
    "    samples = torch.zeros(N ** 3, 4)\n",
    "\n",
    "    # transform first 3 columns\n",
    "    # to be the x, y, z index\n",
    "    samples[:, 2] = overall_index % N\n",
    "    samples[:, 1] = (overall_index.long() / N) % N\n",
    "    samples[:, 0] = ((overall_index.long() / N) / N) % N\n",
    "\n",
    "    # transform first 3 columns\n",
    "    # to be the x, y, z coordinate\n",
    "    samples[:, 0] = (samples[:, 0] * voxel_size) + voxel_origin[2]\n",
    "    samples[:, 1] = (samples[:, 1] * voxel_size) + voxel_origin[1]\n",
    "    samples[:, 2] = (samples[:, 2] * voxel_size) + voxel_origin[0]\n",
    "\n",
    "    num_samples = N ** 3\n",
    "\n",
    "    samples.requires_grad = False\n",
    "\n",
    "    head = 0\n",
    "\n",
    "    while head < num_samples:\n",
    "        sample_subset = samples[head : min(head + max_batch, num_samples), 0:3].cuda()\n",
    "\n",
    "        samples[head : min(head + max_batch, num_samples), 3] = \\\n",
    "                decode_sdf(decoder, latent_vec, sample_subset).squeeze(1).detach().cpu()\n",
    "        head += max_batch\n",
    "\n",
    "    sdf_values = samples[:, 3]\n",
    "    sdf_values = sdf_values.reshape(N, N, N)\n",
    "\n",
    "    end = time.time()\n",
    "    #print(\"sampling takes: %f\" % (end - start))\n",
    "\n",
    "    return convert_sdf_samples_to_ply(\n",
    "        sdf_values.data.cpu(),\n",
    "        voxel_origin,\n",
    "        voxel_size,\n",
    "        ply_filename + \".ply\",\n",
    "        offset,\n",
    "        scale,\n",
    "    )\n",
    "\n",
    "def convert_sdf_samples_to_ply(\n",
    "    pytorch_3d_sdf_tensor,\n",
    "    voxel_grid_origin,\n",
    "    voxel_size,\n",
    "    ply_filename_out,\n",
    "    offset=None,\n",
    "    scale=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Convert sdf samples to .ply\n",
    "\n",
    "    :param pytorch_3d_sdf_tensor: a torch.FloatTensor of shape (n,n,n)\n",
    "    :voxel_grid_origin: a list of three floats: the bottom, left, down origin of the voxel grid\n",
    "    :voxel_size: float, the size of the voxels\n",
    "    :ply_filename_out: string, path of the filename to save to\n",
    "\n",
    "    This function adapted from: https://github.com/RobotLocomotion/spartan\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    numpy_3d_sdf_tensor = pytorch_3d_sdf_tensor.numpy()\n",
    "\n",
    "    verts, faces, normals, values = skimage.measure.marching_cubes_lewiner(\n",
    "        numpy_3d_sdf_tensor, level=0.0, spacing=[voxel_size] * 3\n",
    "    )\n",
    "\n",
    "    # transform from voxel coordinates to camera coordinates\n",
    "    # note x and y are flipped in the output of marching_cubes\n",
    "    mesh_points = np.zeros_like(verts)\n",
    "    mesh_points[:, 0] = voxel_grid_origin[0] + verts[:, 0]\n",
    "    mesh_points[:, 1] = voxel_grid_origin[1] + verts[:, 1]\n",
    "    mesh_points[:, 2] = voxel_grid_origin[2] + verts[:, 2]\n",
    "\n",
    "    # apply additional offset and scale\n",
    "    if scale is not None:\n",
    "        mesh_points = mesh_points / scale\n",
    "    if offset is not None:\n",
    "        mesh_points = mesh_points - offset\n",
    "\n",
    "    # try writing to the ply file\n",
    "\n",
    "    num_verts = verts.shape[0]\n",
    "    num_faces = faces.shape[0]\n",
    "\n",
    "    verts_tuple = np.zeros((num_verts,), dtype=[(\"x\", \"f4\"), (\"y\", \"f4\"), (\"z\", \"f4\")])\n",
    "    norms_tuple = np.zeros((num_verts,), dtype=[(\"x\", \"f4\"), (\"y\", \"f4\"), (\"z\", \"f4\")])\n",
    "\n",
    "    for i in range(0, num_verts):\n",
    "        verts_tuple[i] = tuple(mesh_points[i, :])\n",
    "        norms_tuple[i] = tuple(normals[i, :])\n",
    "\n",
    "    faces_building = []\n",
    "    for i in range(0, num_faces):\n",
    "        faces_building.append(((faces[i, :].tolist(),)))\n",
    "    faces_tuple = np.array(faces_building, dtype=[(\"vertex_indices\", \"i4\", (3,))])\n",
    "\n",
    "    el_verts = plyfile.PlyElement.describe(verts_tuple, \"vertex\")\n",
    "    el_faces = plyfile.PlyElement.describe(faces_tuple, \"face\")\n",
    "    el_norms = plyfile.PlyElement.describe(norms_tuple, \"normals\")\n",
    "\n",
    "    ply_data = plyfile.PlyData([el_verts, el_faces, el_norms])\n",
    "    return ply_data\n",
    "\n",
    "def decode_sdf(decoder, latent_vector, queries):\n",
    "    num_samples = queries.shape[0]\n",
    "\n",
    "    if latent_vector is None:\n",
    "        inputs = queries\n",
    "    else:\n",
    "        latent_repeat = latent_vector.expand(num_samples, -1)\n",
    "        inputs = torch.cat([latent_repeat, queries], 1)\n",
    "\n",
    "    sdf = decoder(inputs)\n",
    "\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-JM2oaapieWC"
   },
   "outputs": [],
   "source": [
    "def compute_lift_faces_diff(data_instance, answers, axis=0):\n",
    "    pressures = torch.mean(answers[data_instance['face'], 0], axis=0)\n",
    "\n",
    "    # TODO: cahnge to x if needed\n",
    "    pos = data_instance['x']\n",
    "    cross_prod = (pos[data_instance['face'][1]] - pos[data_instance['face'][0]]).cross(\n",
    "                  pos[data_instance['face'][2]] - pos[data_instance['face'][0]])\n",
    "    mult = -cross_prod[:, axis] / 2\n",
    "    lift = torch.mul(pressures, mult)\n",
    "    return torch.sum(lift[~torch.isnan(lift)])\n",
    "\n",
    "def boundsLoss(points, box=[(-1, 1, 0)]):\n",
    "    loss = 0\n",
    "    for l, r, i in box:\n",
    "        loss +=  torch.mean(F.relu(-points[:, i] + l))  \\\n",
    "               + torch.mean(F.relu( points[:, i] - r))\n",
    "    return loss\n",
    "\n",
    "def innerBoundsLoss(points, r=1, center=(0, 0, 0)):\n",
    "    radiuses = torch.sum( (points - torch.Tensor(center).to('cuda:0')) ** 2 , dim=1)\n",
    "    return torch.mean(F.relu(r - radiuses))\n",
    "\n",
    "def calculate_loss(mesh, local_preds, axis=0, constraint_rad=0.1):\n",
    "    loss =  (1 - axis) * compute_lift_faces_diff(mesh, local_preds, axis=0) + \\\n",
    "                  axis * compute_lift_faces_diff(mesh, local_preds, axis=1)\n",
    "    \n",
    "    loss += boundsLoss(mesh['x'], box=[(-0.6, 0.6, 0)])\n",
    "    loss += innerBoundsLoss(mesh['x'], r=constraint_rad**2, center=(-0.05, 0.05, 0))  \\\n",
    "          + innerBoundsLoss(mesh['x'], r=(constraint_rad / 2)**2, center=(0.3, 0, 0))\n",
    "    return loss\n",
    "\n",
    "def soft_constraints(latent, latent_vectors, num_neignours_constr, alpha_penalty):\n",
    "    # Soft-constraints\n",
    "    distances, indeces = LATENT_KD_TREE.query(latent.cpu().detach(), k=num_neignours_constr)\n",
    "    torch.sum((initial_la - latent_vectors[indeces.squeeze()]) ** 2, dim=2).mean()\n",
    "    return penalty * alpha_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-I_hhsIwieWF"
   },
   "outputs": [],
   "source": [
    "def computeAvgTransform():\n",
    "    objects = list()\n",
    "    for (dirpath, dirnames, filenames) in os.walk(\"/cvlabdata2/home/artem/Data/cars_remeshed_dsdf/transforms/\"):\n",
    "        objects += [os.path.join(dirpath, file) for file in filenames if file[-4:] == '.npy']\n",
    "    \n",
    "    matricies = []\n",
    "    for obj in objects:\n",
    "        matricies.append(np.load(obj))\n",
    "    \n",
    "    return np.mean(np.array(matricies), axis=0)\n",
    "\n",
    "def transformPoints(points):\n",
    "    matrix = torch.cuda.FloatTensor(AvgTransform)\n",
    "    column = torch.zeros((len(points), 1), device=\"cuda:0\") + 1\n",
    "    stacked = torch.cat([points, column], dim=1)\n",
    "    transformed = torch.matmul(matrix, stacked.t()).t()[:, :3]\n",
    "    return transformed\n",
    "\n",
    "def make_mesh_from_points(points, ply_mesh):\n",
    "    transformed_points = transformPoints(points)\n",
    "    \n",
    "    edges = trimesh.geometry.faces_to_edges(ply_mesh['face']['vertex_indices'])\n",
    "    np_points = transformed_points.cpu().detach().numpy()\n",
    "    edge_attr = [np_points[a] - np_points[b] for a, b in edges]\n",
    "    mesh = {'x': transformed_points, \n",
    "        'face':torch.tensor(ply_mesh['face']['vertex_indices'], dtype=torch.long).to('cuda:0').t(),\n",
    "        'edge_attr':torch.tensor(edge_attr, dtype=torch.float).to('cuda:0'),\n",
    "        'edge_index':torch.tensor(edges, dtype=torch.long).t().contiguous().to('cuda:0')\n",
    "        }\n",
    "    return mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zw1U5NI-ieWJ"
   },
   "outputs": [],
   "source": [
    "class SplineBlock(nn.Module):\n",
    "    def __init__(self, num_in_features, num_outp_features, mid_features, kernel=3, dim=3, batchnorm1=True):\n",
    "        super(SplineBlock, self).__init__()\n",
    "        self.batchnorm1 = batchnorm1\n",
    "        self.conv1 = SplineConv(num_in_features, mid_features, dim, kernel, is_open_spline=False)\n",
    "        if self.batchnorm1:\n",
    "            self.batchnorm1 = torch.nn.BatchNorm1d(mid_features)\n",
    "        self.conv2 = SplineConv(mid_features, 2 * mid_features, dim, kernel, is_open_spline=False)\n",
    "        self.batchnorm2 = torch.nn.BatchNorm1d(2 * mid_features)\n",
    "        self.conv3 = SplineConv(2 * mid_features + 3, num_outp_features, dim, kernel, is_open_spline=False)\n",
    "  \n",
    "    def forward(self, res, data):\n",
    "        if self.batchnorm1:\n",
    "            res = F.elu(self.batchnorm1(self.conv1(res, data['edge_index'], data['edge_attr'])))\n",
    "        else:\n",
    "            res = F.elu(self.conv1(res, data['edge_index'], data['edge_attr']))\n",
    "        res = F.elu(self.batchnorm2(self.conv2(res, data['edge_index'], data['edge_attr'])))\n",
    "#         res = F.elu(self.conv2(res, data.edge_index, data.edge_attr))\n",
    "        res = torch.cat([res, data['x']], dim=1)\n",
    "        res = self.conv3(res, data['edge_index'], data['edge_attr'])\n",
    "        return res\n",
    "\n",
    "class SplineCNN8Residuals(nn.Module):\n",
    "    def __init__(self, num_features, kernel=3, dim=3):\n",
    "        super(SplineCNN8Residuals, self).__init__()\n",
    "        self.block1 = SplineBlock(num_features, 16, 8, kernel, dim)\n",
    "        self.block2 = SplineBlock(16, 64, 32, kernel, dim)\n",
    "        self.block3 = SplineBlock(64, 64, 128, kernel, dim)\n",
    "        self.block4 = SplineBlock(64, 8, 16, kernel, dim)\n",
    "        self.block5 = SplineBlock(11, 32, 16, kernel, dim)\n",
    "        self.block6 = SplineBlock(32, 64, 32, kernel, dim)\n",
    "        self.block7 = SplineBlock(64, 64, 128, kernel, dim)\n",
    "        self.block8 = SplineBlock(75, 4, 16, kernel, dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        res = data['x']\n",
    "        res = self.block1(res, data)\n",
    "        res = self.block2(res, data)\n",
    "        res = self.block3(res, data)\n",
    "        res4 = self.block4(res, data)\n",
    "        res = torch.cat([res4, data['x']], dim=1)\n",
    "        res = self.block5(res, data)\n",
    "        res = self.block6(res, data)\n",
    "        res = self.block7(res, data)\n",
    "        res = torch.cat([res, res4, data['x']], dim=1)\n",
    "        res = self.block8(res, data)\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OwDmuXRZieWL"
   },
   "outputs": [],
   "source": [
    "DIR_for_dump_data = './data_for_this_experiments'\n",
    "experiment_directory = DIR_for_dump_data\n",
    "\n",
    "model = SplineCNN8Residuals(3)\n",
    "model.load_state_dict(torch.load(experiment_directory + \"/cfdModel.nn\"))\n",
    "model = model.to(\"cuda:0\")\n",
    "model = model.eval()\n",
    "\n",
    "decoder = load_model(experiment_directory, \"decoderModel\")\n",
    "latent_vectors = load_latent_vectors(experiment_directory, \"latentCodes\")\n",
    "latent_vectors = latent_vectors.detach()\n",
    "\n",
    "LATENT_TO_OPTIMIZE = latent_vectors[32]\n",
    "LATENT_KD_TREE = KDTree(np.array([lv.cpu().detach().numpy()[0] for lv in latent_vectors]))\n",
    "AvgTransform = np.load(DIR_for_dump_data + \"/avg_trans_matrix.npy\") #computeAvgTransform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j20y4chdieWN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "V863f0ccmme7",
    "outputId": "ed10375f-2c29-45ec-9a2b-207d7fed98b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3987, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 189,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "_j5ogXlB0hfG",
    "outputId": "4193bfb1-a7e1-489d-b699-649fa806a951"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jul 31 12:53:53 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.51.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   73C    P0    76W / 149W |  10670MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                 ERR! |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w3z7xq1QieWR"
   },
   "outputs": [],
   "source": [
    "initial_la = latent_vectors[32]\n",
    "ply_mesh = create_mesh( decoder,\n",
    "                        initial_la,\n",
    "                        N=256,\n",
    "                        max_batch=int(2 ** 18),\n",
    "                        offset=None,\n",
    "                        scale=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1HL3A11cx3wm"
   },
   "outputs": [],
   "source": [
    "points = torch.cuda.FloatTensor(np.hstack(( ply_mesh['vertex']['x'][:, None], \n",
    "                                            ply_mesh['vertex']['y'][:, None], \n",
    "                                            ply_mesh['vertex']['z'][:, None])))\n",
    "\n",
    "# from mesh to pressure field\n",
    "points.requires_grad = True\n",
    "mesh = make_mesh_from_points(points, ply_mesh)\n",
    "#del ply_mesh, points\n",
    "local_preds = model(mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kQgi519pBA_L"
   },
   "outputs": [],
   "source": [
    "loss = calculate_loss(mesh, local_preds, axis=0, constraint_rad=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ayzvv36zI1S5"
   },
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "dL_dp = points.grad.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hr_1kLvqJTQ0"
   },
   "outputs": [],
   "source": [
    "points.grad.data.zero_()\n",
    "sdf_value = decode_sdf(decoder, initial_la, points)\n",
    "sdf_value.backward(torch.ones([len(points), 1], dtype=torch.float32).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BLPn3Y81J5Ab"
   },
   "outputs": [],
   "source": [
    "# assemble constant \n",
    "mults = [-p1.dot(p2) for p1, p2 in zip(dL_dp, points.grad)]       \n",
    "multipliers = torch.cuda.FloatTensor(mults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y7RtRhTUJewl"
   },
   "outputs": [],
   "source": [
    "points = points.detach()\n",
    "initial_la = initial_la.detach().requires_grad_(True)\n",
    "latent_inputs = initial_la.expand(points.shape[0], -1)\n",
    "inputs = torch.cat([latent_inputs, points], 1).cuda() \n",
    "sdf_value = decoder(inputs)\n",
    "final_loss = torch.sum(sdf_value.squeeze() * multipliers)\n",
    "final_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "colab_type": "code",
    "id": "n1cxfRK9ozBp",
    "outputId": "fba5cec2-6a64-4275-f58e-9752fbdebedb"
   },
   "outputs": [],
   "source": [
    "#initial_la.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "id": "T440V-0ao248",
    "outputId": "b0ddcc89-4900-4f2b-8403-228a8fea66f3"
   },
   "outputs": [],
   "source": [
    "#initial_la.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-3A3dHkwYmKs"
   },
   "outputs": [],
   "source": [
    "apenalty = soft_constraints(initial_la, latent_vectors, num_neignours_constr=10, alpha_penalty=0.2)\n",
    "apenalty.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 901
    },
    "colab_type": "code",
    "id": "MJJO70Umo7LV",
    "outputId": "4b78bd74-8ce6-4c60-f209-a7eafae12fa0"
   },
   "outputs": [],
   "source": [
    "#initial_la.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "aTs5dZL0CuEg",
    "outputId": "2ef245aa-ee5b-498c-bad0-054458a7631d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0002, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 191,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fu57UXaPofQ9",
    "outputId": "d943245e-229e-42a9-e299-264325375b4c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3987, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 192,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "kpIdfkoR_P4K",
    "outputId": "8cc7c36f-56c3-4769-ac0b-5234b45d4ff9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(966.7463, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "innerBoundsLoss(mesh['x'], r=(0.5 / 2)**2, center=(0.3, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "T-pitvgF_znG",
    "outputId": "395691c7-70f2-4a76-c31b-21d710e385e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2548, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boundsLoss(mesh['x'], box=[(-0.006, 0.006, 0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "dsOoQcbC1ai1",
    "outputId": "ec93170e-e62f-4933-dd4a-b5debd0f3c2a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.7641, -0.4888, -0.5677,  0.0418], device='cuda:0',\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ccq6n-_m1h4f",
    "outputId": "efc9c605-a1c0-4b2d-e527-31090152df0f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100733, 3])"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6y6WBozCieWT"
   },
   "outputs": [],
   "source": [
    "ply_mesh.write(\"data_for_this_experiments/mesh32.ply\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UkBBg5DtieWV"
   },
   "outputs": [],
   "source": [
    "# from latent to mesh/point\n",
    "with torch.no_grad():\n",
    "    ply_mesh = create_mesh( decoder,\n",
    "                            latent,\n",
    "                            N=N,\n",
    "                            max_batch=int(2 ** 18),\n",
    "                            offset=None,\n",
    "                            scale=None)\n",
    "points = torch.cuda.FloatTensor(np.hstack(( ply_mesh['vertex']['x'][:, None], \n",
    "                                            ply_mesh['vertex']['y'][:, None], \n",
    "                                            ply_mesh['vertex']['z'][:, None])))\n",
    "\n",
    "# from mesh to pressure field\n",
    "points.requires_grad = True\n",
    "mesh = make_mesh_from_points(points, ply_mesh)\n",
    "local_preds = model(mesh)\n",
    "loss = calculate_loss(mesh, local_preds, axis=axis, constraint_rad=constraint_rad)\n",
    "loss.backward()\n",
    "dL_dp = points.grad.clone()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# calculate mesh normal\n",
    "points.grad.data.zero_()\n",
    "sdf_value = decode_sdf(decoder, latent, points)\n",
    "sdf_value.backward(torch.ones([len(points), 1], dtype=torch.float32).cuda())\n",
    "\n",
    "# assemble constant \n",
    "mults = [-p1.dot(p2) for p1, p2 in zip(dL_dp, points.grad)]       \n",
    "multipliers = torch.cuda.FloatTensor(mults)\n",
    "\n",
    "\n",
    "\n",
    "# get gradient of sdf w.r.t. latent\n",
    "#optimizer.zero_grad()\n",
    "latent.grad.data.zero_()\n",
    "sdf_value = torch.squeeze(.decode_sdf(decoder, latent, points))\n",
    "final_loss = torch.sum(sdf_value * multipliers)\n",
    "final_loss.backward()\n",
    "\n",
    "# artificial loss\n",
    "apenalty = soft_constraints(latent, latent_vectors, num_neignours_constr)\n",
    "apenalty.backward()\n",
    "\n",
    "#print(\"Latent grad penalized: \", torch.sum(latent.grad ** 2))\n",
    "\n",
    "#optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zovrDIQKieWZ"
   },
   "outputs": [],
   "source": [
    "def method4_to_arbitatry_loss(points, ply_mesh, model, constraint_rad=0.1, axis=0):\n",
    "    initial_dir = points.grad.clone()\n",
    "    points.grad.data.zero_()\n",
    "\n",
    "    mesh = make_mesh_from_points(points, ply_mesh)\n",
    "    #signs = compute_signs_for_loss(mesh, transformPoints(normals, AvgTransform))\n",
    "    local_preds = model(mesh)\n",
    "    loss = calculate_loss(mesh, local_preds, axis=axis, constraint_rad=constraint_rad)\n",
    "    loss.backward()\n",
    "\n",
    "    sign = [-p1.dot(p2) for p1, p2 in zip(initial_dir, points.grad)]\n",
    "    \n",
    "    return sign, loss, local_preds, mesh\n",
    "\n",
    "\n",
    "\n",
    "def optimize_shape_deepSDF(decoder, latent, initial_points=None, num_points=None, \n",
    "                           num_iters=100, point_iters=100, num_neignours_constr=10,\n",
    "                           lr=0.2, decreased_by=2, adjust_lr_every=10, alpha_penalty=0.05,\n",
    "                           multiplier_func=method4_to_arbitatry_loss, verbose=None, save_to_dir=None, N=256):\n",
    "\n",
    "    def adjust_learning_rate(\n",
    "        initial_lr, optimizer, num_iterations, decreased_by, adjust_lr_every\n",
    "    ):\n",
    "        lr = initial_lr * ((1 / decreased_by) ** (num_iterations // adjust_lr_every)) \\\n",
    "                        * ((punch_lr_at_reindex_by) ** (num_iterations // reindex_latent_each))\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "            \n",
    "        return lr\n",
    "    \n",
    "    if not os.path.exists(os.path.join(save_to_dir, 'meshes')):\n",
    "        os.makedirs(os.path.join(save_to_dir, 'meshes'))\n",
    "    if not os.path.exists(os.path.join(save_to_dir, 'predictions')):\n",
    "        os.makedirs(os.path.join(save_to_dir, 'predictions'))\n",
    "\n",
    "    ref_latent = latent.clone().detach()\n",
    "    decoder.eval()\n",
    "    latent = latent.clone()\n",
    "    latent.requires_grad = True\n",
    "    optimizer = torch.optim.SGD([latent], lr=lr)\n",
    "\n",
    "    loss_plot = []\n",
    "    latent_dist = []\n",
    "    lr_plot = []\n",
    "\n",
    "    for i in range(num_iters):\n",
    "\n",
    "        time_start = time.time()\n",
    "\n",
    "        save_path = os.path.join(save_to_dir, 'meshes/' + str(i).zfill(5) + \".ply\")\n",
    "        preds_save_path = os.path.join(save_to_dir, 'predictions/' + str(i).zfill(5) + \".npy\")\n",
    "\n",
    "\n",
    "        cur_rl = adjust_learning_rate(lr, optimizer, i, decreased_by, adjust_lr_every)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            ply_mesh = create_mesh( decoder,\n",
    "                                    latent,\n",
    "                                    N=N,\n",
    "                                    max_batch=int(2 ** 18),\n",
    "                                    offset=None,\n",
    "                                    scale=None)\n",
    "\n",
    "        points = torch.cuda.FloatTensor(np.hstack(( ply_mesh['vertex']['x'][:, None], \n",
    "                                                    ply_mesh['vertex']['y'][:, None], \n",
    "                                                    ply_mesh['vertex']['z'][:, None])))\n",
    "\n",
    "        points.requires_grad = True\n",
    "\n",
    "        sdf_value = decode_sdf(decoder, latent, points)\n",
    "        sdf_value.backward(torch.ones([len(points), 1], dtype=torch.float32).cuda())\n",
    "\n",
    "        mults, loss_value, preds, transformed_mesh = multiplier_func(points, ply_mesh)         \n",
    "        multipliers = torch.cuda.FloatTensor(mults)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        sdf_value = torch.squeeze(deep_sdf.utils.decode_sdf(decoder, latent, points))\n",
    "\n",
    "        final_loss = torch.sum(sdf_value * multipliers)\n",
    "        final_loss.backward()\n",
    "\n",
    "\n",
    "        # Soft-constraints\n",
    "        distances, indeces = LATENT_KD_TREE.query(latent.cpu().detach(), k=num_neignours_constr)\n",
    "        penalty = torch.mean(\n",
    "                    torch.stack([torch.sum( \n",
    "                                    (latent - latent_vectors[indeces[0][i]]) ** 2\n",
    "                                 )\n",
    "                                 for i in range(len(indeces[0]))]\n",
    "                               )\n",
    "                    )\n",
    "        apenalty = penalty * alpha_penalty\n",
    "        apenalty.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "       \n",
    "\n",
    "        tri_mesh = get_trimesh_from_torch_geo_with_colors(transformed_mesh, preds)\n",
    "        tri_mesh.export(save_path)\n",
    "        np.save(preds_save_path, preds.cpu().detach().numpy())\n",
    "\n",
    "        if save_to_dir is not None:\n",
    "            plot_points_from_torch\n",
    "\n",
    "        loss_plot.append(loss_value.cpu().detach().numpy())\n",
    "        latent_dist.append(torch.sum((latent - ref_latent) ** 2 ).cpu().detach().numpy() )\n",
    "        lr_plot.append(penalty)\n",
    "\n",
    "        time_end = time.time()\n",
    "\n",
    "        if verbose is not None and i % verbose == 0:\n",
    "            print('Iter ', i, 'Loss: ', loss_value.detach().cpu().numpy(), ' LD: ', lr_plot[-1])\n",
    "        np.save(os.path.join(save_to_dir, \"latent%d.npy\" % i), latent.cpu().detach().numpy())\n",
    "        \n",
    "    np.save(os.path.join(save_to_dir, \"loss_plot.npy\"), loss_plot)\n",
    "    np.save(os.path.join(save_to_dir, \"latent_dist.npy\"), latent_dist)\n",
    "    np.save(os.path.join(save_to_dir, \"lr_plot.npy\"), lr_plot)\n",
    "\n",
    "\n",
    "\n",
    "def make_full_transformation(initial_latent, ref_latent, experiment_name, \n",
    "                             decoder, model, alpha_penalty=0.05, constraint_rad=0.1, axis=0, **kwargs):\n",
    "    '''\n",
    "    kwargs:\n",
    "        num_iters=1000, \n",
    "        adjust_lr_every=10, \n",
    "        decreased_by=1.2,\n",
    "        lr=0.005\n",
    "        verbose=10,\n",
    "    '''\n",
    "\n",
    "    #ref_points = get_points_from_latent(decoder, ref_latent, N=128)\n",
    "    save_to_dir = experiment_name\n",
    "    if not os.path.exists(save_to_dir):\n",
    "        os.makedirs(save_to_dir)\n",
    "\n",
    "    #np.save(os.path.join(save_to_dir, \"target_verts.npy\"), ref_points)\n",
    "\n",
    "    optimize_shape_deepSDF(decoder, initial_latent, initial_points=None,\n",
    "                                           alpha_penalty=alpha_penalty,\n",
    "                                           num_points=None, point_iters=2,\n",
    "                                           multiplier_func=lambda x, y: \n",
    "                                               method4_to_arbitatry_loss(x, y, model, \n",
    "                                                                         constraint_rad=constraint_rad, \n",
    "                                                                         axis=axis),\n",
    "                                           save_to_dir=save_to_dir, **kwargs)\n",
    "\n",
    "experiment_directory = \"/cvlabdata2/home/artem/DeepSDF/examples/cars_cleared/\"\n",
    "checkpoint = \"latest\"\n",
    "decoder = load_model(experiment_directory, checkpoint).to('cuda:0')\n",
    "latent_vectors = ws.load_latent_vectors(experiment_directory, checkpoint)\n",
    "model = None\n",
    "DIR_for_dump_data = './data_for_this_experiments'\n",
    "\n",
    "LATENT_TO_OPTIMIZE = latent_vectors[32]\n",
    "punch_lr_at_reindex_by=1\n",
    "reindex_latent_each = 10000\n",
    "\n",
    "make_full_transformation(LATENT_TO_OPTIMIZE.detach(),\n",
    "                         DIR_for_dump_data, decoder, model,\n",
    "                         alpha_penalty=0.2, axis=0,\n",
    "                         constraint_rad=0.05,\n",
    "                         num_iters=30,\n",
    "                         adjust_lr_every=20,\n",
    "                         decreased_by=1.1, \n",
    "                         lr=0.2,\n",
    "                         verbose=None,\n",
    "                         N=256,\n",
    "                         num_neignours_constr=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iJJyuM71ieWb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f_H2o92JieWf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s-LdeJ1zieWi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wZ85ST4pieWl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cY9La8xqieWm"
   },
   "outputs": [],
   "source": [
    "mesh = {'point_pos': tansformed_points, \n",
    "        'edge_vec':torch.tensor(edge_attr, dtype=torch.float).to('cuda:0'),\n",
    "        'edge_index':torch.tensor(edges, dtype=torch.long).t().contiguous().to('cuda:0')\n",
    "        }"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "checkCFDloss.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
