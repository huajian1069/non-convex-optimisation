{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class optimizer(ABC):\n",
    "    @abstractmethod\n",
    "    def set_parameters(self, para):\n",
    "        '''\n",
    "        input: parameters, in dictionary\n",
    "        '''\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def optimise(self, objective_cls):\n",
    "        '''\n",
    "        input: objective function class\n",
    "        output: empirical found optimal, optimum, and statistics of procedure information\n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "class adjust_optimizer(optimizer):\n",
    "    def adjust(self, x0, obj):\n",
    "        self.x0 = x0\n",
    "        return self.optimise(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cma_es(optimizer):\n",
    "    def set_parameters(self, paras):\n",
    "        self.mean0 = paras['mean0'] \n",
    "        self.std = paras['std']\n",
    "        self.tol = paras['tol']\n",
    "        self.adjust_func = paras['adjust_func']\n",
    "        self.max_iter = 400\n",
    "        # set none to use default value \n",
    "        self.cluster_size = None\n",
    "        self.survival_size = None\n",
    "    def optimise(self, obj):\n",
    "        '''\n",
    "        @param obj: objective function class instance\n",
    "        return arg: found minimum arguments\n",
    "               val: found minimum value\n",
    "               stats: collection of recorded statistics for post-analysis\n",
    "        '''                  \n",
    "        def update_mean(x):\n",
    "            return (weights @ x).reshape(dim, 1)\n",
    "        def update_ps(ps, sigma, C, mean, mean_old):\n",
    "            return (1 - cs) * ps + np.sqrt(cs * (2 - cs) * mueff) * invsqrtC @ (mean - mean_old) / sigma \n",
    "        def update_pc(pc, sigma, ps, mean, mean_old):\n",
    "            hsig = np.abs(ps) / np.sqrt(1 - (1 - cs)**(2 * iter_/lambda_)) / chiN < 1.4 + 2/(dim + 1)\n",
    "            return (1 - cc) * pc + hsig * np.sqrt(cc * (2 - cc) * mueff) * (mean - mean_old) / sigma\n",
    "        def update_C(C, pc, x, mean_old, sigma):\n",
    "            hsig = np.abs(ps) / np.sqrt(1 - (1 - cs)**(2 * iter_/lambda_)) / chiN < 1.4 + 2/(dim + 1)\n",
    "            artmp = (1 / sigma) * (x - mean_old.reshape(1, dim))\n",
    "            return (1 - c1 - cmu) * C + c1 * (pc * pc.T + (1 - hsig) * cc * (2 - cc) * C) + cmu * artmp.T @ np.diag(weights) @ artmp\n",
    "        def update_sigma(sigma, ps):\n",
    "            return sigma * np.exp((cs / damps) * (np.linalg.norm(ps)/ chiN - 1))\n",
    "        def not_moving(stats, tol):\n",
    "            return np.linalg.norm(stats['arg'][-1] - stats['arg'][-2]) < tol \\\n",
    "                or np.linalg.norm(stats['val'][-1] - stats['val'][-2]) < tol \\\n",
    "                or np.linalg.norm(stats['mean'][-1] - stats['mean'][-2]) < tol    \n",
    "\n",
    "        print(\"*******starting optimisation from intitial mean: \", mean0.ravel())\n",
    "        # User defined input parameters \n",
    "        dim = 2    \n",
    "        sigma = 0.3\n",
    "        D = self.std / sigma\n",
    "        mean = self.mean0\n",
    "\n",
    "        # the size of solutions group\n",
    "        lambda_ = 4 + int(3 * np.log(dim)) if self.cluster_size == None else self.cluster_size  \n",
    "        # only best \"mu\" solutions are used to generate iterations\n",
    "        mu = int(lambda_ / 2) if self.survival_size == None else self.survival_size\n",
    "        # used to combine best \"mu\" solutions                                               \n",
    "        weights = np.log(mu + 1/2) - np.log(np.arange(mu) + 1) \n",
    "        weights = weights / np.sum(weights)     \n",
    "        mueff = np.sum(weights)**2 / np.sum(weights**2) \n",
    "\n",
    "        # Strategy parameter setting: Adaptation\n",
    "        # time constant for cumulation for C\n",
    "        cc = (4 + mueff / dim) / (dim + 4 + 2 * mueff / dim)  \n",
    "        # t-const for cumulation for sigma control\n",
    "        cs = (mueff + 2) / (dim + mueff + 5)  \n",
    "        # learning rate for rank-one update of C\n",
    "        c1 = 2 / ((dim + 1.3)**2 + mueff)    \n",
    "        # and for rank-mu update\n",
    "        cmu = min(1 - c1, 2 * (mueff - 2 + 1 / mueff) / ((dim + 2)**2 + mueff))  \n",
    "        # damping for sigma, usually close to 1  \n",
    "        damps = 1 + 2 * max(0, np.sqrt((mueff - 1)/( dim + 1)) - 1) + cs                                                                 \n",
    "\n",
    "        # Initialize dynamic (internal) strategy parameters and constants\n",
    "        # evolution paths for C and sigma\n",
    "        pc = np.zeros((dim, 1))     \n",
    "        ps = np.zeros((dim, 1)) \n",
    "        # B defines the coordinate system\n",
    "        B = np.eye(dim)       \n",
    "        # covariance matrix C\n",
    "        C = B * np.diag(D**2) * B.T \n",
    "        # C^-1/2 \n",
    "        invsqrtC = B * np.diag(D**-1) * B.T   \n",
    "        # expectation of ||N(0,I)|| == norm(randn(N,1)) \n",
    "        chiN = dim**0.5 * (1 - 1/(4 * dim) + 1 / (21 * dim**2))  \n",
    "\n",
    "        # --------------------  Initialization --------------------------------  \n",
    "        x, x_old, f = np.zeros((lambda_, dim)), np.zeros((lambda_, dim)), np.zeros((lambda_,))\n",
    "        stats = {}\n",
    "        stats['val'], stats['arg'] = [], []\n",
    "        stats['x_adjust'] = []\n",
    "        iter_eval, stats['evals_per_iter'] = np.zeros((lambda_, )), []\n",
    "        stats['mean'], stats['std'] = [], []\n",
    "        iter_, eval_ = 0, 0\n",
    "\n",
    "        # initial data in record\n",
    "        for i in range(lambda_):\n",
    "            x[i] = (mean + np.random.randn(dim, 1)).ravel()\n",
    "            f[i] = obj.func(x[i])\n",
    "        idx = np.argsort(f)\n",
    "        x_ascending = x[idx]\n",
    "        stats['arg'].append(x_ascending)\n",
    "        stats['val'].append(f[idx])\n",
    "        stats['mean'].append(mean)\n",
    "        stats['std'].append(sigma * B @ np.diag(D))\n",
    "        stats['evals_per_iter'].append(np.ones((lambda_,)))\n",
    "        stats['x_adjust'].append(np.vstack((x.T.copy(), x.T.copy())))\n",
    "\n",
    "        # optimise by iterations\n",
    "        try:\n",
    "            while iter_ < self.max_iter:\n",
    "                iter_ += 1\n",
    "                # generate candidate solutions with some stochastic elements\n",
    "                for i in range(lambda_):\n",
    "                    x[i] = (mean + sigma * B @ np.diag(D) @ np.random.randn(dim, 1)).ravel() \n",
    "                    x_old[i] = x[i]\n",
    "                    x[i], eval_cnt = self.adjust_func.adjust(x[i], obj)\n",
    "                    f[i] = obj.func(x[i])\n",
    "                    eval_ += eval_cnt\n",
    "                    iter_eval[i] = eval_cnt\n",
    "                # sort the value and positions of solutions \n",
    "                idx = np.argsort(f)\n",
    "                x_ascending = x[idx]\n",
    "\n",
    "                # update the parameter for next iteration\n",
    "                mean_old = mean\n",
    "                mean = update_mean(x_ascending[:mu])\n",
    "                ps =   update_ps(ps, sigma, C, mean, mean_old)\n",
    "                pc =   update_pc(pc, sigma, ps, mean, mean_old)\n",
    "                sigma = update_sigma(sigma, ps)\n",
    "                C =    update_C(C, pc, x_ascending[:mu], mean_old, sigma)\n",
    "                C = np.triu(C) + np.triu(C, 1).T\n",
    "                D, B = np.linalg.eig(C)\n",
    "                D = np.sqrt(D)\n",
    "                invsqrtC = B @ np.diag(D**-1) @ B\n",
    "\n",
    "                # record data during process for post analysis\n",
    "                stats['arg'].append(x_ascending)\n",
    "                stats['val'].append(f[idx])\n",
    "                stats['mean'].append(mean)\n",
    "                stats['std'].append(sigma * B @ np.diag(D))\n",
    "                stats['evals_per_iter'].append(iter_eval.copy())\n",
    "                stats['x_adjust'].append(np.vstack((x_old.T.copy(), x.T.copy())))\n",
    "\n",
    "                # check the stop condition\n",
    "                if np.max(D) > (np.min(D) * 1e6):\n",
    "                    stats['status'] = 'diverge'\n",
    "                    print('diverge, concentrate in low dimension manifold')\n",
    "                    break\n",
    "                if not_moving(stats, self.tol) :\n",
    "                    break   \n",
    "        except np.linalg.LinAlgError as err:\n",
    "            stats['status'] = 'diverge'\n",
    "            print('diverge, raise LinAlgError!')\n",
    "        finally:\n",
    "            print('eigenvalue of variance = {}'.format(D))\n",
    "            print('min = {}, total iterations = {}, total evaluatios = {}\\n position = {} {}\\n'.format(f[0], iter_, eval_, x_ascending[0, 0], x_ascending[0, 1]))\n",
    "\n",
    "        # carry statistics info before quit\n",
    "        stats['arg'] = np.array(stats['arg'])\n",
    "        stats['val'] = np.array(stats['val'])\n",
    "        stats['mean'] = np.array(stats['mean'])\n",
    "        stats['std'] = np.array(stats['std'])\n",
    "        stats['evals_per_iter'] = np.array(stats['evals_per_iter'])\n",
    "        stats['x_adjust'] = np.array(stats['x_adjust'])\n",
    "        return stats['arg'][-1][0], stats['val'][-1][0], stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class do_nothing(adjust_optimizer):\n",
    "    def optimise(self, obj):\n",
    "        return self.x0, 1\n",
    "    \n",
    "class round_off(adjust_optimizer):\n",
    "    def optimise(self, obj):\n",
    "        return np.round(self.x0), 1\n",
    "    \n",
    "class line_search(adjust_optimizer):\n",
    "    def __init__(self, alpha, beta):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.max_iter = 4\n",
    "        self.tol = 1e-2\n",
    "    def set_parameters(self, paras):\n",
    "        self.alpha = paras['alpha']\n",
    "        self.beta = paras['beta']\n",
    "        self.max_iter = paras['max_iter']\n",
    "        self.tol = paras['tol']\n",
    "    def optimise(self, obj):\n",
    "        '''\n",
    "        @param x0: initial point position\n",
    "        @param alpha: initial step size\n",
    "        @param beta: control the armijo condition\n",
    "        @return x: point position after moving to local minimum\n",
    "        '''\n",
    "        x = self.x0.copy()\n",
    "        tao = 0.5\n",
    "        fx = obj.func(x)\n",
    "        p = - obj.dfunc(x)\n",
    "        fnx = obj.func(x + self.alpha * p)\n",
    "        eval_cnt = 4\n",
    "        for k in range(self.max_iter):\n",
    "            while fnx > fx + self.alpha * self.beta * (-p @ p):\n",
    "                alpha *= tao\n",
    "                fnx = obj.func(x + self.alpha * p)\n",
    "                eval_cnt += 1\n",
    "            x += self.alpha * p\n",
    "            fx = fnx\n",
    "            p = -obj.dfunc(x)\n",
    "            fnx = obj.func(x + self.alpha * p)\n",
    "            eval_cnt += 2\n",
    "            if np.linalg.norm(p) < self.tol:\n",
    "                break\n",
    "        return x, eval_cnt\n",
    "\n",
    "class line_search_1step(adjust_optimizer):\n",
    "    def __init__(self, alpha, beta):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.max_iter = 4\n",
    "        self.tol = 1e-2\n",
    "    def set_parameters(self, paras):\n",
    "        self.alpha = paras['alpha']\n",
    "        self.beta = paras['beta']\n",
    "        self.max_iter = paras['max_iter']\n",
    "        self.tol = paras['tol']\n",
    "    def optimiser(self, obj):\n",
    "        '''\n",
    "        @param x0: initial point position\n",
    "        @param alpha: initial step size\n",
    "        @param beta: control the armijo condition\n",
    "        @return x: point position after moving to local minimum\n",
    "        '''\n",
    "        x = self.x0.copy()\n",
    "        tao = 0.5\n",
    "        fx = obj.func(x)\n",
    "        p = - obj.dfunc(x)\n",
    "        eval_cnt = 4\n",
    "        while obj.func(x + self.alpha * p) > fx + self.alpha * self.beta * (-p @ p):\n",
    "            self.alpha *= tao\n",
    "            eval_cnt += 1\n",
    "        x += self.alpha * p\n",
    "        return x, eval_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-ada] *",
   "language": "python",
   "name": "conda-env-miniconda3-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
