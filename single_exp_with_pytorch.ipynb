{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from library.experiments import *\n",
    "from library.objective_function import *\n",
    "from library.optimiser import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*******starting optimisation from intitial point:  [16.23  23.023]\n",
      "total evaluatios = 201\n",
      "gradient at stop position = tensor([-0.0001,  0.0001]),\n",
      "modified graident = tensor([-2.1003e-06, -8.8239e-06])\n",
      "found minimum position = [15.999424 22.999178], found minimum = 19.61959457397461\n",
      "Result:  local minimum\n",
      "found minimum: 19.61959457397461, minimum position: [15.999424 22.999178], evals: 201\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('local minimum', tensor(19.6196, grad_fn=<AddBackward0>), 201)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init and setup one experiment\n",
    "exp = single_experiment()\n",
    "# One experiment: setup objective function\n",
    "ak = ackley()\n",
    "exp.set_objective(ak)\n",
    "ad = adam()\n",
    "optParas = {\n",
    "         'x0': torch.tensor([16.23, 23.023], requires_grad=True),\n",
    "         'alpha': 0.1,\n",
    "         'beta_1': 0.9, \n",
    "         'beta_2': 0.999, \n",
    "         'epsilon': 1e-11, \n",
    "         'max_iter': 2000,\n",
    "         'tol': 1e-6,              \n",
    "         'verbose': True,\n",
    "         'record': False }\n",
    "ad.set_parameters(optParas)\n",
    "exp.set_optimizer(ad)\n",
    "exp.do()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*******starting optimisation from intitial point:  [16.23  23.023]\n",
      "total evaluatios = 41\n",
      "gradient at stop position = [0.00045202 0.00023452]\n",
      "found minimum position = [15.999435 22.99918 ], found minimum = 19.61959457397461\n",
      "Result:  local minimum\n",
      "found minimum: 19.61959457397461, minimum position: [15.999435 22.99918 ], evals: 41\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('local minimum', tensor(19.6196, grad_fn=<AddBackward0>), 41)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init and setup one experiment\n",
    "exp = single_experiment()\n",
    "# One experiment: setup objective function\n",
    "ak = ackley()\n",
    "exp.set_objective(ak)\n",
    "ln = line_search()\n",
    "optParas = {\n",
    "    'x0': torch.tensor([16.23, 23.023], requires_grad=True),\n",
    "    'alpha': 1,\n",
    "    'beta': 0.1, \n",
    "    'max_iter': 1000,\n",
    "    'tol': 1e-3,              \n",
    "    'verbose': True,\n",
    "    'record': False\n",
    "}\n",
    "ln.set_parameters(optParas)\n",
    "exp.set_optimizer(ln)\n",
    "exp.do()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cma_es(adjust_optimizer):\n",
    "    def __init__(self, dim=2):\n",
    "        self.dim = dim\n",
    "        paras = {'x0': torch.zeros((dim,)),\n",
    "                 'std': torch.ones((dim,)) * 3, \n",
    "                 'tol': 1e-5, \n",
    "                 'adjust_func': do_nothing(), \n",
    "                 'record': False, \n",
    "                 'verbose': False}\n",
    "        self.set_parameters(paras)\n",
    "    def set_parameters(self, paras):\n",
    "        self.paras = paras\n",
    "        self.x0 = paras['x0'] \n",
    "        self.std = paras['std']\n",
    "        self.tol = paras['tol']\n",
    "        self.adjust_func = paras['adjust_func']\n",
    "        self.max_iter = 400 if 'max_iter' not in paras.keys() else paras['max_iter']\n",
    "        # set none to use default value \n",
    "        self.cluster_size = None if 'cluster_size' not in paras.keys() else paras['cluster_size']\n",
    "        self.survival_size = None if 'survival_size' not in paras.keys() else paras['survival_size']\n",
    "        self.record = True if 'record' not in paras.keys() else paras['record']\n",
    "        self.verbose = True if 'verbose' not in paras.keys() else paras['verbose']\n",
    "   \n",
    "    def __update_mean(self, x):\n",
    "        return (self.weights @ x).reshape(self.dim, 1)\n",
    "    \n",
    "    def __update_ps(self):\n",
    "        return (1 - self.cs) * self.ps + torch.sqrt(self.cs * (2 - self.cs) * self.MU_EFF) * self.invsqrtC @ (self.mean - self.mean_old) / self.sigma \n",
    "    \n",
    "    def __update_pc(self):\n",
    "        hsig = (torch.norm(self.ps) / torch.sqrt(1 - (1 - self.cs)**(2 * self.iter_/self.lambda_)) / self.chiN < 1.4 + 2/(self.dim + 1)).int()\n",
    "        return (1 - self.cc) * self.pc + hsig * torch.sqrt(self.cc * (2 - self.cc) * self.MU_EFF) * (self.mean - self.mean_old) / self.sigma\n",
    "    \n",
    "    def __update_C(self):\n",
    "        hsig = (torch.norm(self.ps) / torch.sqrt(1 - (1 - cs)**(2 * iter_/lambda_)) / chiN < (1.4 + 2/(dim + 1))).int()\n",
    "        artmp = (1 / self.sigma) * (x - mean_old.reshape(1, dim))\n",
    "        return (1 - self.c1 - self.cmu) * self.C + self.c1 * (self.pc * self.pc.T + (1 - self.hsig) * self.cc * (2 - self.cc) * C) + self.cmu * artmp.T @ torch.diag(self.weights) @ artmp\n",
    "    \n",
    "    def __update_sigma(self):\n",
    "        return self.sigma * torch.exp((self.cs / self.damps) * (torch.norm(self.ps)/ self.chiN - 1))\n",
    "    \n",
    "    def __is_not_moving(arg, val, pre_arg, pre_val, tol):\n",
    "        dis_arg = torch.norm(arg - pre_arg, dim=1).mean()\n",
    "        dis_val = torch.abs(val - pre_val).mean()\n",
    "        return (dis_arg < tol and dis_val < tol) \n",
    "\n",
    "    def __step(self):\n",
    "        for i in range(lambda_):\n",
    "            x[i] = (mean + sigma * B @ torch.diag(D) @ torch.randn(dim, 1)).squeeze()\n",
    "            x_old[i] = x[i]\n",
    "            x[i], f[i], inner_stats[i] = self.adjust_func.adjust(x[i].clone().detach().requires_grad_(True), obj)\n",
    "            eval_ += inner_stats[i]['evals']\n",
    "            iter_eval[i] = inner_stats[i]['evals']\n",
    "        # sort the value and positions of solutions \n",
    "        idx = torch.argsort(f.detach())\n",
    "        x_ascending = x[idx]\n",
    "\n",
    "        # update the parameter for next iteration\n",
    "        mean_old = mean\n",
    "        mean = self__update_mean(x_ascending[:mu])\n",
    "        self.__update_ps()\n",
    "        self.__update_pc()\n",
    "        self.__update_sigma()\n",
    "        self.__update_C()\n",
    "        C = torch.triu(C) + torch.triu(C, 1).T\n",
    "        D, B = torch.eig(C, eigenvectors=True)\n",
    "        D = torch.sqrt(D[:,0])\n",
    "        invsqrtC = B @ torch.diag(D**-1) @ B\n",
    "\n",
    "        # record data during process for post analysis\n",
    "        if self.record:\n",
    "            self.stats['inner'].append(inner_stats.clone().detach().numpy())\n",
    "            self.stats['arg'].append(x_ascending.detach().numpy())\n",
    "            self.stats['val'].append(f[idx].detach().numpy())\n",
    "            self.stats['mean'].append(mean.detach().numpy())\n",
    "            self.stats['std'].append((sigma * B @ np.diag(D)).detach().numpy())\n",
    "            self.stats['evals_per_iter'].append(iter_eval.clone().detach().numpy())\n",
    "            self.stats['x_adjust'].append(np.vstack((x_old.T.clone().detach().numpy(), x.T.clone().detach().numpy())))\n",
    "        # record best solution    \n",
    "        if best_val > f[idx[0]]:\n",
    "            best_val = f[idx[0]]\n",
    "            best_arg = x_ascending[0]\n",
    "        return x_ascending, f[idx]\n",
    "        \n",
    "    def __setup():\n",
    "                # User defined input parameters \n",
    "        self.sigma = 0.3\n",
    "        self.D = self.std / sigma\n",
    "        self.mean = self.x0.reshape(dim, 1)\n",
    "        # the size of solutions group\n",
    "        self.lambda_ = 4 + int(3 * np.log(dim)) if self.cluster_size == None else self.cluster_size  \n",
    "        # only best \"mu\" solutions are used to generate iterations\n",
    "        self.mu = int(lambda_ / 2) if self.survival_size == None else self.survival_size\n",
    "        # used to combine best \"mu\" solutions                                               \n",
    "        weights = np.log(mu + 1/2) - torch.log(torch.arange(mu, dtype=torch.float) + 1) \n",
    "        self.weights = (weights / torch.sum(weights)).float()    \n",
    "        self.MU_EFF = 1 / torch.sum(weights**2) \n",
    "\n",
    "        # Strategy parameter setting: Adaptation\n",
    "        # time constant for cumulation for C\n",
    "        self.cc = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.MU_EFF / self.dim)  \n",
    "        # t-const for cumulation for sigma control\n",
    "        self.cs = (self.MU_EFF + 2) / (self.dim + self.MU_EFF + 5)  \n",
    "        # learning rate for rank-one update of C\n",
    "        self.c1 = 2 / ((self.dim + 1.3)**2 + self.MU_EFF)    \n",
    "        # and for rank-mu update\n",
    "        self.cmu = min(1 - self.c1, 2 * (sefl.MU_EFF - 2 + 1 / self.MU_EFF) / ((self.dim + 2)**2 + self.MU_EFF))  \n",
    "        # damping for sigma, usually close to 1  \n",
    "        self.damps = 1 + 2 * max(0, np.sqrt((self.MU_EFF - 1)/( self.dim + 1)) - 1) + self.cs                                                                 \n",
    "\n",
    "        # Initialize dynamic (internal) strategy parameters and constants\n",
    "        # evolution paths for C and sigma\n",
    "        self.pc = torch.zeros((self.dim, 1))     \n",
    "        self.ps = torch.zeros((self.dim, 1)) \n",
    "        # B defines the coordinate system\n",
    "        self.B = torch.eye(int(self.dim))       \n",
    "        # covariance matrix C\n",
    "        self.C = self.B * torch.diag(self.D**2) * self.B.T \n",
    "        # C^-1/2 \n",
    "        self.invsqrtC = self.B * torch.diag(self.D**-1) * self.B.T   \n",
    "        # expectation of ||N(0,I)|| == norm(randn(N,1)) \n",
    "        self.chiN = self.dim**0.5 * (1 - 1/(4 * self.dim) + 1 / (21 * self.dim**2))  \n",
    "\n",
    "        # --------------------  Initialization --------------------------------  \n",
    "        x, x_old, f = torch.zeros((lambda_, dim)), torch.zeros((lambda_, dim)), torch.zeros((lambda_,))\n",
    "        self.stats = {}\n",
    "        inner_stats = {}\n",
    "        self.stats['inner'] = []\n",
    "        self.stats['val'], stats['arg'] = [], []\n",
    "        self.stats['x_adjust'] = []\n",
    "        iter_eval, stats['evals_per_iter'] = torch.zeros((lambda_, )), []\n",
    "        inner_stats = [{}] * lambda_\n",
    "        self.stats['mean'], stats['std'] = [], []\n",
    "        self.stats['status'] = None\n",
    "        iter_, eval_ = 0, 0\n",
    "        \n",
    "    def optimise(self, obj):\n",
    "        '''\n",
    "        @param obj: objective function class instance\n",
    "        return arg: found minimum arguments\n",
    "               val: found minimum value\n",
    "               stats: collection of recorded statistics for post-analysis\n",
    "        '''                  \n",
    "        if self.verbose:\n",
    "            print(\"\\n\\n*******starting optimisation from intitial mean: \", self.x0.squeeze().detach().numpy())\n",
    "        self.__setup()\n",
    "        \n",
    "        # initial data in record\n",
    "        for i in range(lambda_):\n",
    "            x[i,:] = (mean + torch.randn(dim, 1)).squeeze()\n",
    "            f[i] = obj.func(x[i])\n",
    "        idx = torch.argsort(f.detach())\n",
    "        x_ascending = x[idx]\n",
    "        if self.record:\n",
    "            stats['inner'].append(inner_stats.detach().numpy())\n",
    "            stats['arg'].append(x_ascending.detach().numpy())\n",
    "            stats['val'].append(f[idx].detach().numpy())\n",
    "            stats['mean'].append(mean.detach().numpy())\n",
    "            stats['std'].append(sigma * B @ torch.diag(D))\n",
    "            stats['evals_per_iter'].append(torch.ones((lambda_,)).detach().numpy())\n",
    "            stats['x_adjust'].append(np.vstack((x.T.clone().detach().numpy(), x.T.clone().detach().numpy())))\n",
    "        arg = x_ascending\n",
    "        val = f[idx]\n",
    "        \n",
    "        pre_arg = x_ascending\n",
    "        pre_val = f[idx]\n",
    "        self.best_val = 1e4\n",
    "        self.best_arg = None\n",
    "        # optimise by iterations\n",
    "        try:\n",
    "            while iter_ < self.max_iter:\n",
    "                iter_ += 1\n",
    "                arg, val = self.__step()\n",
    "                # check the stop condition\n",
    "                if torch.max(D) > (torch.min(D) * 1e6):\n",
    "                    stats['status'] = 'diverge'\n",
    "                    print('diverge, concentrate in low dimension manifold')\n",
    "                    break\n",
    "                if self.__is_not_moving(arg, val, pre_arg, pre_val, self.tol) :\n",
    "                    break\n",
    "                pre_arg = arg\n",
    "                pre_val = val\n",
    "        except np.linalg.LinAlgError as err:\n",
    "            stats['status'] = 'diverge'\n",
    "            print('diverge, raise LinAlgError!')\n",
    "        finally:\n",
    "            if self.verbose:\n",
    "                print('eigenvalue of variance = {}'.format(D))\n",
    "                print('total iterations = {}, total evaluatios = {}'.format(iter_, eval_))\n",
    "                print('found minimum position = {}, found minimum = {}'.format(best_arg.detach().numpy(), best_val.detach().numpy()))\n",
    "\n",
    "        # carry statistics info before quit\n",
    "        if self.record:\n",
    "            stats['arg'] = np.array(stats['arg'])\n",
    "            stats['val'] = np.array(stats['val'])\n",
    "            stats['mean'] = np.array(stats['mean'])\n",
    "            stats['std'] = np.array(stats['std'])\n",
    "            stats['evals_per_iter'] = np.array(stats['evals_per_iter'])\n",
    "            stats['x_adjust'] = np.array(stats['x_adjust'])\n",
    "        stats['evals'] = eval_\n",
    "        return best_arg, best_val, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*******starting optimisation from intitial mean:  [196.23  123.023]\n",
      "eigenvalue of variance = tensor([0.2339, 0.1790], grad_fn=<SqrtBackward>)\n",
      "total iterations = 95, total evaluatios = 570\n",
      "found minimum position = [197.00003 123.00017], found minimum = 20.0\n",
      "Result:  local minimum\n",
      "found minimum: 20.0, minimum position: [197.00003 123.00017], evals: 570\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('local minimum', tensor(20., grad_fn=<SelectBackward>), 570)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init and setup one experiment\n",
    "exp = single_experiment()\n",
    "# One experiment: setup objective function\n",
    "ak = ackley()\n",
    "exp.set_objective(ak)\n",
    "opt = cma_es(dim=2)\n",
    "optParas ={'x0': torch.tensor([196.23, 123.023], requires_grad=True),\n",
    "           'std': torch.ones((2,)) * 3, \n",
    "           'tol': 1e-3, \n",
    "           'adjust_func': do_nothing(), \n",
    "           'record': False, \n",
    "           'verbose': True}\n",
    "opt.set_parameters(optParas)\n",
    "exp.set_optimizer(opt)\n",
    "exp.do()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*******starting optimisation from intitial mean:  tensor([196.2300, 123.0230], grad_fn=<SqueezeBackward0>)\n",
      "eigenvalue of variance = tensor([ 8.1473, 12.1323], grad_fn=<SqrtBackward>)\n",
      "total iterations = 35, total evaluatios = 12136\n",
      "found minimum position = tensor([ 4.1871e-07, -1.2184e-07], grad_fn=<SelectBackward>), found minimum = 9.5367431640625e-07\n",
      "Result:  global minimum\n",
      "found minimum: 9.5367431640625e-07, minimum position: [ 4.1870697e-07 -1.2184466e-07], evals: 12136\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('global minimum', tensor(9.5367e-07, grad_fn=<SelectBackward>), 12136)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init and setup one experiment\n",
    "exp = single_experiment()\n",
    "# One experiment: setup objective function\n",
    "ak = ackley()\n",
    "exp.set_objective(ak)\n",
    "\n",
    "inner_opt = line_search()\n",
    "innerOptParas = {\n",
    "   'x0': None,\n",
    "    'alpha': 1,\n",
    "    'beta': 0.1, \n",
    "    'max_iter': 100,\n",
    "    'tol': 1e-1,              \n",
    "    'verbose': False,\n",
    "    'record': False\n",
    "}\n",
    "inner_opt.set_parameters(innerOptParas)\n",
    "\n",
    "opt = cma_es()\n",
    "optParas ={'x0': torch.tensor([196.23, 123.023], requires_grad=True),\n",
    "           'std': torch.ones((2,)) * 30, \n",
    "           'tol': 1e-3, \n",
    "           'adjust_func': inner_opt, \n",
    "           'record': False, \n",
    "           'verbose': True}\n",
    "opt.set_parameters(optParas)\n",
    "exp.set_optimizer(opt)\n",
    "exp.do()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*******starting optimisation from intitial mean:  tensor([196.2300, 123.0230], grad_fn=<SqueezeBackward0>)\n",
      "eigenvalue of variance = tensor([0.0018, 0.0028], grad_fn=<SqrtBackward>)\n",
      "total iterations = 400, total evaluatios = 186400\n",
      "found minimum position = tensor([14.0245, -5.0542], grad_fn=<SelectBackward>), found minimum = 17.66341209411621\n",
      "Result:  local minimum\n",
      "found minimum: 17.66341209411621, minimum position: [14.024487 -5.054191], evals: 186400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('local minimum', tensor(17.6634, grad_fn=<SelectBackward>), 186400)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp = single_experiment()\n",
    "# setup objective function\n",
    "ak = ackley()\n",
    "exp.set_objective(ak)\n",
    "\n",
    "inner_opt = adam()\n",
    "optParas = {\n",
    "         'x0': None,\n",
    "         'alpha': 1,\n",
    "         'beta_1': 0.9, \n",
    "         'beta_2': 0.999, \n",
    "         'epsilon': 1e-11, \n",
    "         'max_iter': 100,\n",
    "         'tol': 1e-1,              \n",
    "         'verbose': False,\n",
    "         'record': False }\n",
    "inner_opt.set_parameters(optParas)\n",
    "\n",
    "opt = cma_es()\n",
    "optParas ={'x0': torch.tensor([196.23, 123.023], requires_grad=True),\n",
    "           'std': torch.ones((2,)) * 30, \n",
    "           'tol': 1e-3, \n",
    "           'adjust_func': inner_opt, \n",
    "           'record': False, \n",
    "           'verbose': True}\n",
    "opt.set_parameters(optParas)\n",
    "exp.set_optimizer(opt)\n",
    "exp.do()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-ada] *",
   "language": "python",
   "name": "conda-env-miniconda3-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
